{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b13a764",
   "metadata": {},
   "source": [
    "##### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2472f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface==0.1.2\n",
      "  Obtaining dependency information for langchain-huggingface==0.1.2 from https://files.pythonhosted.org/packages/9d/f8/77a303ddc492f6eed8bf0979f2bc6db4fa6eb1089c5e9f0f977dd87bc9c2/langchain_huggingface-0.1.2-py3-none-any.whl.metadata\n",
      "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.30.2)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.3.58)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface==0.1.2)\n",
      "  Obtaining dependency information for sentence-transformers>=2.6.0 from https://files.pythonhosted.org/packages/45/2d/1151b371f28caae565ad384fdc38198f1165571870217aedda230b9d7497/sentence_transformers-4.1.0-py3-none-any.whl.metadata\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.21.1)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface==0.1.2)\n",
      "  Obtaining dependency information for transformers>=4.39.0 from https://files.pythonhosted.org/packages/a9/b6/5257d04ae327b44db31f15cce39e6020cc986333c715660b1315a9724d82/transformers-4.51.3-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.13.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.11.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.15.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (11.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (0.5.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2025.4.26)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.6.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.3.1)\n",
      "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Installing collected packages: transformers, sentence-transformers, langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.1.2 sentence-transformers-4.1.0 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## 추가 패키지 설치\n",
    "pip install langchain-huggingface==0.1.2\n",
    "pip install langgraph==0.2.34\n",
    "pip install gradio==4.44.1\n",
    "!pip install langchain-community==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e1596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adaptive_self_rag\n",
    "금융상품(예: 정기예금, 입출금자유예금) 관련 질의에 대해:\n",
    "1. 질문 라우팅 → (금융상품 관련이면) 문서 검색 (병렬 서브 그래프) → 문서 평가 → (조건부) 질문 재작성 → 답변 생성\n",
    "   / (금융상품과 무관하면) LLM fallback을 통해 바로 답변 생성\n",
    "그리고 생성된 답변의 품질(환각, 관련성) 평가 후 필요시 재생성 또는 재작성하는 Adaptive Self-RAG 체인.\n",
    "\"\"\"\n",
    "\n",
    "#############################\n",
    "# 1. 기본 환경 및 라이브러리\n",
    "#############################\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 기타 유틸\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "from textwrap import dedent\n",
    "from operator import add\n",
    "from heapq import merge\n",
    "from typing import List, Literal, Sequence, TypedDict, Annotated, Tuple\n",
    "\n",
    "# 서치 알고리즘\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# LangChain, Chroma, LLM 관련\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Grader 평가지표용\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 그래프 관련\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Gradio 관련\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fece5f9",
   "metadata": {},
   "source": [
    "##### 2. 임베딩 및 DB설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "class LangChainSentenceTransformer(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"[INFO] Using device: {device}\")\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text, show_progress_bar=False, convert_to_numpy=True).tolist()\n",
    "\n",
    "embeddings_model_koMultitask = LangChainSentenceTransformer(\"jhgan/ko-sroberta-multitask\") # 768차원 임배딩 모델로 변경\n",
    "\n",
    "# Chroma DB 경로\n",
    "CHROMA_DIR = \"./../findata/chroma_db\"\n",
    "\n",
    "# JSON 데이터 경로\n",
    "FIXED_JSON_PATH = \"./../findata/processed_fixed_deposit.json\"\n",
    "DEMAND_JSON_PATH = \"./../findata/processed_demand_deposit.json\"\n",
    "\n",
    "# DB 이름\n",
    "FIXED_COLLECTION = \"processed_fixed_deposit\"\n",
    "DEMAND_COLLECTION = \"processed_demand_deposit\"\n",
    "\n",
    "def load_and_prepare_all_documents(json_paths: list[str]) -> list[Document]:\n",
    "    docs: list[Document] = []\n",
    "    for path in json_paths:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        for entry in data[\"documents\"]:\n",
    "            # 본문\n",
    "            content = entry[\"content\"]\n",
    "            # metadata 필드 영어 키로 통일\n",
    "            md = entry.get(\"metadata\", {})\n",
    "            bank         = entry.get(\"bank\",         md.get(\"은행\"))\n",
    "            product_name = entry.get(\"product_name\", md.get(\"상품명\"))\n",
    "            category     = entry.get(\"type\")\n",
    "            pdf_link     = md.get(\"pdf_link\")\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"id\":           entry.get(\"id\"),\n",
    "                        \"type\":         category,\n",
    "                        \"bank\":         bank,\n",
    "                        \"product_name\": product_name,\n",
    "                        \"pdf_link\":     pdf_link\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    return docs\n",
    "\n",
    "# JSON 파일 경로 리스트\n",
    "ALL_JSON = [\n",
    "    FIXED_JSON_PATH,\n",
    "    DEMAND_JSON_PATH\n",
    "]\n",
    "\n",
    "all_documents = load_and_prepare_all_documents(ALL_JSON)\n",
    "corpus = [doc.page_content.split() for doc in all_documents]\n",
    "bm25_index = BM25Okapi(corpus)\n",
    "\n",
    "# 인제스천\n",
    "vector_db = Chroma(\n",
    "    embedding_function=embeddings_model_koMultitask,\n",
    "    collection_name=\"combined_products_koMultitask\",  # 임배딩 모델에 따른 인제스천 이름 변경\n",
    "    persist_directory=CHROMA_DIR,\n",
    ")\n",
    "\n",
    "# 한 번만 인제스천\n",
    "if not vector_db._collection.count():\n",
    "    vector_db.add_documents(all_documents)\n",
    "\n",
    "def extract_bank(query: str) -> str | None:\n",
    "    m = re.search(r'([가-힣A-Za-z0-9]+(?:은행|뱅크))', query)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def extract_product(query: str) -> str | None:\n",
    "    # “~통장”, “~예금”, “~대출” 등으로 마침\n",
    "    m = re.search(r'([가-힣A-Za-z0-9]+(?:통장|예금|대출))', query)\n",
    "    return m.group(1).strip() if m else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f6ba7",
   "metadata": {},
   "source": [
    "##### 3. 서치알고리즘 및 도구(검색 함수) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef46902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _search_with_filters(query: str, filters: dict, top_k: int) -> list[Document]:\n",
    "    # 1) BM25: 전체 코퍼스에서 score 계산 → metadata 필터 적용해 top_k\n",
    "    tokenized = query.split()\n",
    "    scores = bm25_index.get_scores(tokenized)\n",
    "    idxs   = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    bm25_docs = []\n",
    "    for i in idxs:\n",
    "        d = all_documents[i]\n",
    "        if all(filters.get(k) is None or d.metadata.get(k)==filters[k] for k in filters):\n",
    "            bm25_docs.append(d)\n",
    "            if len(bm25_docs)>=top_k: break\n",
    "\n",
    "    # 2) 벡터: Chroma의 filter 파라미터 사용 (단일키 vs 다중키에 따라 $and 로 묶어서 전달)\n",
    "    meta = {k: v for k, v in filters.items() if v is not None}\n",
    "    if not meta:\n",
    "        vec_docs = vector_db.similarity_search(query, k=top_k)\n",
    "    elif len(meta) == 1:\n",
    "        # 단일 필터터\n",
    "        vec_docs = vector_db.similarity_search(query, k=top_k, filter=meta)\n",
    "    else:\n",
    "        # 여러 필터는 하나의 연산자($and)로 묶어서 넘겨야 함\n",
    "        and_list = [{k: v} for k, v in meta.items()]\n",
    "        vec_docs = vector_db.similarity_search(\n",
    "                    query, \n",
    "                    k=top_k, \n",
    "                    filter={\"$and\": and_list}\n",
    "        )\n",
    "\n",
    "    # 3) 중복 제거 병합\n",
    "    seen, merged = set(), []\n",
    "    for d in bm25_docs + vec_docs:\n",
    "        uid = d.metadata[\"id\"]\n",
    "        if uid not in seen:\n",
    "            seen.add(uid)\n",
    "            # PDF 링크가 있으면 page_content에 추가\n",
    "            pdf = d.metadata.get(\"pdf_link\")\n",
    "            if pdf and \"pdf_link\" not in d.page_content:\n",
    "                d.page_content += f\"\\n\\n📄 [상품설명서 PDF 보기]({pdf})\"\n",
    "            merged.append(d)\n",
    "    return merged\n",
    "\n",
    "# 하이브리드 서치 알고리즘\n",
    "def hybrid_core_search(query: str, category: str, bank: str=None, product_name: str=None, top_k: int=2) -> List[Document]:\n",
    "    # 1) 메타 필터 준비 (category 필수 포함)\n",
    "    filters = {\"type\": category}\n",
    "    if bank: \n",
    "        filters[\"bank\"] = bank\n",
    "    if product_name: \n",
    "        filters[\"product_name\"] = product_name\n",
    "\n",
    "    # 2) 필터 레벨별로 점진적 검색\n",
    "    filter_levels = [\n",
    "        filters,\n",
    "        {**filters, **{\"product_name\": None}},  # 상품명 제외\n",
    "        {**filters, **{\"bank\": None}},          # 은행 제외\n",
    "        {\"category\": category}                  # 카테고리만\n",
    "    ]\n",
    "\n",
    "    # 3) 순서대로 BM25+벡터 병렬 검색 → 결과 반환\n",
    "    for flt in filter_levels:\n",
    "        docs = _search_with_filters(query, flt, top_k=top_k)\n",
    "        if docs:\n",
    "            return docs\n",
    "    return []\n",
    "\n",
    "@tool\n",
    "def search_fixed_deposit(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Search for relevant fixed deposit (정기예금) product information using semantic similarity.\n",
    "    This tool retrieves products matching the user's query, such as interest rates or terms.\n",
    "    \"\"\"\n",
    "    bank, product = extract_bank(query), extract_product(query)\n",
    "    return hybrid_core_search(query, category=\"정기예금\", bank=bank, product_name=product)\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_demand_deposit(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Search for demand deposit (입출금자유예금) product information using semantic similarity.\n",
    "    This tool retrieves products matching the user's query, such as flexible withdrawal or interest features.\n",
    "    \"\"\"\n",
    "    bank, product = extract_bank(query), extract_product(query)\n",
    "    return hybrid_core_search(query, category=\"입출금자유예금\", bank=bank, product_name=product)\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    This tool serves as a supplementary utility for the financial product recommendation model.\n",
    "    It retrieves up-to-date external information via web search using the Tavily API, \n",
    "    especially when relevant data is not available in the local vector databases\n",
    "\n",
    "    Unlike the RAG-based tools that query embedded product databases,\n",
    "    this tool is designed to handle broader or real-time questions—such as current interest rates, financial trends,\n",
    "    or general queries outside the scope of structured deposit data.\n",
    "\n",
    "    It returns the top 2 semantically relevant documents from the web.\n",
    "    \"\"\"\n",
    "\n",
    "    tavily_search = TavilySearchResults(max_results=2)\n",
    "    docs = tavily_search.invoke(query)\n",
    "\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        formatted_docs.append(\n",
    "            Document(\n",
    "                page_content= f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>',\n",
    "                metadata={\"source\": \"web search\", \"url\": doc[\"url\"]}\n",
    "                )\n",
    "        )\n",
    "\n",
    "    if len(formatted_docs) > 0:\n",
    "        return formatted_docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "\n",
    "tools = [search_fixed_deposit, search_demand_deposit, web_search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff8277",
   "metadata": {},
   "source": [
    "##### 4. llm초기화 & 도구 바인딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bbaf7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39bb28",
   "metadata": {},
   "source": [
    "##### 5. LLM 체인 (Retrieval Grader / Answer Generator / Hallucination / Answer Graders / Question Re-writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f85bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================================================\n",
      " \n",
      "LLM 체인\n",
      "\n",
      "# (1) Retrieval Grader\n",
      "\n",
      "\n",
      "# (3) Hallucination Grader\n",
      "\n",
      "\n",
      "# (4) Answer Grader\n",
      "\n",
      "\n",
      "# (5) Question Re-writer\n",
      "\n",
      "\n",
      "# (6) Generation Evaluation & Decision Nodes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# 5. LLM 체인 (Retrieval Grader / Answer Generator / Hallucination / Answer Graders / Question Re-writer)\n",
    "#############################\n",
    "print(\"\\n===================================================================\\n \")\n",
    "print(\"LLM 체인\\n\")\n",
    "print(\"# (1) Retrieval Grader\\n\")\n",
    "\n",
    "# (1) Retrieval Grader (검색평가)\n",
    "class BinaryGradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_BinaryGradeDocuments = llm.with_structured_output(BinaryGradeDocuments)\n",
    "\n",
    "system_prompt = \"\"\"You are an expert in evaluating the relevance of search results to user queries.\n",
    "\n",
    "[Evaluation criteria]\n",
    "1. 키워드 관련성: 문서가 질문의 주요 단어나 유사어를 포함하는지 확인\n",
    "2. 의미적 관련성: 문서의 전반적인 주제가 질문의 의도와 일치하는지 평가\n",
    "3. 부분 관련성: 질문의 일부를 다루거나 맥락 정보를 제공하는 문서도 고려\n",
    "4. 답변 가능성: 직접적인 답이 아니더라도 답변 형성에 도움될 정보 포함 여부 평가\n",
    "\n",
    "[Scoring]\n",
    "- Rate 'yes' if relevant, 'no' if not\n",
    "- Default to 'no' when uncertain\n",
    "\n",
    "[Key points]\n",
    "- Consider the full context of the query, not just word matching\n",
    "- Rate as relevant if useful information is present, even if not a complete answer\n",
    "\n",
    "Your evaluation is crucial for improving information retrieval systems. Provide balanced assessments.\n",
    "\"\"\"\n",
    "# 채점 프롬프트 템플릿\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"[Retrieved document]\\n{document}\\n\\n[User question]\\n{question}\")\n",
    "])\n",
    "\n",
    "retrieval_grader_binary = grade_prompt | structured_llm_BinaryGradeDocuments\n",
    "\n",
    "# question = \"어떤 예금 상품이 있는지 설명해주세요.\"\n",
    "# print(f'\\nquestion : {question}\\n')\n",
    "# retrieved_docs = fixed_deposit_db.similarity_search(question, k=2)\n",
    "# print(f\"검색된 문서 수: {len(retrieved_docs)}\")\n",
    "# print(\"===============================================================================\")\n",
    "# print()\n",
    "\n",
    "# relevant_docs = []\n",
    "# for doc in retrieved_docs:\n",
    "#     print(\"문서:\\n\", doc.page_content)\n",
    "#     print(\"---------------------------------------------------------------------------\")\n",
    "\n",
    "#     relevance = retrieval_grader_binary.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "#     print(f\"문서 관련성: {relevance}\")\n",
    "\n",
    "#     if relevance.binary_score == 'yes':\n",
    "#         relevant_docs.append(doc)\n",
    "    \n",
    "#     print(\"===========================================================================\")\n",
    "\n",
    "\n",
    "# (2) Answer Generator (일반 RAG)\n",
    "\n",
    "# (3) Hallucination Grader\n",
    "print(\"\\n# (3) Hallucination Grader\\n\")\n",
    "\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_HradeHallucinations = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# 환각 평가를 위한 시스템 프롬프트 정의\n",
    "halluci_system_prompt = \"\"\"\n",
    "You are an expert evaluator assessing whether an LLM-generated answer is grounded in and supported by a given set of facts.\n",
    "\n",
    "[Your task]\n",
    "    - Review the LLM-generated answer.\n",
    "    - Determine if the answer is fully supported by the given facts.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - 답변에 주어진 사실이나 명확히 추론할 수 있는 정보 외의 내용이 없어야 합니다.\n",
    "    - 답변의 모든 핵심 내용이 주어진 사실에서 비롯되어야 합니다.\n",
    "    - 사실적 정확성에 집중하고, 글쓰기 스타일이나 완전성은 평가하지 않습니다.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer is factually grounded and fully supported.\n",
    "    - 'no': The answer includes information or claims not based on the given facts.\n",
    "\n",
    "Your evaluation is crucial in ensuring the reliability and factual accuracy of AI-generated responses. Be thorough and critical in your assessment.\n",
    "\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", halluci_system_prompt),\n",
    "        (\"human\", \"[Set of facts]\\n{documents}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_HradeHallucinations\n",
    "# hallucination = hallucination_grader.invoke({\"documents\": relevant_docs, \"generation\": generation})\n",
    "# print(f\"환각 평가: {hallucination}\")\n",
    "\n",
    "print(\"\\n# (4) Answer Grader\\n\")\n",
    "# (4) Answer Grader \n",
    "class BinaryGradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_BinaryGradeAnswer = llm.with_structured_output(BinaryGradeAnswer)\n",
    "grade_system_prompt = \"\"\"\n",
    "You are an expert evaluator tasked with assessing whether an LLM-generated answer effectively addresses and resolves a user's question.\n",
    "\n",
    "[Your task]\n",
    "    - Carefully analyze the user's question to understand its core intent and requirements.\n",
    "    - Determine if the LLM-generated answer sufficiently resolves the question.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - 관련성: 답변이 질문과 직접적으로 관련되어야 합니다.\n",
    "    - 완전성: 질문의 모든 측면이 다뤄져야 합니다.\n",
    "    - 정확성: 제공된 정보가 정확하고 최신이어야 합니다.\n",
    "    - 명확성: 답변이 명확하고 이해하기 쉬워야 합니다.\n",
    "    - 구체성: 질문의 요구 사항에 맞는 상세한 답변이어야 합니다.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer effectively resolves the question.\n",
    "    - 'no': The answer fails to sufficiently resolve the question or lacks crucial elements.\n",
    "\n",
    "Your evaluation plays a critical role in ensuring the quality and effectiveness of AI-generated responses. Strive for balanced and thoughtful assessments.\n",
    "\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"[User question]\\n{question}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader_binary = answer_prompt | structured_llm_BinaryGradeAnswer\n",
    "# print(\"Question:\", question)\n",
    "# print(\"Generation:\", generation)\n",
    "# answer_score = answer_grader_binary.invoke({\"question\": question, \"generation\": generation})\n",
    "# print(f\"답변 평가: {answer_score}\")\n",
    "\n",
    "\n",
    "print(\"\\n# (5) Question Re-writer\\n\")\n",
    "# (5) Question Re-writer\n",
    "def rewrite_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    입력 질문을 벡터 검색에 최적화된 형태로 재작성한다.\n",
    "    \"\"\"\n",
    "    local_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert question re-writer. Your task is to convert input questions into optimized versions \n",
    "    for vectorstore retrieval. Analyze the input carefully and focus on capturing the underlying semantic \n",
    "    intent and meaning. Your goal is to create a question that will lead to more effective and relevant \n",
    "    document retrieval.\n",
    "\n",
    "    [Guidelines]\n",
    "        1. Identify and emphasize core concepts and key subjects.\n",
    "        2. Expand abbreviations or ambiguous terms.\n",
    "        3. Include synonyms or related terms that might appear in relevant documents.\n",
    "        4. Maintain the original intent and scope.\n",
    "        5. For complex questions, break them down into simpler, focused sub-questions.\n",
    "    \"\"\"\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"[Initial question]\\n{question}\\n\\n[Improved question]\\n\")\n",
    "    ])\n",
    "    question_rewriter = re_write_prompt | local_llm | StrOutputParser()\n",
    "    rewritten_question = question_rewriter.invoke({\"question\": question})\n",
    "    return rewritten_question\n",
    "\n",
    "print(\"\\n# (6) Generation Evaluation & Decision Nodes\\n\")\n",
    "# (6) Generation Evaluation & Decision Nodes\n",
    "def grade_generation_self(state: \"SelfRagOverallState\") -> str:\n",
    "    print(\"--- 답변 평가 (생성) ---\")\n",
    "    print(f\"--- 생성된 답변: {state['generation']} ---\")\n",
    "    if state['num_generations'] > 2:\n",
    "        print(\"--- 생성 횟수 초과, 종료 -> end ---\")\n",
    "        return \"end\"\n",
    "    # 평가를 위한 문서 텍스트 구성\n",
    "    print(\"--- 답변 할루시네이션 평가 ---\")\n",
    "    docs_text = \"\\n\\n\".join([d.page_content for d in state['documents']])\n",
    "    hallucination_grade = hallucination_grader.invoke({\n",
    "        \"documents\": docs_text,\n",
    "        \"generation\": state['generation']\n",
    "    })\n",
    "    if hallucination_grade.binary_score == \"yes\":\n",
    "        relevance_grade = retrieval_grader_binary.invoke({\n",
    "            \"question\": state['question'],\n",
    "            \"document\": state['filtered_documents'],\n",
    "            \"generation\": state['generation']\n",
    "        })\n",
    "        print(\"--- 답변-질문 관련성 평가 ---\")\n",
    "        if relevance_grade.binary_score == \"yes\":\n",
    "            print(\"--- 생성된 답변이 질문을 잘 해결함 ---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"--- 답변 관련성이 부족 -> transform_query ---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "def decide_to_generate_self(state: \"SelfRagOverallState\") -> str:\n",
    "    print(\"--- 평가된 문서 분석 ---\")\n",
    "    if state['num_generations'] > 1:\n",
    "        print(\"--- 생성 횟수 초과, 생성 결정 ---\")\n",
    "        return \"generate\"\n",
    "    # 여기서는 필터링된 문서가 존재하는지 확인\n",
    "    if not state['filtered_documents']:\n",
    "        print(\"--- 관련 문서 없음 -> transform_query ---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"--- 관련 문서 존재 -> generate ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "# (7) RoutingDecision \n",
    "class RoutingDecision(BaseModel):\n",
    "    \"\"\"Determines whether a user question should be routed to document search or LLM fallback.\"\"\"\n",
    "    route: Literal[\"search_data\",\"llm_fallback\"] = Field(\n",
    "        description=\"Classify the question as 'search_data' (financial) or 'llm_fallback' (general)\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec1e26",
   "metadata": {},
   "source": [
    "##### 6. 상태 정의 및 노드 함수 (전체 Adaptive 체인)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29163d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 통합: SelfRagOverallState (질문, 생성, 원본 문서, 필터 문서, 생성 횟수)\n",
    "#TODO\n",
    "\n",
    "# 메인 그래프 상태 정의\n",
    "class SelfRagOverallState(TypedDict):\n",
    "    \"\"\"\n",
    "    Adaptive Self-RAG 체인의 전체 상태를 관리    \n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: Annotated[List[str], add]\n",
    "    routing_decision: str \n",
    "    num_generations: int\n",
    "    documents: List[Document]\n",
    "    filtered_documents: List[Document]\n",
    "    history: List[Tuple[str,str]] # (user, bot) 메시지 쌍 저장용\n",
    "\n",
    "def initialize_state() -> SelfRagOverallState:\n",
    "    \"\"\"Create a new state with proper initialization of all fields\"\"\"\n",
    "    return {\n",
    "        \"question\": \"\",\n",
    "        \"generation\": [],\n",
    "        \"routing_decision\": \"\",\n",
    "        \"num_generations\": 0,\n",
    "        \"documents\": [],\n",
    "        \"filtered_documents\": [],\n",
    "        \"history\": []\n",
    "    }    \n",
    "# 새로운 재작성 전용 LLM 체인 - 히스토리 답변이 있는 경우 이전 대화 맥락에 맞게 질문을 수정하여 문서 서치하기 위함\n",
    "def contextualize_query(state: SelfRagOverallState) -> dict:\n",
    "    # 최근 3턴 히스토리 추출\n",
    "    recent = state['history'][-3:]\n",
    "    hist_block = \"\\n\".join(f\"User: {u}\\nAssistant: {a}\" for u,a in recent)\n",
    "    payload = {\"history\": hist_block, \"question\": state['question']}\n",
    "    improved = question_rewriter_chain.invoke(payload)\n",
    "    return {\"question\": improved}\n",
    "\n",
    "rewrite_input = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 금융 상품 챗봇 AI와 유저의 대화 히스토리를 기반으로 마지막 유저의 질문을 분석하여 금융상품 추천 RAG 시스템이 문서를 잘 찾을 수 있게 질문을 구체적으로 재작성하세요.\"\n",
    "    \"재작성된 질문은 길이가 너무 길어지지 않게 하며, 유저가 원히는 핵심이 무엇인지 명확하게 들어나는 문자이어야 합니다.\"\n",
    "    \"적용되는 RAG의 서치알고리즘은 백터유사도를 기반으로 하기 때문에 이를 고려하여 질문을 재작성하세요.\"\n",
    "    \"만약 [History]에 아무것도 없거나 유저의 마지막 질의가 맥락상 금융상품과 관련된 것이 아니라면 유저의 질문을 그대로 작성하세요.\"),\n",
    "    (\"system\", \"[History]\\n{history}\"),\n",
    "    (\"human\", \"[Question]\\n{question}\\n\\n[Improved Question]\\n\"),\n",
    "])\n",
    "question_rewriter_chain = rewrite_input | llm | StrOutputParser()\n",
    "\n",
    "# 질문 재작성 노드 (변경 후 검색 루프)\n",
    "def transform_query_self(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- 질문 개선 ---\")\n",
    "    new_question = rewrite_question(state['question'])\n",
    "    print(f\"--- 개선된 질문 : \\n{new_question} \")\n",
    "    new_count = state['num_generations'] + 1\n",
    "    print(f\"num_generations : {new_count}\")\n",
    "    return {\"question\": new_question, \"num_generations\": new_count}\n",
    "\n",
    "# 답변 생성 노드 (서브 그래프로부터 받은 필터 문서 우선 사용, 이전 대화를 참고 할 수 있도록 수정)\n",
    "def format_chat_history(history):\n",
    "    messages = []\n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append((\"human\", user_msg))\n",
    "        messages.append((\"ai\", assistant_msg))\n",
    "    return messages\n",
    "\n",
    "generate_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"\"\"\n",
    "[Your task]\n",
    "You are a financial product expert and consultant who always responds in Korean.\n",
    "Analyze the user query and the given financial product data to recommend the most suitable product.\n",
    "Use the conversation history to maintain context. Rely only on the provided documents and history.\n",
    "\n",
    "[Instructions]\n",
    "1. 질문과 관련된 정보를 문맥에서 신중하게 확인합니다.\n",
    "2. 답변에 질문과 직접 관련된 정보만 사용합니다.\n",
    "3. 문맥에 명시되지 않은 내용에 대해 추측하지 않습니다.\n",
    "4. 불필요한 정보를 피하고, 명확하게 작성합니다.\n",
    "5. 문맥에서 정확한 답변을 생성할 수 없다면 마지막에 \"더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.\"를 추가합니다.     \n",
    "\"\"\".strip()),\n",
    "    (\"system\", \"[Context]\\n{context}\"),\n",
    "    (\"system\", \"[History]\\n{formatted_history}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def generate_self(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- 답변 생성 (히스토리 포함) ---\")\n",
    "    \n",
    "    # 최근 대화 제한\n",
    "    recent_history = state[\"history\"][10:] if len(state[\"history\"]) > 5 else state[\"history\"][:5]\n",
    "\n",
    "    # 대화 히스토리 포맷팅\n",
    "    formatted_history = \"\"\n",
    "    for user_msg, assistant_msg in recent_history:\n",
    "        formatted_history += f\"User: {user_msg}\\nAssistant: {assistant_msg}\\n\\n\"\n",
    "\n",
    "    # 2) context 직렬화\n",
    "    docs = state['filtered_documents'] or state['documents']\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs) if docs else \"관련 문서 없음\"\n",
    "\n",
    "    # 3) 프롬프트에 값 넣고 LLM 호출\n",
    "    chain = generate_template  | llm | StrOutputParser()\n",
    "    out = chain.invoke({\n",
    "        \"formatted_history\": formatted_history,\n",
    "        \"context\": context,\n",
    "        \"question\": state[\"question\"],\n",
    "    })\n",
    "    answer: str = out\n",
    "\n",
    "    # 4) 상태 업데이트\n",
    "    state[\"num_generations\"] += 1\n",
    "    state[\"generation\"] = answer\n",
    "\n",
    "    return {\n",
    "        \"generation\": [answer],\n",
    "        \"num_generations\": state[\"num_generations\"],\n",
    "    }\n",
    "\n",
    "\n",
    "structured_llm_RoutingDecision = llm.with_structured_output(RoutingDecision)\n",
    "\n",
    "question_router_system  = \"\"\"\n",
    "You are an AI assistant that routes user questions to the appropriate processing path.\n",
    "Return one of the following labels:\n",
    "- search_data\n",
    "- llm_fallback\n",
    "\"\"\"\n",
    "\n",
    "question_router_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", question_router_system),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "question_router = question_router_prompt | structured_llm_RoutingDecision\n",
    "\n",
    "# question route 노드 \n",
    "def route_question_adaptive(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- 질문 판단 (일반 or 금융) ---\")\n",
    "    print(f\"질문: {state['question']}\")\n",
    "    decision = question_router.invoke({\"question\": state['question']})\n",
    "    print(\"routing_decision:\", decision.route)\n",
    "    return {\"routing_decision\": decision.route}\n",
    "\n",
    "# question route 분기 함수 \n",
    "def route_question_adaptive_self(state: SelfRagOverallState) -> str:\n",
    "    \"\"\"\n",
    "    질문 분석 및 라우팅: 사용자의 질문을 분석하여 '금융질문'인지 '일반질문'인지 판단\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if state['routing_decision'] == \"llm_fallback\":\n",
    "            print(\"--- 일반질문으로 라우팅 ---\")\n",
    "            return \"llm_fallback\"\n",
    "        else:\n",
    "            print(\"--- 금융질문으로 라우팅 ---\")\n",
    "            return \"search_data\"\n",
    "    except Exception as e:\n",
    "        print(f\"--- 질문 분석 중 Exception 발생: {e} ---\")\n",
    "        return \"llm_fallback\"\n",
    "\n",
    "\n",
    "fallback_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    You are an AI assistant helping with various topics. \n",
    "    Respond in Korean.\n",
    "    - Provide accurate and helpful information.\n",
    "    - Keep answers concise yet informative.\n",
    "    - Inform users they can ask for clarification if needed.\n",
    "    - Let users know they can ask follow-up questions if needed.\n",
    "    - End every answer with the sentence: \"저는 금융상품 질문에 특화되어 있습니다. 금융상품관련 질문을 주세요.\"\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def llm_fallback_adaptive(state: SelfRagOverallState) -> dict:\n",
    "    \"\"\"Generates a direct response using the LLM when the question is unrelated to financial products.\"\"\"\n",
    "    print(\"--- 일반 질문 Fallback (히스토리 반영) ---\")\n",
    "    \n",
    "    # 대화 히스토리 포맷팅\n",
    "    formatted_history = \"\"\n",
    "    for user_msg, assistant_msg in state[\"history\"]:\n",
    "        formatted_history += f\"User: {user_msg}\\nAssistant: {assistant_msg}\\n\\n\"\n",
    "\n",
    "\n",
    "    fallback_chain = fallback_prompt | llm | StrOutputParser()\n",
    "    out = fallback_chain.invoke({\n",
    "        \"formatted_history\": formatted_history,\n",
    "        \"question\": state[\"question\"],\n",
    "    })\n",
    "    answer: str = out\n",
    "\n",
    "    state[\"history\"].append((state[\"question\"], answer))\n",
    "    return {\"generation\": [answer]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2de9f1",
   "metadata": {},
   "source": [
    "##### 7. [서브 그래프 통합] - 병렬 검색 서브 그래프 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe00bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 실제 적용할 은행명 리스트 (추가 필요시 아래에 계속 추가)\n",
    "# BANK_NAME_LIST = [\n",
    "#     \"국민\", \"국민은행\", \"수협\", \"수협은행\", \"IBK\", \"기업은행\", \"IBK기업은행\",\n",
    "#     \"신한\", \"신한은행\", \"농협\", \"NH\", \"농협은행\", \"우리\", \"우리은행\",\n",
    "#     \"하나\", \"하나은행\", \"카카오\", \"카카오뱅크\", \"카카오은행\", \"부산\", \"부산은행\"\n",
    "#     # ... 추가적으로 원하는 은행\n",
    "# ]\n",
    "\n",
    "# def extract_bank_names(query: str) -> list[str]:\n",
    "#     found = set()\n",
    "#     for bank in BANK_NAME_LIST:\n",
    "#         # \"국민\", \"국민은행\", \"IBK기업은행\" 등 중복 매칭 방지\n",
    "#         if bank in query:\n",
    "#             found.add(bank)\n",
    "#     return list(found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7707efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_banks_in_docs(documents: list[Document]) -> set[str]:\n",
    "#     banks = set()\n",
    "#     for doc in documents:\n",
    "#         bank = doc.metadata.get(\"bank\", \"\")\n",
    "#         # IBK기업은행, IBK, 기업은행 모두 매칭할 수 있도록 처리 필요시 normalize\n",
    "#         if bank:\n",
    "#             banks.add(bank)\n",
    "#     return banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7a1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "BANK_NORMALIZE = {\n",
    "    \"국민\": \"KB국민은행\",\n",
    "    \"국민은행\": \"KB국민은행\",\n",
    "    \"수협\": \"Sh수협은행\",\n",
    "    \"수협은행\": \"Sh수협은행\",\n",
    "    \"신한\": \"신한은행\",\n",
    "    \"신한은행\": \"신한은행\",\n",
    "    \"농협\": \"NH농협은행\",\n",
    "    \"NH\": \"NH농협은행\",\n",
    "    \"농협은행\": \"NH농협은행\",\n",
    "    \"우리\": \"우리은행\",\n",
    "    \"우리은행\": \"우리은행\",\n",
    "    \"IBK\": \"IBK기업은행\",\n",
    "    \"기업은행\": \"IBK기업은행\",\n",
    "    \"IBK기업은행\": \"IBK기업은행\",\n",
    "    \"하나\": \"하나은행\",\n",
    "    \"하나은행\": \"하나은행\",\n",
    "    \"카카오\": \"카카오뱅크\",\n",
    "    \"카카오뱅크\": \"카카오뱅크\",\n",
    "    \"부산\": \"부산은행\",\n",
    "    \"부산은행\": \"부산은행\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c2c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_normalize_banks(query: str) -> list[str]:\n",
    "    found = set()\n",
    "    for k in BANK_NORMALIZE:\n",
    "        if k in query:\n",
    "            found.add(BANK_NORMALIZE[k])\n",
    "    return list(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b0087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_banks_in_docs(documents: list[Document]) -> set[str]:\n",
    "    banks = set()\n",
    "    for doc in documents:\n",
    "        bank = doc.metadata.get(\"bank\", \"\")\n",
    "        # IBK기업은행, IBK, 기업은행 모두 매칭할 수 있도록 처리 필요시 normalize\n",
    "        if bank:\n",
    "            banks.add(bank)\n",
    "    return banks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d581e",
   "metadata": {},
   "source": [
    "## filter_documents 고쳐야 하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0921400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 상태 정의 (검색 서브 그래프 전용) ---\n",
    "class SearchState(TypedDict):\n",
    "    question: str\n",
    "    # generation: str\n",
    "    documents: Annotated[List[Document], add]  # 팬아웃된 각 검색 결과를 누적할 것\n",
    "    filtered_documents: List[Document]         # 관련성 평가를 통과한 문서들\n",
    "\n",
    "# ToolSearchState: SearchState에 추가 정보(datasources) 포함\n",
    "class ToolSearchState(SearchState):\n",
    "    datasources: List[str]  # 참조할 데이터 소스 목록\n",
    "\n",
    "# --- 서브그래프 노드 함수 ---\n",
    "def search_fixed_deposit_node(state: SearchState):\n",
    "    \"\"\"\n",
    "    정기예금 상품 검색 (서브 그래프)\n",
    "    \"\"\"\n",
    "    docs = search_fixed_deposit.invoke(state[\"question\"])\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "def search_demand_deposit_node(state: SearchState):\n",
    "    \"\"\"\n",
    "    입출금자유예금 상품 검색 (서브 그래프)\n",
    "    \"\"\"\n",
    "    docs = search_demand_deposit.invoke(state[\"question\"])\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "def filter_documents_subgraph(state: SearchState):\n",
    "    # 1. 입력값 준비\n",
    "    question = state[\"question\"]                   # 사용자 질문\n",
    "    documents = state[\"documents\"]                 # BM25/벡터 등으로 미리 검색된 모든 문서\n",
    "    filtered_docs = []\n",
    "\n",
    "    # 2. 모든 검색 문서에 대해 LLM 관련성 평가 (싱글/멀티 모두 동일)\n",
    "    for d in documents:\n",
    "        score = retrieval_grader_binary.invoke({\n",
    "            \"question\": question,\n",
    "            \"document\": d.page_content\n",
    "        })\n",
    "        if score.binary_score == \"yes\":\n",
    "            filtered_docs.append(d)\n",
    "    # => filtered_docs에는 '이 질문에 진짜로 의미 있는 문서'만 남음\n",
    "\n",
    "    # 3. 질문에서 요구되는 은행명(엔티티) 추출 및 표준화\n",
    "    requested_banks = extract_and_normalize_banks(question)\n",
    "    # 예) 질문이 \"국민은행 예금 추천\"이면 ['KB국민은행']\n",
    "    #     질문이 \"국민, 수협, IBK기업은행 예금 추천\"이면 ['KB국민은행', 'Sh수협은행', 'IBK기업은행']\n",
    "\n",
    "    # 4. 관련성 통과된 문서에 등장한 은행명 집합 추출\n",
    "    found_banks = get_banks_in_docs(filtered_docs)\n",
    "    # 예) filtered_docs에 국민, 수협 문서만 있으면 found_banks = {'KB국민은행', 'Sh수협은행'}\n",
    "\n",
    "    # ===================== 싱글/없음 엔티티 분기 =====================\n",
    "    if len(requested_banks) <= 1:\n",
    "        # [싱글엔티티 or 엔티티 없음]\n",
    "        # - 질문에서 은행명이 1개 이하로 추출된 경우\n",
    "        # - (1) 관련성 평가만 한 결과(filtered_docs) 반환\n",
    "        # - (2) 누락 은행 보완, 재검색 등 추가 로직 \"생략\"\n",
    "        return {\"filtered_documents\": filtered_docs}\n",
    "\n",
    "    # ===================== 멀티 엔티티(2개 이상) 분기 =====================\n",
    "    # [멀티엔티티] → 커버리지 체크, 누락 보완 로직 진입\n",
    "    missing_banks = [b for b in requested_banks if b not in found_banks]\n",
    "    # 예) requested_banks = ['KB국민은행', 'Sh수협은행', 'IBK기업은행']\n",
    "    #     found_banks = {'KB국민은행', 'Sh수협은행'}\n",
    "    #     => missing_banks = ['IBK기업은행']\n",
    "\n",
    "    PRODUCT_CATEGORIES = [\"정기예금\", \"입출금자유예금\", \"적금\", \"대출\", \"MMDA\"]\n",
    "\n",
    "    # (state에 누락 은행 보완 횟수 관리용 변수 추가)\n",
    "    if \"missing_bank_retry\" not in state:\n",
    "        state[\"missing_bank_retry\"] = 0\n",
    "\n",
    "    # (이미 확보한 은행+카테고리 쌍 관리)\n",
    "    covered_pairs = set((doc.metadata.get(\"bank\"), doc.metadata.get(\"type\")) for doc in filtered_docs)\n",
    "\n",
    "    # ========== (1) 누락 은행 보완 재검색 로직 (최대 2회까지) ==========\n",
    "    if missing_banks and state[\"missing_bank_retry\"] < 2:\n",
    "        # 누락 은행마다, 각 카테고리별로 추가 검색\n",
    "        for bank in missing_banks:\n",
    "            for category in PRODUCT_CATEGORIES:\n",
    "                if (bank, category) in covered_pairs:\n",
    "                    continue  # 이미 확보된 경우 생략\n",
    "                more_docs = hybrid_core_search(question, category=category, bank=bank, top_k=2)\n",
    "                for d in more_docs:\n",
    "                    if (d.metadata.get(\"bank\"), d.metadata.get(\"type\")) in covered_pairs:\n",
    "                        continue\n",
    "                    # 관련성 평가(batch로 할 수도 있음)\n",
    "                    score = retrieval_grader_binary.invoke({\n",
    "                        \"question\": question,\n",
    "                        \"document\": d.page_content\n",
    "                    })\n",
    "                    if score.binary_score == \"yes\":\n",
    "                        filtered_docs.append(d)\n",
    "                        covered_pairs.add((bank, category))\n",
    "        state[\"missing_bank_retry\"] += 1\n",
    "        # 한 번 더 커버리지 체크 하도록(그래프 반복 등) 상태 반환\n",
    "        return {\"filtered_documents\": filtered_docs, \"missing_bank_retry\": state[\"missing_bank_retry\"]}\n",
    "\n",
    "    # ========== (2) 보완 2회 시도 후에도 누락 은행 남을 경우 ==========\n",
    "    if missing_banks and state[\"missing_bank_retry\"] >= 2:\n",
    "        # 더 이상 보완 안 하고, 지금까지 확보한 문서들 중 상위 3개만 남김 (예시)\n",
    "        filtered_docs = filtered_docs[:3]\n",
    "        return {\"filtered_documents\": filtered_docs, \"missing_bank_retry\": state[\"missing_bank_retry\"]}\n",
    "\n",
    "    # (모든 은행이 커버됐거나, 보완이 불필요한 경우)\n",
    "    return {\"filtered_documents\": filtered_docs, \"missing_bank_retry\": state[\"missing_bank_retry\"]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_web_search_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    웹 검색 기반 금융 정보 검색 (서브 그래프)\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print('--- 웹 검색 실행 ---')\n",
    "\n",
    "    docs = web_search.invoke({\"query\": question})  # 딕셔너리 형태로 넘김\n",
    "\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 웹 정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "# --- 질문 라우팅 (서브 그래프 전용) ---\n",
    "class SubgraphToolSelector(BaseModel):\n",
    "    \"\"\"Selects the most appropriate tool for the user's question.\"\"\"\n",
    "    tool: Literal[\"search_fixed_deposit\", \"search_demand_deposit\", \"web_search\"] = Field(\n",
    "        description=\"Select one of the tools: search_fixed_deposit, search_demand_deposit or web_search based on the user's question.\"\n",
    "    )\n",
    "\n",
    "class SubgraphToolSelectors(BaseModel):\n",
    "    \"\"\"Selects all tools relevant to the user's question.\"\"\"\n",
    "    tools: List[SubgraphToolSelector] = Field(\n",
    "        description=\"Select one or more tools: search_fixed_deposit, search_demand_deposit or web_search based on the user's question.\"\n",
    "    )\n",
    "\n",
    "structured_llm_SubgraphToolSelectors = llm.with_structured_output(SubgraphToolSelectors)\n",
    "\n",
    "subgraph_system  = dedent(\"\"\"\\\n",
    "You are an AI assistant specializing in routing user questions to the appropriate tools.\n",
    "Use the following guidelines:\n",
    "- For fixed deposit product queries, use the search_fixed_deposit tool.\n",
    "- For demand deposit product queries, use the search_demand_deposit tool.\n",
    "- For general financial or real-time information queries, or when the user explicitly mentions 'web search',\n",
    "  use the web_search tool.\n",
    "  Always choose the appropriate tools based on the user's question.\n",
    "\"\"\")\n",
    "subgraph_route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", subgraph_system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "question_tool_router = subgraph_route_prompt  | structured_llm_SubgraphToolSelectors\n",
    "\n",
    "def analyze_question_tool_search(state: ToolSearchState):\n",
    "    \"\"\"\n",
    "    질문 분석 및 라우팅: 사용자의 질문에서 참조할 데이터 소스 결정\n",
    "    \"\"\"\n",
    "    print(\"--- 질문 라우팅 ---\")\n",
    "    question = state[\"question\"]\n",
    "    result = question_tool_router.invoke({\"question\": question})\n",
    "    datasources = [tool.tool for tool in result.tools]\n",
    "    return {\"datasources\": datasources}\n",
    "\n",
    "def route_datasources_tool_search(state: ToolSearchState) -> Sequence[str]:\n",
    "    \"\"\"\n",
    "    라우팅 결과에 따라 실행할 검색 노드를 결정 (병렬로 팬아웃)\n",
    "    \"\"\"\n",
    "    datasources = set(state['datasources'])\n",
    "\n",
    "    # 명확히 하나만 선택된 경우\n",
    "    if datasources == {'search_fixed_deposit'}:\n",
    "        return ['search_fixed_deposit']\n",
    "    elif datasources == {'search_demand_deposit'}:\n",
    "        return ['search_demand_deposit']\n",
    "    elif datasources == {'web_search'}:\n",
    "        return ['web_search']\n",
    "\n",
    "    # 도구가 전부 실행되거나 애매모호할 때는 도구 전부 실행\n",
    "    return ['search_fixed_deposit', 'search_demand_deposit', 'web_search']\n",
    "\n",
    "\n",
    "# --- 서브 그래프 빌더 구성 ---\n",
    "search_builder = StateGraph(ToolSearchState)\n",
    "\n",
    "# 노드 추가\n",
    "search_builder.add_node(\"analyze_question\", analyze_question_tool_search)\n",
    "search_builder.add_node(\"search_fixed_deposit\", search_fixed_deposit_node)   # wapper 함수 말고 직접 invoke 함수 사용하는 것으로 수정\n",
    "search_builder.add_node(\"search_demand_deposit\", search_demand_deposit_node) # 마찬가지로 함께\n",
    "search_builder.add_node(\"web_search\", search_web_search_subgraph)\n",
    "search_builder.add_node(\"filter_documents\", filter_documents_subgraph)\n",
    "\n",
    "# 엣지 구성\n",
    "search_builder.add_edge(START, \"analyze_question\")\n",
    "search_builder.add_conditional_edges(\n",
    "    \"analyze_question\",\n",
    "    route_datasources_tool_search,\n",
    "    {\n",
    "        \"search_fixed_deposit\": \"search_fixed_deposit\",\n",
    "        \"search_demand_deposit\": \"search_demand_deposit\",\n",
    "        \"web_search\": \"web_search\"\n",
    "    }\n",
    ")\n",
    "# 두 검색 노드 모두 실행한 후 각각의 결과는 filter_documents로 팬인(fan-in) 처리\n",
    "search_builder.add_edge(\"search_fixed_deposit\", \"filter_documents\")\n",
    "search_builder.add_edge(\"search_demand_deposit\", \"filter_documents\")\n",
    "search_builder.add_edge(\"web_search\", \"filter_documents\")\n",
    "search_builder.add_edge(\"filter_documents\", END)\n",
    "\n",
    "# 서브 그래프 컴파일\n",
    "tool_search_graph = search_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61732ee6",
   "metadata": {},
   "source": [
    "##### 8. [전체 그래프와 결합] - Self-RAG Overall Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a2ed831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 그래프 빌더 (rag_builder) 구성\n",
    "rag_builder = StateGraph(SelfRagOverallState)\n",
    "\n",
    "# 노드 추가: 검색 서브 그래프, 생성, 질문 재작성 등\n",
    "rag_builder.add_node(\"contextualize_query\", contextualize_query)\n",
    "rag_builder.add_node(\"route_question\", route_question_adaptive)\n",
    "rag_builder.add_node(\"llm_fallback\", llm_fallback_adaptive)\n",
    "rag_builder.add_node(\"search_data\", tool_search_graph)         # 서브 그래프로 병렬 검색 및 필터링 수행\n",
    "rag_builder.add_node(\"generate\", generate_self)                # 답변 생성 노드\n",
    "rag_builder.add_node(\"transform_query\", transform_query_self)  # 질문 개선 노드\n",
    "\n",
    "# 전체 그래프 엣지 구성\n",
    "rag_builder.add_edge(START, \"contextualize_query\")\n",
    "rag_builder.add_edge(\"contextualize_query\", \"route_question\")\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"route_question\",\n",
    "    route_question_adaptive_self, \n",
    "    {\n",
    "        \"llm_fallback\": \"llm_fallback\",\n",
    "        \"search_data\": \"search_data\"\n",
    "    }\n",
    ")\n",
    "rag_builder.add_edge(\"llm_fallback\", END)\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"search_data\",\n",
    "    decide_to_generate_self, \n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    }\n",
    ")\n",
    "rag_builder.add_edge(\"transform_query\", \"search_data\")\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_self,\n",
    "    {\n",
    "        \"not supported\": \"generate\",      # 환각 발생 시 재생성\n",
    "        \"not useful\": \"transform_query\",  # 관련성 부족 시 질문 재작성 후 재검색\n",
    "        \"useful\": END,\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# MemorySaver 인스턴스 생성 (대화 상태를 저장할 in-memory 키-값 저장소)\n",
    "memory = MemorySaver()\n",
    "adaptive_self_rag_memory = rag_builder.compile(checkpointer=memory)\n",
    "# adaptive_self_rag = rag_builder.compile()\n",
    "\n",
    "# 그래프 파일 저장하기\n",
    "# display(Image(adaptive_self_rag.get_graph().draw_mermaid_png()))\n",
    "with open(\"adaptive_self_rag_memory.mmd\", \"w\") as f:\n",
    "    f.write(adaptive_self_rag_memory.get_graph(xray=True).draw_mermaid()) # 저장된 mmd 파일에서 코드 복사 후 https://mermaid.live 에 붙여넣기.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c1d55",
   "metadata": {},
   "source": [
    "# 테스트 단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fe0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_bank_coverage(filter_func, question: str):\n",
    "#     \"\"\"\n",
    "#     filter_func: filter_documents_subgraph 함수 (또는 동일한 시그니처 함수)\n",
    "#     question: 테스트용 사용자 질의\n",
    "#     \"\"\"\n",
    "#     # SearchState 형태 맞게 준비\n",
    "#     # 예시: documents는 BM25+벡터 초기 검색 결과를 simulate\n",
    "#     # 여기서는 그냥 search_fixed_deposit, search_demand_deposit 두 가지 호출 결과를 합침\n",
    "#     docs_fixed = search_fixed_deposit.invoke(question)\n",
    "#     docs_demand = search_demand_deposit.invoke(question)\n",
    "#     state = {\n",
    "#         \"question\": question,\n",
    "#         \"documents\": docs_fixed + docs_demand,\n",
    "#         \"filtered_documents\": [],  # 실제 filter에서 생성됨\n",
    "#     }\n",
    "#     # 개발자 전용 테스트 print/log\n",
    "#     print(\"\\n=== [BANK COVERAGE TEST] ===\")\n",
    "#     print(\"테스트 질의:\", question)\n",
    "#     print(\"1차 검색된 문서 은행들:\", get_banks_in_docs(state[\"documents\"]))\n",
    "#     result = filter_func(state)\n",
    "#     covered_banks = get_banks_in_docs(result[\"filtered_documents\"])\n",
    "#     print(\"최종 커버된 은행:\", covered_banks)\n",
    "#     # 원래 질의에서 요구한 은행과 비교\n",
    "#     requested_banks = extract_bank_names(question)\n",
    "#     print(\"요구된 은행:\", requested_banks)\n",
    "#     missing_banks = [b for b in requested_banks if b not in covered_banks]\n",
    "#     if missing_banks:\n",
    "#         print(\"❌ 누락된 은행:\", missing_banks)\n",
    "#     else:\n",
    "#         print(\"✅ 모든 은행 커버 완료!\")\n",
    "#     print(\"=\"*30)\n",
    "#     # 필요하면 개발자만 보는 로그파일 기록도 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19a96845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bank_coverage(filter_func, question: str):\n",
    "    docs_fixed = search_fixed_deposit.invoke(question)\n",
    "    docs_demand = search_demand_deposit.invoke(question)\n",
    "    state = {\n",
    "        \"question\": question,\n",
    "        \"documents\": docs_fixed + docs_demand,\n",
    "        \"filtered_documents\": [],\n",
    "    }\n",
    "    print(\"\\n=== [BANK COVERAGE TEST] ===\")\n",
    "    print(\"테스트 질의:\", question)\n",
    "    print(\"1차 검색된 문서 은행들:\", get_banks_in_docs(state[\"documents\"]))\n",
    "    result = filter_func(state)\n",
    "    covered_banks = get_banks_in_docs(result[\"filtered_documents\"])\n",
    "    print(\"최종 커버된 은행:\", covered_banks)\n",
    "    requested_banks = extract_and_normalize_banks(question)\n",
    "    print(\"요구된 은행:\", requested_banks)\n",
    "    missing_banks = [b for b in requested_banks if b not in covered_banks]\n",
    "    if missing_banks:\n",
    "        print(\"❌ 누락된 은행:\", missing_banks)\n",
    "    else:\n",
    "        print(\"✅ 모든 은행 커버 완료!\")\n",
    "    print(\"=\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec9bfde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [BANK COVERAGE TEST] ===\n",
      "테스트 질의: 국민, 수협, 농협은행의 예금상품 추천해줘\n",
      "1차 검색된 문서 은행들: {'NH농협은행', 'KDB산업은행'}\n",
      "최종 커버된 은행: {'Sh수협은행', 'NH농협은행'}\n",
      "요구된 은행: ['KB국민은행', 'NH농협은행', 'Sh수협은행']\n",
      "❌ 누락된 은행: ['KB국민은행']\n",
      "==============================\n",
      "\n",
      "=== [BANK COVERAGE TEST] ===\n",
      "테스트 질의: 신한, 농협, 하나은행의 입출금 상품 뭐가 좋아?\n",
      "1차 검색된 문서 은행들: {'하나은행'}\n",
      "최종 커버된 은행: {'NH농협은행', '하나은행'}\n",
      "요구된 은행: ['하나은행', '신한은행', 'NH농협은행']\n",
      "❌ 누락된 은행: ['신한은행']\n",
      "==============================\n",
      "\n",
      "=== [BANK COVERAGE TEST] ===\n",
      "테스트 질의: 카카오, 경남은행, 하나은행의 적금 상품 알려줘\n",
      "1차 검색된 문서 은행들: {'iM뱅크(구 대구은행)', 'BNK경남은행', 'Sh수협은행', '우리은행', 'NH농협은행', '카카오뱅크'}\n",
      "최종 커버된 은행: {'BNK경남은행', '카카오뱅크'}\n",
      "요구된 은행: ['하나은행', '카카오뱅크']\n",
      "❌ 누락된 은행: ['하나은행']\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 예시 질의(유저는 절대 안 봄)\n",
    "    test_questions = [\n",
    "        \"국민, 수협, 농협은행의 예금상품 추천해줘\",\n",
    "        \"신한, 농협, 하나은행의 입출금 상품 뭐가 좋아?\",\n",
    "        \"카카오, 경남은행, 하나은행의 적금 상품 알려줘\"\n",
    "    ]\n",
    "    for q in test_questions:\n",
    "        test_bank_coverage(filter_documents_subgraph, q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 선택된 도구: ['web_search']\n",
      "\n",
      "🟡 실행 중: web_search_subgraph\n",
      "--- 웹 검색 실행 ---\n",
      "\n",
      "📄 웹 검색 결과:\n",
      "\n",
      "🔹 결과 1\n",
      "내용: <Document href=\"https://ko.tradingeconomics.com/united-states/interest-rate\"/>\n",
      "연방준비제도는 2025년 5월 세 번째 연속 회의에서 자금 금리를 4.25%–4.50%로 유지했으며, 이는 기대에 부합하는 결정으로, 트럼프 대통령의 관세가 인플레이션을 상승\n",
      "</Document> ...\n",
      "메타데이터: {'source': 'web search', 'url': 'https://ko.tradingeconomics.com/united-states/interest-rate'}\n",
      "\n",
      "🔹 결과 2\n",
      "내용: <Document href=\"https://naver.me/GdyP4ZaO\"/>\n",
      "... 2025 년 5 월 FOMC는 기준금리를 4.25 ~ 4.50 %로 동결하고, 자산축소(QT)를 기존 속도로 이어가기로 했습니다. 주목할 점은 파월이 “인플레이션과\n",
      "</Document> ...\n",
      "메타데이터: {'source': 'web search', 'url': 'https://naver.me/GdyP4ZaO'}\n"
     ]
    }
   ],
   "source": [
    "# # ✅ 1. 질문 정의\n",
    "# example_state = {\n",
    "#     \"question\": \"2025년 5월 최신 금리 정보를 web search로 찾아줘\"\n",
    "# }\n",
    "\n",
    "# # ✅ 2. 툴 선택 (LLM이 판단)\n",
    "# result = question_tool_router.invoke({\"question\": example_state[\"question\"]})\n",
    "\n",
    "# # ✅ 3. 선택된 툴 출력\n",
    "# selected_tools = [tool.tool for tool in result.tools]\n",
    "# print(\"✅ 선택된 도구:\", selected_tools)\n",
    "\n",
    "# # ✅ 4. web_search가 선택됐을 경우 실행\n",
    "# if \"web_search\" in selected_tools:\n",
    "#     print(\"\\n🟡 실행 중: web_search_subgraph\")\n",
    "\n",
    "#     # SearchState 구조와 일치시켜 호출\n",
    "#     search_result = search_web_search_subgraph({\"question\": example_state[\"question\"]})\n",
    "\n",
    "#     print(\"\\n📄 웹 검색 결과:\")\n",
    "#     for i, doc in enumerate(search_result[\"documents\"], start=1):\n",
    "#         print(f\"\\n🔹 결과 {i}\")\n",
    "#         print(\"내용:\", doc.page_content[:300], \"...\")\n",
    "#         print(\"메타데이터:\", doc.metadata)\n",
    "\n",
    "# else:\n",
    "#     print(\"❌ web_search는 선택되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bf5fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pdf_link 삽입 보조함수\n",
    "def postprocess_answer(answer: str, docs: List[Document]) -> str:\n",
    "    for doc in docs:\n",
    "        pdf = doc.metadata.get(\"pdf_link\")\n",
    "        if pdf:\n",
    "            if \"상품설명서\" not in answer:\n",
    "                answer += f\"\\n\\n [상품설명서 PDF 보기]({pdf})\"\n",
    "            break\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3baa60",
   "metadata": {},
   "source": [
    "##### 9. Gradio Chatbot 구성 및 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d12fdd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://35085cb6b42b38a7c6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://35085cb6b42b38a7c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 질문 판단 (일반 or 금융) ---\n",
      "질문: 수협은행, 농협, 국민은행의 예금 상품을 추천해 주세요.\n",
      "routing_decision: search_data\n",
      "--- 금융질문으로 라우팅 ---\n",
      "--- 질문 라우팅 ---\n",
      "--- 웹 검색 실행 ---\n",
      "--- 평가된 문서 분석 ---\n",
      "--- 관련 문서 존재 -> generate ---\n",
      "--- 답변 생성 (히스토리 포함) ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: ['수협은행의 예금 상품으로는 \"Sh평생주거래우대통장\"이 있습니다. 이 상품은 기본금리가 0.05%이며, 우대조건을 충족할 경우 최고금리 0.2%를 적용받을 수 있습니다. 매일 최종잔액에 따라 금리가 달라지며, 가입은 영업점이나 스마트뱅킹을 통해 가능합니다.\\n\\n농협은행의 예금 상품으로는 \"NH고향사랑기부예금\"이 있으며, 최고금리는 3.90%입니다. 이 상품은 기부를 통해 지역사회에 기여할 수 있는 장점이 있습니다.\\n\\nKB국민은행의 \"KB마이핏통장\"은 기본금리가 0.1%이고, 우대금리를 포함한 최고금리는 1.5%입니다. 이 상품은 만 18세 이상 만 38세 이하의 실명의 개인이 가입할 수 있으며, 비상금 이율을 제공하는 조건이 있습니다.\\n\\n각 상품의 특성과 조건을 고려하여 본인에게 가장 적합한 상품을 선택하시기 바랍니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.'] ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\n",
      "--- 답변 생성 (히스토리 포함) ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: ['수협은행의 예금 상품으로는 \"Sh평생주거래우대통장\"이 있습니다. 이 상품은 기본금리가 0.05%이며, 우대조건을 충족할 경우 최고금리 0.2%를 적용받을 수 있습니다. 매일 최종잔액에 따라 금리가 달라지며, 가입은 영업점이나 스마트뱅킹을 통해 가능합니다.\\n\\n농협은행의 예금 상품으로는 \"NH고향사랑기부예금\"이 있으며, 최고금리는 3.90%입니다. 이 상품은 기부를 통해 지역사회에 기여할 수 있는 장점이 있습니다.\\n\\nKB국민은행의 \"KB마이핏통장\"은 기본금리가 0.1%이고, 우대금리를 포함한 최고금리는 1.5%입니다. 이 상품은 만 18세 이상 만 38세 이하의 실명의 개인이 가입할 수 있으며, 비상금 이율을 제공하는 조건이 있습니다.\\n\\n각 상품의 특성과 조건을 고려하여 본인에게 가장 적합한 상품을 선택하시기 바랍니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.', '수협은행의 예금 상품으로는 \"Sh평생주거래우대통장\"이 있습니다. 이 상품은 기본금리가 0.05%이며, 우대조건을 충족할 경우 최고금리 0.2%를 적용받을 수 있습니다. 월지급 방식으로 이자가 지급되며, 가입은 영업점이나 스마트뱅킹을 통해 가능합니다.\\n\\n농협은행의 예금 상품으로는 \"NH고향사랑기부예금\"이 있으며, 최고금리는 3.90%입니다. 이 상품은 기부를 통해 사회에 기여할 수 있는 장점이 있습니다.\\n\\nKB국민은행의 \"KB마이핏통장\"은 기본금리가 0.1%이고, 우대금리를 포함한 최고금리는 1.5%입니다. 이 상품은 만 18세 이상 만 38세 이하의 실명의 개인이 가입할 수 있으며, 분기별로 이자가 지급됩니다.\\n\\n각 상품의 특성과 조건을 고려하여 본인에게 가장 적합한 상품을 선택하시기 바랍니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.'] ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\n",
      "--- 답변 생성 (히스토리 포함) ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: ['수협은행의 예금 상품으로는 \"Sh평생주거래우대통장\"이 있습니다. 이 상품은 기본금리가 0.05%이며, 우대조건을 충족할 경우 최고금리 0.2%를 적용받을 수 있습니다. 매일 최종잔액에 따라 금리가 달라지며, 가입은 영업점이나 스마트뱅킹을 통해 가능합니다.\\n\\n농협은행의 예금 상품으로는 \"NH고향사랑기부예금\"이 있으며, 최고금리는 3.90%입니다. 이 상품은 기부를 통해 지역사회에 기여할 수 있는 장점이 있습니다.\\n\\nKB국민은행의 \"KB마이핏통장\"은 기본금리가 0.1%이고, 우대금리를 포함한 최고금리는 1.5%입니다. 이 상품은 만 18세 이상 만 38세 이하의 실명의 개인이 가입할 수 있으며, 비상금 이율을 제공하는 조건이 있습니다.\\n\\n각 상품의 특성과 조건을 고려하여 본인에게 가장 적합한 상품을 선택하시기 바랍니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.', '수협은행의 예금 상품으로는 \"Sh평생주거래우대통장\"이 있습니다. 이 상품은 기본금리가 0.05%이며, 우대조건을 충족할 경우 최고금리 0.2%를 적용받을 수 있습니다. 월지급 방식으로 이자가 지급되며, 가입은 영업점이나 스마트뱅킹을 통해 가능합니다.\\n\\n농협은행의 예금 상품으로는 \"NH고향사랑기부예금\"이 있으며, 최고금리는 3.90%입니다. 이 상품은 기부를 통해 사회에 기여할 수 있는 장점이 있습니다.\\n\\nKB국민은행의 \"KB마이핏통장\"은 기본금리가 0.1%이고, 우대금리를 포함한 최고금리는 1.5%입니다. 이 상품은 만 18세 이상 만 38세 이하의 실명의 개인이 가입할 수 있으며, 분기별로 이자가 지급됩니다.\\n\\n각 상품의 특성과 조건을 고려하여 본인에게 가장 적합한 상품을 선택하시기 바랍니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.', '수협은행의 예금 상품으로는 \"Sh평생주거래우대통장\"이 있습니다. 이 상품은 기본금리가 0.05%이며, 우대조건을 충족할 경우 최고금리 0.2%를 적용받을 수 있습니다. 매일 최종잔액에 따라 금리가 달라지며, 가입은 영업점이나 스마트뱅킹을 통해 가능합니다.\\n\\n농협은행의 예금 상품으로는 \"NH고향사랑기부예금\"이 있으며, 최고금리는 3.90%입니다. 이 상품은 기부를 통해 지역사회에 기여할 수 있는 장점이 있습니다.\\n\\n국민은행의 \"KB마이핏통장\"은 기본금리가 0.1%이고, 우대조건을 충족할 경우 최고금리 1.5%를 적용받을 수 있습니다. 이 통장은 만 18세 이상 만 38세 이하의 실명의 개인이 가입할 수 있으며, 다양한 입금 실적에 따라 비상금 이율을 제공받을 수 있습니다.\\n\\n각 상품의 특성을 고려하여 본인의 상황에 맞는 상품을 선택하시면 좋겠습니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.'] ---\n",
      "--- 생성 횟수 초과, 종료 -> end ---\n",
      "--- History 확인 ---\n",
      "[('수협은행이랑 농협이랑 국민은행 각 은행들의 예금 상품 추천해줘', '수협은행의 예금 상품으로는 \"Sh평생주거래우대통장\"이 있습니다. 이 상품은 기본금리가 0.05%이며, 우대조건을 충족할 경우 최고금리 0.2%를 적용받을 수 있습니다. 매일 최종잔액에 따라 금리가 달라지며, 가입은 영업점이나 스마트뱅킹을 통해 가능합니다.\\n\\n농협은행의 예금 상품으로는 \"NH고향사랑기부예금\"이 있으며, 최고금리는 3.90%입니다. 이 상품은 기부를 통해 지역사회에 기여할 수 있는 장점이 있습니다.\\n\\n국민은행의 \"KB마이핏통장\"은 기본금리가 0.1%이고, 우대조건을 충족할 경우 최고금리 1.5%를 적용받을 수 있습니다. 이 통장은 만 18세 이상 만 38세 이하의 실명의 개인이 가입할 수 있으며, 다양한 입금 실적에 따라 비상금 이율을 제공받을 수 있습니다.\\n\\n각 상품의 특성을 고려하여 본인의 상황에 맞는 상품을 선택하시면 좋겠습니다. 더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.\\n\\n [상품설명서 PDF 보기](http://localhost:8000/pdf/demand_deposit/수협/수협_평생주거래우대통장.pdf)')]\n"
     ]
    }
   ],
   "source": [
    "# 챗봇 클래스\n",
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.thread_id = str(uuid.uuid4())\n",
    "\n",
    "    def chat(self, message: str, history: List[Tuple[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        입력 메시지와 대화 이력을 기반으로 Adaptive Self-RAG 체인을 호출하고,\n",
    "        응답을 반환합니다.\n",
    "        \"\"\"\n",
    "        config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
    "        state = initialize_state()\n",
    "        state[\"question\"] = message\n",
    "        \n",
    "        # history가 있으면 추가\n",
    "        if history:\n",
    "            state[\"history\"] = history\n",
    "        \n",
    "        result = adaptive_self_rag_memory.invoke(state, config=config)\n",
    "\n",
    "        gen_list = result.get(\"generation\", [])\n",
    "        docs = result.get(\"filtered_documents\", [])\n",
    "\n",
    "        if not gen_list:\n",
    "            bot_response = \"죄송합니다. 답변을 생성할 수 없습니다.\"\n",
    "        else:\n",
    "            raw_answer = gen_list[-1]\n",
    "            bot_response = postprocess_answer(raw_answer, docs)\n",
    "\n",
    "        # 대화 이력 업데이트\n",
    "        state[\"history\"].append((message, bot_response))\n",
    "        print(f\"--- History 확인 ---\\n{state['history']}\")\n",
    "        return bot_response\n",
    "\n",
    "\n",
    "# 챗봇 인스턴스 생성\n",
    "chatbot = ChatBot() \n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot.chat,\n",
    "    title=\"Adaptive Self-RAG 기반 RAG 챗봇 시스템\",\n",
    "    description=\"정기예금, 입출금자유예금 상품 및 기타 질문에 답변합니다.\",\n",
    "    examples=[\n",
    "        \"정기예금 상품 중 금리가 가장 높은 것은?\",\n",
    "        \"정기예금과 입출금자유예금은 어떤 차이점이 있나요?\",\n",
    "        \"은행의 예금 상품을 추천해 주세요.\"\n",
    "    ],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "# Gradio 앱 실행: 이 파일을 메인으로 실행할 때만 띄웁니다.\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eefd7c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e4f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
