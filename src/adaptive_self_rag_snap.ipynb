{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b13a764",
   "metadata": {},
   "source": [
    "##### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2472f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface==0.1.2\n",
      "  Obtaining dependency information for langchain-huggingface==0.1.2 from https://files.pythonhosted.org/packages/9d/f8/77a303ddc492f6eed8bf0979f2bc6db4fa6eb1089c5e9f0f977dd87bc9c2/langchain_huggingface-0.1.2-py3-none-any.whl.metadata\n",
      "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.30.2)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.3.58)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface==0.1.2)\n",
      "  Obtaining dependency information for sentence-transformers>=2.6.0 from https://files.pythonhosted.org/packages/45/2d/1151b371f28caae565ad384fdc38198f1165571870217aedda230b9d7497/sentence_transformers-4.1.0-py3-none-any.whl.metadata\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.21.1)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface==0.1.2)\n",
      "  Obtaining dependency information for transformers>=4.39.0 from https://files.pythonhosted.org/packages/a9/b6/5257d04ae327b44db31f15cce39e6020cc986333c715660b1315a9724d82/transformers-4.51.3-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.13.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.11.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.15.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (11.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (0.5.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2025.4.26)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.6.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.3.1)\n",
      "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Installing collected packages: transformers, sentence-transformers, langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.1.2 sentence-transformers-4.1.0 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## 추가 패키지 설치\n",
    "pip install langchain-huggingface==0.1.2\n",
    "pip install langgraph==0.2.34\n",
    "pip install gradio==4.44.1\n",
    "!pip install langchain-community==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e1596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adaptive_self_rag\n",
    "금융상품(예: 정기예금, 입출금자유예금) 관련 질의에 대해:\n",
    "1. 질문 라우팅 → (금융상품 관련이면) 문서 검색 (병렬 서브 그래프) → 문서 평가 → (조건부) 질문 재작성 → 답변 생성\n",
    "   / (금융상품과 무관하면) LLM fallback을 통해 바로 답변 생성\n",
    "그리고 생성된 답변의 품질(환각, 관련성) 평가 후 필요시 재생성 또는 재작성하는 Adaptive Self-RAG 체인.\n",
    "\"\"\"\n",
    "\n",
    "#############################\n",
    "# 1. 기본 환경 및 라이브러리\n",
    "#############################\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 기타 유틸\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "from textwrap import dedent\n",
    "from operator import add\n",
    "from heapq import merge\n",
    "from typing import List, Literal, Sequence, TypedDict, Annotated, Tuple\n",
    "\n",
    "# 서치 알고리즘\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# LangChain, Chroma, LLM 관련\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Grader 평가지표용\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 그래프 관련\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Gradio 관련\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fece5f9",
   "metadata": {},
   "source": [
    "##### 2. 임베딩 및 DB설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ed524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "class LangChainSentenceTransformer(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"[INFO] Using device: {device}\")\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text, show_progress_bar=False, convert_to_numpy=True).tolist()\n",
    "\n",
    "embeddings_model_koMultitask = LangChainSentenceTransformer(\"jhgan/ko-sroberta-multitask\") # 768차원 임배딩 모델로 변경\n",
    "\n",
    "# Chroma DB 경로\n",
    "CHROMA_DIR = \"./../findata/chroma_db\"\n",
    "\n",
    "# JSON 데이터 경로\n",
    "FIXED_JSON_PATH = \"./../findata/processed_fixed_deposit.json\"\n",
    "DEMAND_JSON_PATH = \"./../findata/processed_demand_deposit.json\"\n",
    "\n",
    "# DB 이름\n",
    "FIXED_COLLECTION = \"processed_fixed_deposit\"\n",
    "DEMAND_COLLECTION = \"processed_demand_deposit\"\n",
    "\n",
    "def load_and_prepare_all_documents(json_paths: list[str]) -> list[Document]:\n",
    "    docs: list[Document] = []\n",
    "    for path in json_paths:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        for entry in data[\"documents\"]:\n",
    "            # 본문\n",
    "            content = entry[\"content\"]\n",
    "            # metadata 필드 영어 키로 통일\n",
    "            md = entry.get(\"metadata\", {})\n",
    "            bank         = entry.get(\"bank\",         md.get(\"은행\"))\n",
    "            product_name = entry.get(\"product_name\", md.get(\"상품명\"))\n",
    "            category     = entry.get(\"type\")\n",
    "            pdf_link     = md.get(\"pdf_link\")\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"id\":           entry.get(\"id\"),\n",
    "                        \"type\":         category,\n",
    "                        \"bank\":         bank,\n",
    "                        \"product_name\": product_name,\n",
    "                        \"pdf_link\":     pdf_link\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    return docs\n",
    "\n",
    "# JSON 파일 경로 리스트\n",
    "ALL_JSON = [\n",
    "    FIXED_JSON_PATH,\n",
    "    DEMAND_JSON_PATH\n",
    "]\n",
    "\n",
    "all_documents = load_and_prepare_all_documents(ALL_JSON)\n",
    "corpus = [doc.page_content.split() for doc in all_documents]\n",
    "bm25_index = BM25Okapi(corpus)\n",
    "\n",
    "# 인제스천\n",
    "vector_db = Chroma(\n",
    "    embedding_function=embeddings_model_koMultitask,\n",
    "    collection_name=\"combined_products_koMultitask\",  # 임배딩 모델에 따른 인제스천 이름 변경\n",
    "    persist_directory=CHROMA_DIR,\n",
    ")\n",
    "\n",
    "# 한 번만 인제스천\n",
    "if not vector_db._collection.count():\n",
    "    vector_db.add_documents(all_documents)\n",
    "\n",
    "def extract_bank(query: str) -> str | None:\n",
    "    m = re.search(r'([가-힣A-Za-z0-9]+(?:은행|뱅크))', query)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def extract_product(query: str) -> str | None:\n",
    "    # “~통장”, “~예금”, “~대출” 등으로 마침\n",
    "    m = re.search(r'([가-힣A-Za-z0-9]+(?:통장|예금|대출))', query)\n",
    "    return m.group(1).strip() if m else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f6ba7",
   "metadata": {},
   "source": [
    "##### 3. 서치알고리즘 및 도구(검색 함수) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef46902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _search_with_filters(query: str, filters: dict, top_k: int) -> list[Document]:\n",
    "    # 1) BM25: 전체 코퍼스에서 score 계산 → metadata 필터 적용해 top_k\n",
    "    tokenized = query.split()\n",
    "    scores = bm25_index.get_scores(tokenized)\n",
    "    idxs   = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    bm25_docs = []\n",
    "    for i in idxs:\n",
    "        d = all_documents[i]\n",
    "        if all(filters.get(k) is None or d.metadata.get(k)==filters[k] for k in filters):\n",
    "            bm25_docs.append(d)\n",
    "            if len(bm25_docs)>=top_k: break\n",
    "\n",
    "    # 2) 벡터: Chroma의 filter 파라미터 사용 (단일키 vs 다중키에 따라 $and 로 묶어서 전달)\n",
    "    meta = {k: v for k, v in filters.items() if v is not None}\n",
    "    if not meta:\n",
    "        vec_docs = vector_db.similarity_search(query, k=top_k)\n",
    "    elif len(meta) == 1:\n",
    "        # 단일 필터터\n",
    "        vec_docs = vector_db.similarity_search(query, k=top_k, filter=meta)\n",
    "    else:\n",
    "        # 여러 필터는 하나의 연산자($and)로 묶어서 넘겨야 함\n",
    "        and_list = [{k: v} for k, v in meta.items()]\n",
    "        vec_docs = vector_db.similarity_search(\n",
    "                    query, \n",
    "                    k=top_k, \n",
    "                    filter={\"$and\": and_list}\n",
    "        )\n",
    "\n",
    "    # 3) 중복 제거 병합\n",
    "    seen, merged = set(), []\n",
    "    for d in bm25_docs + vec_docs:\n",
    "        uid = d.metadata[\"id\"]\n",
    "        if uid not in seen:\n",
    "            seen.add(uid)\n",
    "            merged.append(d)\n",
    "    return merged\n",
    "\n",
    "# 하이브리드 서치 알고리즘\n",
    "def hybrid_core_search(query: str, category: str, bank: str=None, product_name: str=None, top_k: int=2) -> List[Document]:\n",
    "    # 1) 메타 필터 준비 (category 필수 포함)\n",
    "    filters = {\"type\": category}\n",
    "    if bank: \n",
    "        filters[\"bank\"] = bank\n",
    "    if product_name: \n",
    "        filters[\"product_name\"] = product_name\n",
    "\n",
    "    # 2) 필터 레벨별로 점진적 검색\n",
    "    filter_levels = [\n",
    "        filters,\n",
    "        {**filters, **{\"product_name\": None}},  # 상품명 제외\n",
    "        {**filters, **{\"bank\": None}},          # 은행 제외\n",
    "        {\"category\": category}                  # 카테고리만\n",
    "    ]\n",
    "\n",
    "    # 3) 순서대로 BM25+벡터 병렬 검색 → 결과 반환\n",
    "    for flt in filter_levels:\n",
    "        docs = _search_with_filters(query, flt, top_k=top_k)\n",
    "        if docs:\n",
    "            return docs\n",
    "    return []\n",
    "\n",
    "@tool\n",
    "def search_fixed_deposit(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Search for relevant fixed deposit (정기예금) product information using semantic similarity.\n",
    "    This tool retrieves products matching the user's query, such as interest rates or terms.\n",
    "    \"\"\"\n",
    "    bank, product = extract_bank(query), extract_product(query)\n",
    "    return hybrid_core_search(query, category=\"정기예금\", bank=bank, product_name=product)\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_demand_deposit(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Search for demand deposit (입출금자유예금) product information using semantic similarity.\n",
    "    This tool retrieves products matching the user's query, such as flexible withdrawal or interest features.\n",
    "    \"\"\"\n",
    "    bank, product = extract_bank(query), extract_product(query)\n",
    "    return hybrid_core_search(query, category=\"입출금자유예금\", bank=bank, product_name=product)\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    This tool serves as a supplementary utility for the financial product recommendation model.\n",
    "    It retrieves up-to-date external information via web search using the Tavily API, \n",
    "    especially when relevant data is not available in the local vector databases\n",
    "\n",
    "    Unlike the RAG-based tools that query embedded product databases,\n",
    "    this tool is designed to handle broader or real-time questions—such as current interest rates, financial trends,\n",
    "    or general queries outside the scope of structured deposit data.\n",
    "\n",
    "    It returns the top 2 semantically relevant documents from the web.\n",
    "    \"\"\"\n",
    "\n",
    "    tavily_search = TavilySearchResults(max_results=2)\n",
    "    docs = tavily_search.invoke(query)\n",
    "\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        formatted_docs.append(\n",
    "            Document(\n",
    "                page_content= f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>',\n",
    "                metadata={\"source\": \"web search\", \"url\": doc[\"url\"]}\n",
    "                )\n",
    "        )\n",
    "\n",
    "    if len(formatted_docs) > 0:\n",
    "        return formatted_docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "\n",
    "tools = [search_fixed_deposit, search_demand_deposit, web_search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff8277",
   "metadata": {},
   "source": [
    "##### 4. llm초기화 & 도구 바인딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bbaf7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39bb28",
   "metadata": {},
   "source": [
    "##### 5. LLM 체인 (Retrieval Grader / Answer Generator / Hallucination / Answer Graders / Question Re-writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f85bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================================================\n",
      " \n",
      "LLM 체인\n",
      "\n",
      "# (1) Retrieval Grader\n",
      "\n",
      "\n",
      "# (3) Hallucination Grader\n",
      "\n",
      "\n",
      "# (4) Answer Grader\n",
      "\n",
      "\n",
      "# (5) Question Re-writer\n",
      "\n",
      "\n",
      "# (6) Generation Evaluation & Decision Nodes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# 5. LLM 체인 (Retrieval Grader / Answer Generator / Hallucination / Answer Graders / Question Re-writer)\n",
    "#############################\n",
    "print(\"\\n===================================================================\\n \")\n",
    "print(\"LLM 체인\\n\")\n",
    "print(\"# (1) Retrieval Grader\\n\")\n",
    "\n",
    "# (1) Retrieval Grader (검색평가)\n",
    "class BinaryGradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_BinaryGradeDocuments = llm.with_structured_output(BinaryGradeDocuments)\n",
    "\n",
    "system_prompt = \"\"\"You are an expert in evaluating the relevance of search results to user queries.\n",
    "\n",
    "[Evaluation criteria]\n",
    "1. 키워드 관련성: 문서가 질문의 주요 단어나 유사어를 포함하는지 확인\n",
    "2. 의미적 관련성: 문서의 전반적인 주제가 질문의 의도와 일치하는지 평가\n",
    "3. 부분 관련성: 질문의 일부를 다루거나 맥락 정보를 제공하는 문서도 고려\n",
    "4. 답변 가능성: 직접적인 답이 아니더라도 답변 형성에 도움될 정보 포함 여부 평가\n",
    "\n",
    "[Scoring]\n",
    "- Rate 'yes' if relevant, 'no' if not\n",
    "- Default to 'no' when uncertain\n",
    "\n",
    "[Key points]\n",
    "- Consider the full context of the query, not just word matching\n",
    "- Rate as relevant if useful information is present, even if not a complete answer\n",
    "\n",
    "Your evaluation is crucial for improving information retrieval systems. Provide balanced assessments.\n",
    "\"\"\"\n",
    "# 채점 프롬프트 템플릿\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"[Retrieved document]\\n{document}\\n\\n[User question]\\n{question}\")\n",
    "])\n",
    "\n",
    "retrieval_grader_binary = grade_prompt | structured_llm_BinaryGradeDocuments\n",
    "\n",
    "# question = \"어떤 예금 상품이 있는지 설명해주세요.\"\n",
    "# print(f'\\nquestion : {question}\\n')\n",
    "# retrieved_docs = fixed_deposit_db.similarity_search(question, k=2)\n",
    "# print(f\"검색된 문서 수: {len(retrieved_docs)}\")\n",
    "# print(\"===============================================================================\")\n",
    "# print()\n",
    "\n",
    "# relevant_docs = []\n",
    "# for doc in retrieved_docs:\n",
    "#     print(\"문서:\\n\", doc.page_content)\n",
    "#     print(\"---------------------------------------------------------------------------\")\n",
    "\n",
    "#     relevance = retrieval_grader_binary.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "#     print(f\"문서 관련성: {relevance}\")\n",
    "\n",
    "#     if relevance.binary_score == 'yes':\n",
    "#         relevant_docs.append(doc)\n",
    "    \n",
    "#     print(\"===========================================================================\")\n",
    "\n",
    "\n",
    "# (2) Answer Generator (일반 RAG)\n",
    "\n",
    "# (3) Hallucination Grader\n",
    "print(\"\\n# (3) Hallucination Grader\\n\")\n",
    "\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_HradeHallucinations = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# 환각 평가를 위한 시스템 프롬프트 정의\n",
    "halluci_system_prompt = \"\"\"\n",
    "You are an expert evaluator assessing whether an LLM-generated answer is grounded in and supported by a given set of facts.\n",
    "\n",
    "[Your task]\n",
    "    - Review the LLM-generated answer.\n",
    "    - Determine if the answer is fully supported by the given facts.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - 답변에 주어진 사실이나 명확히 추론할 수 있는 정보 외의 내용이 없어야 합니다.\n",
    "    - 답변의 모든 핵심 내용이 주어진 사실에서 비롯되어야 합니다.\n",
    "    - 사실적 정확성에 집중하고, 글쓰기 스타일이나 완전성은 평가하지 않습니다.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer is factually grounded and fully supported.\n",
    "    - 'no': The answer includes information or claims not based on the given facts.\n",
    "\n",
    "Your evaluation is crucial in ensuring the reliability and factual accuracy of AI-generated responses. Be thorough and critical in your assessment.\n",
    "\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", halluci_system_prompt),\n",
    "        (\"human\", \"[Set of facts]\\n{documents}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_HradeHallucinations\n",
    "# hallucination = hallucination_grader.invoke({\"documents\": relevant_docs, \"generation\": generation})\n",
    "# print(f\"환각 평가: {hallucination}\")\n",
    "\n",
    "print(\"\\n# (4) Answer Grader\\n\")\n",
    "# (4) Answer Grader \n",
    "class BinaryGradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_BinaryGradeAnswer = llm.with_structured_output(BinaryGradeAnswer)\n",
    "grade_system_prompt = \"\"\"\n",
    "You are an expert evaluator tasked with assessing whether an LLM-generated answer effectively addresses and resolves a user's question.\n",
    "\n",
    "[Your task]\n",
    "    - Carefully analyze the user's question to understand its core intent and requirements.\n",
    "    - Determine if the LLM-generated answer sufficiently resolves the question.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - 관련성: 답변이 질문과 직접적으로 관련되어야 합니다.\n",
    "    - 완전성: 질문의 모든 측면이 다뤄져야 합니다.\n",
    "    - 정확성: 제공된 정보가 정확하고 최신이어야 합니다.\n",
    "    - 명확성: 답변이 명확하고 이해하기 쉬워야 합니다.\n",
    "    - 구체성: 질문의 요구 사항에 맞는 상세한 답변이어야 합니다.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer effectively resolves the question.\n",
    "    - 'no': The answer fails to sufficiently resolve the question or lacks crucial elements.\n",
    "\n",
    "Your evaluation plays a critical role in ensuring the quality and effectiveness of AI-generated responses. Strive for balanced and thoughtful assessments.\n",
    "\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"[User question]\\n{question}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader_binary = answer_prompt | structured_llm_BinaryGradeAnswer\n",
    "# print(\"Question:\", question)\n",
    "# print(\"Generation:\", generation)\n",
    "# answer_score = answer_grader_binary.invoke({\"question\": question, \"generation\": generation})\n",
    "# print(f\"답변 평가: {answer_score}\")\n",
    "\n",
    "\n",
    "print(\"\\n# (5) Question Re-writer\\n\")\n",
    "# (5) Question Re-writer\n",
    "def rewrite_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    입력 질문을 벡터 검색에 최적화된 형태로 재작성한다.\n",
    "    \"\"\"\n",
    "    local_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert question re-writer. Your task is to convert input questions into optimized versions \n",
    "    for vectorstore retrieval. Analyze the input carefully and focus on capturing the underlying semantic \n",
    "    intent and meaning. Your goal is to create a question that will lead to more effective and relevant \n",
    "    document retrieval.\n",
    "\n",
    "    [Guidelines]\n",
    "        1. Identify and emphasize core concepts and key subjects.\n",
    "        2. Expand abbreviations or ambiguous terms.\n",
    "        3. Include synonyms or related terms that might appear in relevant documents.\n",
    "        4. Maintain the original intent and scope.\n",
    "        5. For complex questions, break them down into simpler, focused sub-questions.\n",
    "    \"\"\"\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"[Initial question]\\n{question}\\n\\n[Improved question]\\n\")\n",
    "    ])\n",
    "    question_rewriter = re_write_prompt | local_llm | StrOutputParser()\n",
    "    rewritten_question = question_rewriter.invoke({\"question\": question})\n",
    "    return rewritten_question\n",
    "\n",
    "print(\"\\n# (6) Generation Evaluation & Decision Nodes\\n\")\n",
    "# (6) Generation Evaluation & Decision Nodes\n",
    "def grade_generation_self(state: \"SelfRagOverallState\") -> str:\n",
    "    print(\"--- 답변 평가 (생성) ---\")\n",
    "    print(f\"--- 생성된 답변: {state['generation']} ---\")\n",
    "    if state['num_generations'] > 2:\n",
    "        print(\"--- 생성 횟수 초과, 종료 -> end ---\")\n",
    "        return \"end\"\n",
    "    # 평가를 위한 문서 텍스트 구성\n",
    "    print(\"--- 답변 할루시네이션 평가 ---\")\n",
    "    docs_text = \"\\n\\n\".join([d.page_content for d in state['documents']])\n",
    "    hallucination_grade = hallucination_grader.invoke({\n",
    "        \"documents\": docs_text,\n",
    "        \"generation\": state['generation']\n",
    "    })\n",
    "    if hallucination_grade.binary_score == \"yes\":\n",
    "        relevance_grade = retrieval_grader_binary.invoke({\n",
    "            \"question\": state['question'],\n",
    "            \"document\": state['filtered_documents'],\n",
    "            \"generation\": state['generation']\n",
    "        })\n",
    "        print(\"--- 답변-질문 관련성 평가 ---\")\n",
    "        if relevance_grade.binary_score == \"yes\":\n",
    "            print(\"--- 생성된 답변이 질문을 잘 해결함 ---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"--- 답변 관련성이 부족 -> transform_query ---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "def decide_to_generate_self(state: \"SelfRagOverallState\") -> str:\n",
    "    print(\"--- 평가된 문서 분석 ---\")\n",
    "    if state['num_generations'] > 1:\n",
    "        print(\"--- 생성 횟수 초과, 생성 결정 ---\")\n",
    "        return \"generate\"\n",
    "    # 여기서는 필터링된 문서가 존재하는지 확인\n",
    "    if not state['filtered_documents']:\n",
    "        print(\"--- 관련 문서 없음 -> transform_query ---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"--- 관련 문서 존재 -> generate ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "# (7) RoutingDecision \n",
    "class RoutingDecision(BaseModel):\n",
    "    \"\"\"Determines whether a user question should be routed to document search or LLM fallback.\"\"\"\n",
    "    route: Literal[\"search_data\",\"llm_fallback\"] = Field(\n",
    "        description=\"Classify the question as 'search_data' (financial) or 'llm_fallback' (general)\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec1e26",
   "metadata": {},
   "source": [
    "##### 6. 상태 정의 및 노드 함수 (전체 Adaptive 체인)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29163d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 통합: SelfRagOverallState (질문, 생성, 원본 문서, 필터 문서, 생성 횟수)\n",
    "#TODO\n",
    "\n",
    "# 메인 그래프 상태 정의\n",
    "class SelfRagOverallState(TypedDict):\n",
    "    \"\"\"\n",
    "    Adaptive Self-RAG 체인의 전체 상태를 관리    \n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: Annotated[List[str], add]\n",
    "    routing_decision: str \n",
    "    num_generations: int\n",
    "    documents: List[Document]\n",
    "    filtered_documents: List[Document]\n",
    "    history: List[Tuple[str,str]] # (user, bot) 메시지 쌍 저장용\n",
    "\n",
    "def initialize_state() -> SelfRagOverallState:\n",
    "    \"\"\"Create a new state with proper initialization of all fields\"\"\"\n",
    "    return {\n",
    "        \"question\": \"\",\n",
    "        \"generation\": [],\n",
    "        \"routing_decision\": \"\",\n",
    "        \"num_generations\": 0,\n",
    "        \"documents\": [],\n",
    "        \"filtered_documents\": [],\n",
    "        \"history\": []\n",
    "    }    \n",
    "# 새로운 재작성 전용 LLM 체인 - 히스토리 답변이 있는 경우 이전 대화 맥락에 맞게 질문을 수정하여 문서 서치하기 위함\n",
    "def contextualize_query(state: SelfRagOverallState) -> dict:\n",
    "    # 최근 3턴 히스토리 추출\n",
    "    recent = state['history'][-3:]\n",
    "    hist_block = \"\\n\".join(f\"User: {u}\\nAssistant: {a}\" for u,a in recent)\n",
    "    payload = {\"history\": hist_block, \"question\": state['question']}\n",
    "    improved = question_rewriter_chain.invoke(payload)\n",
    "    return {\"question\": improved}\n",
    "\n",
    "rewrite_input = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 금융 상품 챗봇 AI와 유저의 대화 히스토리를 기반으로 마지막 유저의 질문을 분석하여 금융상품 추천 RAG 시스템이 문서를 잘 찾을 수 있게 질문을 구체적으로 재작성하세요.\"\n",
    "    \"재작성된 질문은 길이가 너무 길어지지 않게 하며, 유저가 원히는 핵심이 무엇인지 명확하게 들어나는 문자이어야 합니다.\"\n",
    "    \"적용되는 RAG의 서치알고리즘은 백터유사도를 기반으로 하기 때문에 이를 고려하여 질문을 재작성하세요.\"\n",
    "    \"만약 [History]에 아무것도 없거나 유저의 마지막 질의가 맥락상 금융상품과 관련된 것이 아니라면 유저의 질문을 그대로 작성하세요.\"),\n",
    "    (\"system\", \"[History]\\n{history}\"),\n",
    "    (\"human\", \"[Question]\\n{question}\\n\\n[Improved Question]\\n\"),\n",
    "])\n",
    "question_rewriter_chain = rewrite_input | llm | StrOutputParser()\n",
    "\n",
    "# 질문 재작성 노드 (변경 후 검색 루프)\n",
    "def transform_query_self(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- 질문 개선 ---\")\n",
    "    new_question = rewrite_question(state['question'])\n",
    "    print(f\"--- 개선된 질문 : \\n{new_question} \")\n",
    "    new_count = state['num_generations'] + 1\n",
    "    print(f\"num_generations : {new_count}\")\n",
    "    return {\"question\": new_question, \"num_generations\": new_count}\n",
    "\n",
    "# 답변 생성 노드 (서브 그래프로부터 받은 필터 문서 우선 사용, 이전 대화를 참고 할 수 있도록 수정)\n",
    "def format_chat_history(history):\n",
    "    messages = []\n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append((\"human\", user_msg))\n",
    "        messages.append((\"ai\", assistant_msg))\n",
    "    return messages\n",
    "\n",
    "generate_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"\"\"\n",
    "[Your task]\n",
    "You are a financial product expert and consultant who always responds in Korean.\n",
    "Analyze the user query and the given financial product data to recommend the most suitable product.\n",
    "Use the conversation history to maintain context. Rely only on the provided documents and history.\n",
    "\n",
    "[Instructions]\n",
    "1. 질문과 관련된 정보를 문맥에서 신중하게 확인합니다.\n",
    "2. 답변에 질문과 직접 관련된 정보만 사용합니다.\n",
    "3. 문맥에 명시되지 않은 내용에 대해 추측하지 않습니다.\n",
    "4. 불필요한 정보를 피하고, 명확하게 작성합니다.\n",
    "5. 문맥에서 정확한 답변을 생성할 수 없다면 마지막에 \"더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.\"를 추가합니다.     \n",
    "\"\"\".strip()),\n",
    "    (\"system\", \"[Context]\\n{context}\"),\n",
    "    (\"system\", \"[History]\\n{formatted_history}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def generate_self(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- 답변 생성 (히스토리 포함) ---\")\n",
    "    \n",
    "    # 최근 대화 제한\n",
    "    recent_history = state[\"history\"][10:] if len(state[\"history\"]) > 5 else state[\"history\"][:5]\n",
    "\n",
    "    # 대화 히스토리 포맷팅\n",
    "    formatted_history = \"\"\n",
    "    for user_msg, assistant_msg in recent_history:\n",
    "        formatted_history += f\"User: {user_msg}\\nAssistant: {assistant_msg}\\n\\n\"\n",
    "\n",
    "    # 2) context 직렬화\n",
    "    docs = state['filtered_documents'] or state['documents']\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs) if docs else \"관련 문서 없음\"\n",
    "\n",
    "    # 3) 프롬프트에 값 넣고 LLM 호출\n",
    "    chain = generate_template  | llm | StrOutputParser()\n",
    "    out = chain.invoke({\n",
    "        \"formatted_history\": formatted_history,\n",
    "        \"context\": context,\n",
    "        \"question\": state[\"question\"],\n",
    "    })\n",
    "    answer: str = out\n",
    "\n",
    "    # 4) 상태 업데이트\n",
    "    state[\"num_generations\"] += 1\n",
    "    state[\"generation\"] = answer\n",
    "\n",
    "    return {\n",
    "        \"generation\": [answer],\n",
    "        \"num_generations\": state[\"num_generations\"],\n",
    "    }\n",
    "\n",
    "\n",
    "structured_llm_RoutingDecision = llm.with_structured_output(RoutingDecision)\n",
    "\n",
    "question_router_system  = \"\"\"\n",
    "You are an AI assistant that routes user questions to the appropriate processing path.\n",
    "Return one of the following labels:\n",
    "- search_data\n",
    "- llm_fallback\n",
    "\"\"\"\n",
    "\n",
    "question_router_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", question_router_system),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "question_router = question_router_prompt | structured_llm_RoutingDecision\n",
    "\n",
    "# question route 노드 \n",
    "def route_question_adaptive(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- 질문 판단 (일반 or 금융) ---\")\n",
    "    print(f\"질문: {state['question']}\")\n",
    "    decision = question_router.invoke({\"question\": state['question']})\n",
    "    print(\"routing_decision:\", decision.route)\n",
    "    return {\"routing_decision\": decision.route}\n",
    "\n",
    "# question route 분기 함수 \n",
    "def route_question_adaptive_self(state: SelfRagOverallState) -> str:\n",
    "    \"\"\"\n",
    "    질문 분석 및 라우팅: 사용자의 질문을 분석하여 '금융질문'인지 '일반질문'인지 판단\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if state['routing_decision'] == \"llm_fallback\":\n",
    "            print(\"--- 일반질문으로 라우팅 ---\")\n",
    "            return \"llm_fallback\"\n",
    "        else:\n",
    "            print(\"--- 금융질문으로 라우팅 ---\")\n",
    "            return \"search_data\"\n",
    "    except Exception as e:\n",
    "        print(f\"--- 질문 분석 중 Exception 발생: {e} ---\")\n",
    "        return \"llm_fallback\"\n",
    "\n",
    "\n",
    "fallback_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    You are an AI assistant helping with various topics. \n",
    "    Respond in Korean.\n",
    "    - Provide accurate and helpful information.\n",
    "    - Keep answers concise yet informative.\n",
    "    - Inform users they can ask for clarification if needed.\n",
    "    - Let users know they can ask follow-up questions if needed.\n",
    "    - End every answer with the sentence: \"저는 금융상품 질문에 특화되어 있습니다. 금융상품관련 질문을 주세요.\"\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def llm_fallback_adaptive(state: SelfRagOverallState) -> dict:\n",
    "    \"\"\"Generates a direct response using the LLM when the question is unrelated to financial products.\"\"\"\n",
    "    print(\"--- 일반 질문 Fallback (히스토리 반영) ---\")\n",
    "    \n",
    "    # 대화 히스토리 포맷팅\n",
    "    formatted_history = \"\"\n",
    "    for user_msg, assistant_msg in state[\"history\"]:\n",
    "        formatted_history += f\"User: {user_msg}\\nAssistant: {assistant_msg}\\n\\n\"\n",
    "\n",
    "\n",
    "    fallback_chain = fallback_prompt | llm | StrOutputParser()\n",
    "    out = fallback_chain.invoke({\n",
    "        \"formatted_history\": formatted_history,\n",
    "        \"question\": state[\"question\"],\n",
    "    })\n",
    "    answer: str = out\n",
    "\n",
    "    state[\"history\"].append((state[\"question\"], answer))\n",
    "    return {\"generation\": [answer]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2de9f1",
   "metadata": {},
   "source": [
    "##### 7. [서브 그래프 통합] - 병렬 검색 서브 그래프 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0921400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 상태 정의 (검색 서브 그래프 전용) ---\n",
    "class SearchState(TypedDict):\n",
    "    question: str\n",
    "    # generation: str\n",
    "    documents: Annotated[List[Document], add]  # 팬아웃된 각 검색 결과를 누적할 것\n",
    "    filtered_documents: List[Document]         # 관련성 평가를 통과한 문서들\n",
    "\n",
    "# ToolSearchState: SearchState에 추가 정보(datasources) 포함\n",
    "class ToolSearchState(SearchState):\n",
    "    datasources: List[str]  # 참조할 데이터 소스 목록\n",
    "\n",
    "# --- 서브그래프 노드 함수 ---\n",
    "def search_fixed_deposit_node(state: SearchState):\n",
    "    \"\"\"\n",
    "    정기예금 상품 검색 (서브 그래프)\n",
    "    \"\"\"\n",
    "    docs = search_fixed_deposit.invoke(state[\"question\"])\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "def search_demand_deposit_node(state: SearchState):\n",
    "    \"\"\"\n",
    "    입출금자유예금 상품 검색 (서브 그래프)\n",
    "    \"\"\"\n",
    "    docs = search_demand_deposit.invoke(state[\"question\"])\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "def filter_documents_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    검색된 문서들에 대해 관련성 평가 후 필터링\n",
    "    \"\"\"\n",
    "    print(\"--- 문서 관련성 평가 (서브 그래프) ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader_binary.invoke({\n",
    "            \"question\": question,\n",
    "            \"document\": d.page_content\n",
    "        })\n",
    "        if score.binary_score == \"yes\":\n",
    "            print(\"--- 문서 관련성: 있음 ---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"--- 문서 관련성: 없음 ---\")\n",
    "    return {\"filtered_documents\": filtered_docs}\n",
    "\n",
    "def search_web_search_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    웹 검색 기반 금융 정보 검색 (서브 그래프)\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print('--- 웹 검색 실행 ---')\n",
    "\n",
    "    docs = web_search.invoke({\"query\": question})  # 딕셔너리 형태로 넘김\n",
    "\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 웹 정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "# --- 질문 라우팅 (서브 그래프 전용) ---\n",
    "class SubgraphToolSelector(BaseModel):\n",
    "    \"\"\"Selects the most appropriate tool for the user's question.\"\"\"\n",
    "    tool: Literal[\"search_fixed_deposit\", \"search_demand_deposit\", \"web_search\"] = Field(\n",
    "        description=\"Select one of the tools: search_fixed_deposit, search_demand_deposit or web_search based on the user's question.\"\n",
    "    )\n",
    "\n",
    "class SubgraphToolSelectors(BaseModel):\n",
    "    \"\"\"Selects all tools relevant to the user's question.\"\"\"\n",
    "    tools: List[SubgraphToolSelector] = Field(\n",
    "        description=\"Select one or more tools: search_fixed_deposit, search_demand_deposit or web_search based on the user's question.\"\n",
    "    )\n",
    "\n",
    "structured_llm_SubgraphToolSelectors = llm.with_structured_output(SubgraphToolSelectors)\n",
    "\n",
    "subgraph_system  = dedent(\"\"\"\\\n",
    "You are an AI assistant specializing in routing user questions to the appropriate tools.\n",
    "Use the following guidelines:\n",
    "- For fixed deposit product queries, use the search_fixed_deposit tool.\n",
    "- For demand deposit product queries, use the search_demand_deposit tool.\n",
    "- For general financial or real-time information queries, or when the user explicitly mentions 'web search',\n",
    "  use the web_search tool.\n",
    "  Always choose the appropriate tools based on the user's question.\n",
    "\"\"\")\n",
    "subgraph_route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", subgraph_system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "question_tool_router = subgraph_route_prompt  | structured_llm_SubgraphToolSelectors\n",
    "\n",
    "def analyze_question_tool_search(state: ToolSearchState):\n",
    "    \"\"\"\n",
    "    질문 분석 및 라우팅: 사용자의 질문에서 참조할 데이터 소스 결정\n",
    "    \"\"\"\n",
    "    print(\"--- 질문 라우팅 ---\")\n",
    "    question = state[\"question\"]\n",
    "    result = question_tool_router.invoke({\"question\": question})\n",
    "    datasources = [tool.tool for tool in result.tools]\n",
    "    return {\"datasources\": datasources}\n",
    "\n",
    "def route_datasources_tool_search(state: ToolSearchState) -> Sequence[str]:\n",
    "    \"\"\"\n",
    "    라우팅 결과에 따라 실행할 검색 노드를 결정 (병렬로 팬아웃)\n",
    "    \"\"\"\n",
    "    datasources = set(state['datasources'])\n",
    "\n",
    "    # 명확히 하나만 선택된 경우\n",
    "    if datasources == {'search_fixed_deposit'}:\n",
    "        return ['search_fixed_deposit']\n",
    "    elif datasources == {'search_demand_deposit'}:\n",
    "        return ['search_demand_deposit']\n",
    "    elif datasources == {'web_search'}:\n",
    "        return ['web_search']\n",
    "\n",
    "    # 도구가 전부 실행되거나 애매모호할 때는 도구 전부 실행\n",
    "    return ['search_fixed_deposit', 'search_demand_deposit', 'web_search']\n",
    "\n",
    "\n",
    "# --- 서브 그래프 빌더 구성 ---\n",
    "search_builder = StateGraph(ToolSearchState)\n",
    "\n",
    "# 노드 추가\n",
    "search_builder.add_node(\"analyze_question\", analyze_question_tool_search)\n",
    "search_builder.add_node(\"search_fixed_deposit\", search_fixed_deposit_node)   # wapper 함수 말고 직접 invoke 함수 사용하는 것으로 수정\n",
    "search_builder.add_node(\"search_demand_deposit\", search_demand_deposit_node) # 마찬가지로 함께\n",
    "search_builder.add_node(\"web_search\", search_web_search_subgraph)\n",
    "search_builder.add_node(\"filter_documents\", filter_documents_subgraph)\n",
    "\n",
    "# 엣지 구성\n",
    "search_builder.add_edge(START, \"analyze_question\")\n",
    "search_builder.add_conditional_edges(\n",
    "    \"analyze_question\",\n",
    "    route_datasources_tool_search,\n",
    "    {\n",
    "        \"search_fixed_deposit\": \"search_fixed_deposit\",\n",
    "        \"search_demand_deposit\": \"search_demand_deposit\",\n",
    "        \"web_search\": \"web_search\"\n",
    "    }\n",
    ")\n",
    "# 두 검색 노드 모두 실행한 후 각각의 결과는 filter_documents로 팬인(fan-in) 처리\n",
    "search_builder.add_edge(\"search_fixed_deposit\", \"filter_documents\")\n",
    "search_builder.add_edge(\"search_demand_deposit\", \"filter_documents\")\n",
    "search_builder.add_edge(\"web_search\", \"filter_documents\")\n",
    "search_builder.add_edge(\"filter_documents\", END)\n",
    "\n",
    "# 서브 그래프 컴파일\n",
    "tool_search_graph = search_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61732ee6",
   "metadata": {},
   "source": [
    "##### 8. [전체 그래프와 결합] - Self-RAG Overall Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a2ed831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 그래프 빌더 (rag_builder) 구성\n",
    "rag_builder = StateGraph(SelfRagOverallState)\n",
    "\n",
    "# 노드 추가: 검색 서브 그래프, 생성, 질문 재작성 등\n",
    "rag_builder.add_node(\"contextualize_query\", contextualize_query)\n",
    "rag_builder.add_node(\"route_question\", route_question_adaptive)\n",
    "rag_builder.add_node(\"llm_fallback\", llm_fallback_adaptive)\n",
    "rag_builder.add_node(\"search_data\", tool_search_graph)         # 서브 그래프로 병렬 검색 및 필터링 수행\n",
    "rag_builder.add_node(\"generate\", generate_self)                # 답변 생성 노드\n",
    "rag_builder.add_node(\"transform_query\", transform_query_self)  # 질문 개선 노드\n",
    "\n",
    "# 전체 그래프 엣지 구성\n",
    "rag_builder.add_edge(START, \"contextualize_query\")\n",
    "rag_builder.add_edge(\"contextualize_query\", \"route_question\")\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"route_question\",\n",
    "    route_question_adaptive_self, \n",
    "    {\n",
    "        \"llm_fallback\": \"llm_fallback\",\n",
    "        \"search_data\": \"search_data\"\n",
    "    }\n",
    ")\n",
    "rag_builder.add_edge(\"llm_fallback\", END)\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"search_data\",\n",
    "    decide_to_generate_self, \n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    }\n",
    ")\n",
    "rag_builder.add_edge(\"transform_query\", \"search_data\")\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_self,\n",
    "    {\n",
    "        \"not supported\": \"generate\",      # 환각 발생 시 재생성\n",
    "        \"not useful\": \"transform_query\",  # 관련성 부족 시 질문 재작성 후 재검색\n",
    "        \"useful\": END,\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# MemorySaver 인스턴스 생성 (대화 상태를 저장할 in-memory 키-값 저장소)\n",
    "memory = MemorySaver()\n",
    "adaptive_self_rag_memory = rag_builder.compile(checkpointer=memory)\n",
    "# adaptive_self_rag = rag_builder.compile()\n",
    "\n",
    "# 그래프 파일 저장하기\n",
    "# display(Image(adaptive_self_rag.get_graph().draw_mermaid_png()))\n",
    "with open(\"adaptive_self_rag_memory.mmd\", \"w\") as f:\n",
    "    f.write(adaptive_self_rag_memory.get_graph(xray=True).draw_mermaid()) # 저장된 mmd 파일에서 코드 복사 후 https://mermaid.live 에 붙여넣기.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 선택된 도구: ['web_search']\n",
      "\n",
      "🟡 실행 중: web_search_subgraph\n",
      "--- 웹 검색 실행 ---\n",
      "\n",
      "📄 웹 검색 결과:\n",
      "\n",
      "🔹 결과 1\n",
      "내용: <Document href=\"https://ko.tradingeconomics.com/united-states/interest-rate\"/>\n",
      "연방준비제도는 2025년 5월 세 번째 연속 회의에서 자금 금리를 4.25%–4.50%로 유지했으며, 이는 기대에 부합하는 결정으로, 트럼프 대통령의 관세가 인플레이션을 상승\n",
      "</Document> ...\n",
      "메타데이터: {'source': 'web search', 'url': 'https://ko.tradingeconomics.com/united-states/interest-rate'}\n",
      "\n",
      "🔹 결과 2\n",
      "내용: <Document href=\"https://naver.me/GdyP4ZaO\"/>\n",
      "... 2025 년 5 월 FOMC는 기준금리를 4.25 ~ 4.50 %로 동결하고, 자산축소(QT)를 기존 속도로 이어가기로 했습니다. 주목할 점은 파월이 “인플레이션과\n",
      "</Document> ...\n",
      "메타데이터: {'source': 'web search', 'url': 'https://naver.me/GdyP4ZaO'}\n"
     ]
    }
   ],
   "source": [
    "# # ✅ 1. 질문 정의\n",
    "# example_state = {\n",
    "#     \"question\": \"2025년 5월 최신 금리 정보를 web search로 찾아줘\"\n",
    "# }\n",
    "\n",
    "# # ✅ 2. 툴 선택 (LLM이 판단)\n",
    "# result = question_tool_router.invoke({\"question\": example_state[\"question\"]})\n",
    "\n",
    "# # ✅ 3. 선택된 툴 출력\n",
    "# selected_tools = [tool.tool for tool in result.tools]\n",
    "# print(\"✅ 선택된 도구:\", selected_tools)\n",
    "\n",
    "# # ✅ 4. web_search가 선택됐을 경우 실행\n",
    "# if \"web_search\" in selected_tools:\n",
    "#     print(\"\\n🟡 실행 중: web_search_subgraph\")\n",
    "\n",
    "#     # SearchState 구조와 일치시켜 호출\n",
    "#     search_result = search_web_search_subgraph({\"question\": example_state[\"question\"]})\n",
    "\n",
    "#     print(\"\\n📄 웹 검색 결과:\")\n",
    "#     for i, doc in enumerate(search_result[\"documents\"], start=1):\n",
    "#         print(f\"\\n🔹 결과 {i}\")\n",
    "#         print(\"내용:\", doc.page_content[:300], \"...\")\n",
    "#         print(\"메타데이터:\", doc.metadata)\n",
    "\n",
    "# else:\n",
    "#     print(\"❌ web_search는 선택되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bf5fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pdf_link 삽입 보조함수\n",
    "def postprocess_answer(answer: str, docs: List[Document]) -> str:\n",
    "    for doc in docs:\n",
    "        pdf = doc.metadata.get(\"pdf_link\")\n",
    "        if pdf:\n",
    "            if \"상품설명서\" not in answer:\n",
    "                answer += f\"\\n\\n [상품설명서 PDF 보기]({pdf})\"\n",
    "            break\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3baa60",
   "metadata": {},
   "source": [
    "##### 9. Gradio Chatbot 구성 및 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12fdd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://925e9b33837ce6a662.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://925e9b33837ce6a662.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 질문 판단 (일반 or 금융) ---\n",
      "질문: 최근 기준금리 현황을 알려주세요.\n",
      "routing_decision: search_data\n",
      "--- 금융질문으로 라우팅 ---\n",
      "--- 질문 라우팅 ---\n",
      "--- 웹 검색 실행 ---\n",
      "--- 문서 관련성 평가 (서브 그래프) ---\n",
      "--- 문서 관련성: 있음 ---\n",
      "--- 문서 관련성: 있음 ---\n",
      "--- 평가된 문서 분석 ---\n",
      "--- 관련 문서 존재 -> generate ---\n",
      "--- 답변 생성 (히스토리 포함) ---\n",
      "--- 답변 평가 (생성) ---\n",
      "--- 생성된 답변: ['최근 한국은행의 기준금리는 2025년 2월 25일 기준으로 2.75%입니다. 미국의 기준금리는 현재 4.50%입니다. 추가적인 정보가 필요하시면 말씀해 주세요.'] ---\n",
      "--- 답변 할루시네이션 평가 ---\n",
      "--- 답변-질문 관련성 평가 ---\n",
      "--- 생성된 답변이 질문을 잘 해결함 ---\n",
      "--- History 확인 ---\n",
      "[('최근 기준금리 정보 알려줘', '최근 한국은행의 기준금리는 2025년 2월 25일 기준으로 2.75%입니다. 미국의 기준금리는 현재 4.50%입니다. 추가적인 정보가 필요하시면 말씀해 주세요.')]\n"
     ]
    }
   ],
   "source": [
    "# 챗봇 클래스\n",
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.thread_id = str(uuid.uuid4())\n",
    "\n",
    "    def chat(self, message: str, history: List[Tuple[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        입력 메시지와 대화 이력을 기반으로 Adaptive Self-RAG 체인을 호출하고,\n",
    "        응답을 반환합니다.\n",
    "        \"\"\"\n",
    "        config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
    "        state = initialize_state()\n",
    "        state[\"question\"] = message\n",
    "        \n",
    "        # history가 있으면 추가\n",
    "        if history:\n",
    "            state[\"history\"] = history\n",
    "        \n",
    "        result = adaptive_self_rag_memory.invoke(state, config=config)\n",
    "\n",
    "        gen_list = result.get(\"generation\", [])\n",
    "        docs = result.get(\"filtered_documents\", [])\n",
    "\n",
    "        if not gen_list:\n",
    "            bot_response = \"죄송합니다. 답변을 생성할 수 없습니다.\"\n",
    "        else:\n",
    "            raw_answer = gen_list[-1]\n",
    "            bot_response = postprocess_answer(raw_answer, docs)\n",
    "\n",
    "        # 대화 이력 업데이트\n",
    "        state[\"history\"].append((message, bot_response))\n",
    "        print(f\"--- History 확인 ---\\n{state['history']}\")\n",
    "        return bot_response\n",
    "\n",
    "\n",
    "# 챗봇 인스턴스 생성\n",
    "chatbot = ChatBot() \n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot.chat,\n",
    "    title=\"Adaptive Self-RAG 기반 RAG 챗봇 시스템\",\n",
    "    description=\"정기예금, 입출금자유예금 상품 및 기타 질문에 답변합니다.\",\n",
    "    examples=[\n",
    "        \"정기예금 상품 중 금리가 가장 높은 것은?\",\n",
    "        \"정기예금과 입출금자유예금은 어떤 차이점이 있나요?\",\n",
    "        \"은행의 예금 상품을 추천해 주세요.\"\n",
    "    ],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "# Gradio 앱 실행: 이 파일을 메인으로 실행할 때만 띄웁니다.\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eefd7c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aea65594",
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\graphviz\\backend\\execute.py:78\u001b[39m, in \u001b[36mrun_check\u001b[39m\u001b[34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         proc = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:548\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstderr\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1026\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1023\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1024\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1538\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[39m\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1538\u001b[39m     hp, ht, pid, tid = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[32m   1540\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1547\u001b[39m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[32m   1548\u001b[39m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1551\u001b[39m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mExecutableNotFound\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m dot.edge(\u001b[33m\"\u001b[39m\u001b[33manalyze_question\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mweb_search\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m dot.edge(\u001b[33m\"\u001b[39m\u001b[33mweb_search\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfilter_documents\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mdot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_debug\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpng\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleanup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\graphviz\\_tools.py:171\u001b[39m, in \u001b[36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m     wanted = \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    163\u001b[39m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated.items())\n\u001b[32m    164\u001b[39m     warnings.warn(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m will be reduced\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    165\u001b[39m                   \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m positional args\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    166\u001b[39m                   \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    167\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33m as keyword arg(s)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    168\u001b[39m                   stacklevel=stacklevel,\n\u001b[32m    169\u001b[39m                   category=category)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\graphviz\\rendering.py:122\u001b[39m, in \u001b[36mRender.render\u001b[39m\u001b[34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[39m\n\u001b[32m    118\u001b[39m filepath = \u001b[38;5;28mself\u001b[39m.save(filename, directory=directory, skip_existing=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    120\u001b[39m args.append(filepath)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m rendered = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[32m    125\u001b[39m     log.debug(\u001b[33m'\u001b[39m\u001b[33mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, filepath)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\graphviz\\_tools.py:171\u001b[39m, in \u001b[36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m     wanted = \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    163\u001b[39m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated.items())\n\u001b[32m    164\u001b[39m     warnings.warn(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m will be reduced\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    165\u001b[39m                   \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m positional args\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    166\u001b[39m                   \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    167\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33m as keyword arg(s)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    168\u001b[39m                   stacklevel=stacklevel,\n\u001b[32m    169\u001b[39m                   category=category)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\graphviz\\backend\\rendering.py:326\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[39m\n\u001b[32m    322\u001b[39m cmd += args\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mwork around pytype false alarm\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m os.fspath(outfile)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giho0\\langgraph_main\\venv\\Lib\\site-packages\\graphviz\\backend\\execute.py:81\u001b[39m, in \u001b[36mrun_check\u001b[39m\u001b[34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.errno == errno.ENOENT:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc.stderr:\n",
      "\u001b[31mExecutableNotFound\u001b[39m: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "dot.node(\"analyze_question\")\n",
    "dot.node(\"web_search\")\n",
    "dot.node(\"filter_documents\")\n",
    "\n",
    "dot.edge(\"analyze_question\", \"web_search\")\n",
    "dot.edge(\"web_search\", \"filter_documents\")\n",
    "dot.render(\"web_search_debug\", format=\"png\", cleanup=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
