{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b13a764",
   "metadata": {},
   "source": [
    "##### 1. Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2472f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface==0.1.2\n",
      "  Obtaining dependency information for langchain-huggingface==0.1.2 from https://files.pythonhosted.org/packages/9d/f8/77a303ddc492f6eed8bf0979f2bc6db4fa6eb1089c5e9f0f977dd87bc9c2/langchain_huggingface-0.1.2-py3-none-any.whl.metadata\n",
      "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.30.2)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.3.58)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface==0.1.2)\n",
      "  Obtaining dependency information for sentence-transformers>=2.6.0 from https://files.pythonhosted.org/packages/45/2d/1151b371f28caae565ad384fdc38198f1165571870217aedda230b9d7497/sentence_transformers-4.1.0-py3-none-any.whl.metadata\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-huggingface==0.1.2) (0.21.1)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface==0.1.2)\n",
      "  Obtaining dependency information for transformers>=4.39.0 from https://files.pythonhosted.org/packages/a9/b6/5257d04ae327b44db31f15cce39e6020cc986333c715660b1315a9724d82/transformers-4.51.3-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.13.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.11.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.15.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (11.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (0.5.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (2025.4.26)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.6.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\giho0\\langgraph_main\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.3.1)\n",
      "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Installing collected packages: transformers, sentence-transformers, langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.1.2 sentence-transformers-4.1.0 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## Ï∂îÍ∞Ä Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò\n",
    "pip install langchain-huggingface==0.1.2\n",
    "pip install langgraph==0.2.34\n",
    "pip install gradio==4.44.1\n",
    "!pip install langchain-community==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e1596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adaptive_self_rag\n",
    "Í∏àÏúµÏÉÅÌíà(Ïòà: Ï†ïÍ∏∞ÏòàÍ∏à, ÏûÖÏ∂úÍ∏àÏûêÏú†ÏòàÍ∏à) Í¥ÄÎ†® ÏßàÏùòÏóê ÎåÄÌï¥:\n",
    "1. ÏßàÎ¨∏ ÎùºÏö∞ÌåÖ ‚Üí (Í∏àÏúµÏÉÅÌíà Í¥ÄÎ†®Ïù¥Î©¥) Î¨∏ÏÑú Í≤ÄÏÉâ (Î≥ëÎ†¨ ÏÑúÎ∏å Í∑∏ÎûòÌîÑ) ‚Üí Î¨∏ÏÑú ÌèâÍ∞Ä ‚Üí (Ï°∞Í±¥Î∂Ä) ÏßàÎ¨∏ Ïû¨ÏûëÏÑ± ‚Üí ÎãµÎ≥Ä ÏÉùÏÑ±\n",
    "   / (Í∏àÏúµÏÉÅÌíàÍ≥º Î¨¥Í¥ÄÌïòÎ©¥) LLM fallbackÏùÑ ÌÜµÌï¥ Î∞îÎ°ú ÎãµÎ≥Ä ÏÉùÏÑ±\n",
    "Í∑∏Î¶¨Í≥† ÏÉùÏÑ±Îêú ÎãµÎ≥ÄÏùò ÌíàÏßà(ÌôòÍ∞Å, Í¥ÄÎ†®ÏÑ±) ÌèâÍ∞Ä ÌõÑ ÌïÑÏöîÏãú Ïû¨ÏÉùÏÑ± ÎòêÎäî Ïû¨ÏûëÏÑ±ÌïòÎäî Adaptive Self-RAG Ï≤¥Ïù∏.\n",
    "\"\"\"\n",
    "\n",
    "#############################\n",
    "# 1. Í∏∞Î≥∏ ÌôòÍ≤Ω Î∞è ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
    "#############################\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Í∏∞ÌÉÄ Ïú†Ìã∏\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "from textwrap import dedent\n",
    "from operator import add\n",
    "from heapq import merge\n",
    "from typing import List, Literal, Sequence, TypedDict, Annotated, Tuple\n",
    "\n",
    "# ÏÑúÏπò ÏïåÍ≥†Î¶¨Ï¶ò\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# LangChain, Chroma, LLM Í¥ÄÎ†®\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Grader ÌèâÍ∞ÄÏßÄÌëúÏö©\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Í∑∏ÎûòÌîÑ Í¥ÄÎ†®\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Gradio Í¥ÄÎ†®\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fece5f9",
   "metadata": {},
   "source": [
    "##### 2. ÏûÑÎ≤†Îî© Î∞è DBÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "class LangChainSentenceTransformer(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"[INFO] Using device: {device}\")\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text, show_progress_bar=False, convert_to_numpy=True).tolist()\n",
    "\n",
    "embeddings_model_koMultitask = LangChainSentenceTransformer(\"jhgan/ko-sroberta-multitask\") # 768Ï∞®Ïõê ÏûÑÎ∞∞Îî© Î™®Îç∏Î°ú Î≥ÄÍ≤Ω\n",
    "\n",
    "# Chroma DB Í≤ΩÎ°ú\n",
    "CHROMA_DIR = \"./../findata/chroma_db\"\n",
    "\n",
    "# JSON Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú\n",
    "FIXED_JSON_PATH = \"./../findata/processed_fixed_deposit.json\"\n",
    "DEMAND_JSON_PATH = \"./../findata/processed_demand_deposit.json\"\n",
    "\n",
    "# DB Ïù¥Î¶Ñ\n",
    "FIXED_COLLECTION = \"processed_fixed_deposit\"\n",
    "DEMAND_COLLECTION = \"processed_demand_deposit\"\n",
    "\n",
    "def load_and_prepare_all_documents(json_paths: list[str]) -> list[Document]:\n",
    "    docs: list[Document] = []\n",
    "    for path in json_paths:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        for entry in data[\"documents\"]:\n",
    "            # Î≥∏Î¨∏\n",
    "            content = entry[\"content\"]\n",
    "            # metadata ÌïÑÎìú ÏòÅÏñ¥ ÌÇ§Î°ú ÌÜµÏùº\n",
    "            md = entry.get(\"metadata\", {})\n",
    "            bank         = entry.get(\"bank\",         md.get(\"ÏùÄÌñâ\"))\n",
    "            product_name = entry.get(\"product_name\", md.get(\"ÏÉÅÌíàÎ™Ö\"))\n",
    "            category     = entry.get(\"type\")\n",
    "            pdf_link     = md.get(\"pdf_link\")\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"id\":           entry.get(\"id\"),\n",
    "                        \"type\":         category,\n",
    "                        \"bank\":         bank,\n",
    "                        \"product_name\": product_name,\n",
    "                        \"pdf_link\":     pdf_link\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    return docs\n",
    "\n",
    "# JSON ÌååÏùº Í≤ΩÎ°ú Î¶¨Ïä§Ìä∏\n",
    "ALL_JSON = [\n",
    "    FIXED_JSON_PATH,\n",
    "    DEMAND_JSON_PATH\n",
    "]\n",
    "\n",
    "all_documents = load_and_prepare_all_documents(ALL_JSON)\n",
    "corpus = [doc.page_content.split() for doc in all_documents]\n",
    "bm25_index = BM25Okapi(corpus)\n",
    "\n",
    "# Ïù∏Ï†úÏä§Ï≤ú\n",
    "vector_db = Chroma(\n",
    "    embedding_function=embeddings_model_koMultitask,\n",
    "    collection_name=\"combined_products_koMultitask\",  # ÏûÑÎ∞∞Îî© Î™®Îç∏Ïóê Îî∞Î•∏ Ïù∏Ï†úÏä§Ï≤ú Ïù¥Î¶Ñ Î≥ÄÍ≤Ω\n",
    "    persist_directory=CHROMA_DIR,\n",
    ")\n",
    "\n",
    "# Ìïú Î≤àÎßå Ïù∏Ï†úÏä§Ï≤ú\n",
    "if not vector_db._collection.count():\n",
    "    vector_db.add_documents(all_documents)\n",
    "\n",
    "def extract_bank(query: str) -> str | None:\n",
    "    m = re.search(r'([Í∞Ä-Ìû£A-Za-z0-9]+(?:ÏùÄÌñâ|Î±ÖÌÅ¨))', query)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def extract_product(query: str) -> str | None:\n",
    "    # ‚Äú~ÌÜµÏû•‚Äù, ‚Äú~ÏòàÍ∏à‚Äù, ‚Äú~ÎåÄÏ∂ú‚Äù Îì±ÏúºÎ°ú ÎßàÏπ®\n",
    "    m = re.search(r'([Í∞Ä-Ìû£A-Za-z0-9]+(?:ÌÜµÏû•|ÏòàÍ∏à|ÎåÄÏ∂ú))', query)\n",
    "    return m.group(1).strip() if m else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f6ba7",
   "metadata": {},
   "source": [
    "##### 3. ÏÑúÏπòÏïåÍ≥†Î¶¨Ï¶ò Î∞è ÎèÑÍµ¨(Í≤ÄÏÉâ Ìï®Ïàò) Ï†ïÏùò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef46902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _search_with_filters(query: str, filters: dict, top_k: int) -> list[Document]:\n",
    "    # 1) BM25: Ï†ÑÏ≤¥ ÏΩîÌçºÏä§ÏóêÏÑú score Í≥ÑÏÇ∞ ‚Üí metadata ÌïÑÌÑ∞ Ï†ÅÏö©Ìï¥ top_k\n",
    "    tokenized = query.split()\n",
    "    scores = bm25_index.get_scores(tokenized)\n",
    "    idxs   = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    bm25_docs = []\n",
    "    for i in idxs:\n",
    "        d = all_documents[i]\n",
    "        if all(filters.get(k) is None or d.metadata.get(k)==filters[k] for k in filters):\n",
    "            bm25_docs.append(d)\n",
    "            if len(bm25_docs)>=top_k: break\n",
    "\n",
    "    # 2) Î≤°ÌÑ∞: ChromaÏùò filter ÌååÎùºÎØ∏ÌÑ∞ ÏÇ¨Ïö© (Îã®ÏùºÌÇ§ vs Îã§Ï§ëÌÇ§Ïóê Îî∞Îùº $and Î°ú Î¨∂Ïñ¥ÏÑú Ï†ÑÎã¨)\n",
    "    meta = {k: v for k, v in filters.items() if v is not None}\n",
    "    if not meta:\n",
    "        vec_docs = vector_db.similarity_search(query, k=top_k)\n",
    "    elif len(meta) == 1:\n",
    "        # Îã®Ïùº ÌïÑÌÑ∞ÌÑ∞\n",
    "        vec_docs = vector_db.similarity_search(query, k=top_k, filter=meta)\n",
    "    else:\n",
    "        # Ïó¨Îü¨ ÌïÑÌÑ∞Îäî ÌïòÎÇòÏùò Ïó∞ÏÇ∞Ïûê($and)Î°ú Î¨∂Ïñ¥ÏÑú ÎÑòÍ≤®Ïïº Ìï®\n",
    "        and_list = [{k: v} for k, v in meta.items()]\n",
    "        vec_docs = vector_db.similarity_search(\n",
    "                    query, \n",
    "                    k=top_k, \n",
    "                    filter={\"$and\": and_list}\n",
    "        )\n",
    "\n",
    "    # 3) Ï§ëÎ≥µ Ï†úÍ±∞ Î≥ëÌï©\n",
    "    seen, merged = set(), []\n",
    "    for d in bm25_docs + vec_docs:\n",
    "        uid = d.metadata[\"id\"]\n",
    "        if uid not in seen:\n",
    "            seen.add(uid)\n",
    "            # PDF ÎßÅÌÅ¨Í∞Ä ÏûàÏúºÎ©¥ page_contentÏóê Ï∂îÍ∞Ä\n",
    "            pdf = d.metadata.get(\"pdf_link\")\n",
    "            if pdf and \"pdf_link\" not in d.page_content:\n",
    "                d.page_content += f\"\\n\\nüìÑ [ÏÉÅÌíàÏÑ§Î™ÖÏÑú PDF Î≥¥Í∏∞]({pdf})\"\n",
    "            merged.append(d)\n",
    "    return merged\n",
    "\n",
    "# ÌïòÏù¥Î∏åÎ¶¨Îìú ÏÑúÏπò ÏïåÍ≥†Î¶¨Ï¶ò\n",
    "def hybrid_core_search(query: str, category: str, bank: str=None, product_name: str=None, top_k: int=2) -> List[Document]:\n",
    "    # 1) Î©îÌÉÄ ÌïÑÌÑ∞ Ï§ÄÎπÑ (category ÌïÑÏàò Ìè¨Ìï®)\n",
    "    filters = {\"type\": category}\n",
    "    if bank: \n",
    "        filters[\"bank\"] = bank\n",
    "    if product_name: \n",
    "        filters[\"product_name\"] = product_name\n",
    "\n",
    "    # 2) ÌïÑÌÑ∞ Î†àÎ≤®Î≥ÑÎ°ú Ï†êÏßÑÏ†Å Í≤ÄÏÉâ\n",
    "    filter_levels = [\n",
    "        filters,\n",
    "        {**filters, **{\"product_name\": None}},  # ÏÉÅÌíàÎ™Ö Ï†úÏô∏\n",
    "        {**filters, **{\"bank\": None}},          # ÏùÄÌñâ Ï†úÏô∏\n",
    "        {\"category\": category}                  # Ïπ¥ÌÖåÍ≥†Î¶¨Îßå\n",
    "    ]\n",
    "\n",
    "    # 3) ÏàúÏÑúÎåÄÎ°ú BM25+Î≤°ÌÑ∞ Î≥ëÎ†¨ Í≤ÄÏÉâ ‚Üí Í≤∞Í≥º Î∞òÌôò\n",
    "    for flt in filter_levels:\n",
    "        docs = _search_with_filters(query, flt, top_k=top_k)\n",
    "        if docs:\n",
    "            return docs\n",
    "    return []\n",
    "\n",
    "@tool\n",
    "def search_fixed_deposit(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Search for relevant fixed deposit (Ï†ïÍ∏∞ÏòàÍ∏à) product information using semantic similarity.\n",
    "    This tool retrieves products matching the user's query, such as interest rates or terms.\n",
    "    \"\"\"\n",
    "    bank, product = extract_bank(query), extract_product(query)\n",
    "    return hybrid_core_search(query, category=\"Ï†ïÍ∏∞ÏòàÍ∏à\", bank=bank, product_name=product)\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_demand_deposit(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Search for demand deposit (ÏûÖÏ∂úÍ∏àÏûêÏú†ÏòàÍ∏à) product information using semantic similarity.\n",
    "    This tool retrieves products matching the user's query, such as flexible withdrawal or interest features.\n",
    "    \"\"\"\n",
    "    bank, product = extract_bank(query), extract_product(query)\n",
    "    return hybrid_core_search(query, category=\"ÏûÖÏ∂úÍ∏àÏûêÏú†ÏòàÍ∏à\", bank=bank, product_name=product)\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    This tool serves as a supplementary utility for the financial product recommendation model.\n",
    "    It retrieves up-to-date external information via web search using the Tavily API, \n",
    "    especially when relevant data is not available in the local vector databases\n",
    "\n",
    "    Unlike the RAG-based tools that query embedded product databases,\n",
    "    this tool is designed to handle broader or real-time questions‚Äîsuch as current interest rates, financial trends,\n",
    "    or general queries outside the scope of structured deposit data.\n",
    "\n",
    "    It returns the top 2 semantically relevant documents from the web.\n",
    "    \"\"\"\n",
    "\n",
    "    tavily_search = TavilySearchResults(max_results=2)\n",
    "    docs = tavily_search.invoke(query)\n",
    "\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        formatted_docs.append(\n",
    "            Document(\n",
    "                page_content= f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>',\n",
    "                metadata={\"source\": \"web search\", \"url\": doc[\"url\"]}\n",
    "                )\n",
    "        )\n",
    "\n",
    "    if len(formatted_docs) > 0:\n",
    "        return formatted_docs\n",
    "    \n",
    "    return [Document(page_content=\"Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")]\n",
    "\n",
    "\n",
    "tools = [search_fixed_deposit, search_demand_deposit, web_search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff8277",
   "metadata": {},
   "source": [
    "##### 4. llmÏ¥àÍ∏∞Ìôî & ÎèÑÍµ¨ Î∞îÏù∏Îî©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bbaf7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39bb28",
   "metadata": {},
   "source": [
    "##### 5. LLM Ï≤¥Ïù∏ (Retrieval Grader / Answer Generator / Hallucination / Answer Graders / Question Re-writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f85bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================================================\n",
      " \n",
      "LLM Ï≤¥Ïù∏\n",
      "\n",
      "# (1) Retrieval Grader\n",
      "\n",
      "\n",
      "# (3) Hallucination Grader\n",
      "\n",
      "\n",
      "# (4) Answer Grader\n",
      "\n",
      "\n",
      "# (5) Question Re-writer\n",
      "\n",
      "\n",
      "# (6) Generation Evaluation & Decision Nodes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# 5. LLM Ï≤¥Ïù∏ (Retrieval Grader / Answer Generator / Hallucination / Answer Graders / Question Re-writer)\n",
    "#############################\n",
    "print(\"\\n===================================================================\\n \")\n",
    "print(\"LLM Ï≤¥Ïù∏\\n\")\n",
    "print(\"# (1) Retrieval Grader\\n\")\n",
    "\n",
    "# (1) Retrieval Grader (Í≤ÄÏÉâÌèâÍ∞Ä)\n",
    "class BinaryGradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_BinaryGradeDocuments = llm.with_structured_output(BinaryGradeDocuments)\n",
    "\n",
    "system_prompt = \"\"\"You are an expert in evaluating the relevance of search results to user queries.\n",
    "\n",
    "[Evaluation criteria]\n",
    "1. ÌÇ§ÏõåÎìú Í¥ÄÎ†®ÏÑ±: Î¨∏ÏÑúÍ∞Ä ÏßàÎ¨∏Ïùò Ï£ºÏöî Îã®Ïñ¥ÎÇò Ïú†ÏÇ¨Ïñ¥Î•º Ìè¨Ìï®ÌïòÎäîÏßÄ ÌôïÏù∏\n",
    "2. ÏùòÎØ∏Ï†Å Í¥ÄÎ†®ÏÑ±: Î¨∏ÏÑúÏùò Ï†ÑÎ∞òÏ†ÅÏù∏ Ï£ºÏ†úÍ∞Ä ÏßàÎ¨∏Ïùò ÏùòÎèÑÏôÄ ÏùºÏπòÌïòÎäîÏßÄ ÌèâÍ∞Ä\n",
    "3. Î∂ÄÎ∂Ñ Í¥ÄÎ†®ÏÑ±: ÏßàÎ¨∏Ïùò ÏùºÎ∂ÄÎ•º Îã§Î£®Í±∞ÎÇò Îß•ÎùΩ Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌïòÎäî Î¨∏ÏÑúÎèÑ Í≥†Î†§\n",
    "4. ÎãµÎ≥Ä Í∞ÄÎä•ÏÑ±: ÏßÅÏ†ëÏ†ÅÏù∏ ÎãµÏù¥ ÏïÑÎãàÎçîÎùºÎèÑ ÎãµÎ≥Ä ÌòïÏÑ±Ïóê ÎèÑÏõÄÎê† Ï†ïÎ≥¥ Ìè¨Ìï® Ïó¨Î∂Ä ÌèâÍ∞Ä\n",
    "\n",
    "[Scoring]\n",
    "- Rate 'yes' if relevant, 'no' if not\n",
    "- Default to 'no' when uncertain\n",
    "\n",
    "[Key points]\n",
    "- Consider the full context of the query, not just word matching\n",
    "- Rate as relevant if useful information is present, even if not a complete answer\n",
    "\n",
    "Your evaluation is crucial for improving information retrieval systems. Provide balanced assessments.\n",
    "\"\"\"\n",
    "# Ï±ÑÏ†ê ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"[Retrieved document]\\n{document}\\n\\n[User question]\\n{question}\")\n",
    "])\n",
    "\n",
    "retrieval_grader_binary = grade_prompt | structured_llm_BinaryGradeDocuments\n",
    "\n",
    "# question = \"Ïñ¥Îñ§ ÏòàÍ∏à ÏÉÅÌíàÏù¥ ÏûàÎäîÏßÄ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.\"\n",
    "# print(f'\\nquestion : {question}\\n')\n",
    "# retrieved_docs = fixed_deposit_db.similarity_search(question, k=2)\n",
    "# print(f\"Í≤ÄÏÉâÎêú Î¨∏ÏÑú Ïàò: {len(retrieved_docs)}\")\n",
    "# print(\"===============================================================================\")\n",
    "# print()\n",
    "\n",
    "# relevant_docs = []\n",
    "# for doc in retrieved_docs:\n",
    "#     print(\"Î¨∏ÏÑú:\\n\", doc.page_content)\n",
    "#     print(\"---------------------------------------------------------------------------\")\n",
    "\n",
    "#     relevance = retrieval_grader_binary.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "#     print(f\"Î¨∏ÏÑú Í¥ÄÎ†®ÏÑ±: {relevance}\")\n",
    "\n",
    "#     if relevance.binary_score == 'yes':\n",
    "#         relevant_docs.append(doc)\n",
    "    \n",
    "#     print(\"===========================================================================\")\n",
    "\n",
    "\n",
    "# (2) Answer Generator (ÏùºÎ∞ò RAG)\n",
    "\n",
    "# (3) Hallucination Grader\n",
    "print(\"\\n# (3) Hallucination Grader\\n\")\n",
    "\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_HradeHallucinations = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# ÌôòÍ∞Å ÌèâÍ∞ÄÎ•º ÏúÑÌïú ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ Ï†ïÏùò\n",
    "halluci_system_prompt = \"\"\"\n",
    "You are an expert evaluator assessing whether an LLM-generated answer is grounded in and supported by a given set of facts.\n",
    "\n",
    "[Your task]\n",
    "    - Review the LLM-generated answer.\n",
    "    - Determine if the answer is fully supported by the given facts.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - ÎãµÎ≥ÄÏóê Ï£ºÏñ¥ÏßÑ ÏÇ¨Ïã§Ïù¥ÎÇò Î™ÖÌôïÌûà Ï∂îÎ°†Ìï† Ïàò ÏûàÎäî Ï†ïÎ≥¥ Ïô∏Ïùò ÎÇ¥Ïö©Ïù¥ ÏóÜÏñ¥Ïïº Ìï©ÎãàÎã§.\n",
    "    - ÎãµÎ≥ÄÏùò Î™®Îì† ÌïµÏã¨ ÎÇ¥Ïö©Ïù¥ Ï£ºÏñ¥ÏßÑ ÏÇ¨Ïã§ÏóêÏÑú ÎπÑÎ°ØÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\n",
    "    - ÏÇ¨Ïã§Ï†Å Ï†ïÌôïÏÑ±Ïóê ÏßëÏ§ëÌïòÍ≥†, Í∏ÄÏì∞Í∏∞ Ïä§ÌÉÄÏùºÏù¥ÎÇò ÏôÑÏ†ÑÏÑ±ÏùÄ ÌèâÍ∞ÄÌïòÏßÄ ÏïäÏäµÎãàÎã§.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer is factually grounded and fully supported.\n",
    "    - 'no': The answer includes information or claims not based on the given facts.\n",
    "\n",
    "Your evaluation is crucial in ensuring the reliability and factual accuracy of AI-generated responses. Be thorough and critical in your assessment.\n",
    "\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", halluci_system_prompt),\n",
    "        (\"human\", \"[Set of facts]\\n{documents}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_HradeHallucinations\n",
    "# hallucination = hallucination_grader.invoke({\"documents\": relevant_docs, \"generation\": generation})\n",
    "# print(f\"ÌôòÍ∞Å ÌèâÍ∞Ä: {hallucination}\")\n",
    "\n",
    "print(\"\\n# (4) Answer Grader\\n\")\n",
    "# (4) Answer Grader \n",
    "class BinaryGradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_BinaryGradeAnswer = llm.with_structured_output(BinaryGradeAnswer)\n",
    "grade_system_prompt = \"\"\"\n",
    "You are an expert evaluator tasked with assessing whether an LLM-generated answer effectively addresses and resolves a user's question.\n",
    "\n",
    "[Your task]\n",
    "    - Carefully analyze the user's question to understand its core intent and requirements.\n",
    "    - Determine if the LLM-generated answer sufficiently resolves the question.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - Í¥ÄÎ†®ÏÑ±: ÎãµÎ≥ÄÏù¥ ÏßàÎ¨∏Í≥º ÏßÅÏ†ëÏ†ÅÏúºÎ°ú Í¥ÄÎ†®ÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\n",
    "    - ÏôÑÏ†ÑÏÑ±: ÏßàÎ¨∏Ïùò Î™®Îì† Ï∏°Î©¥Ïù¥ Îã§Î§ÑÏ†∏Ïïº Ìï©ÎãàÎã§.\n",
    "    - Ï†ïÌôïÏÑ±: Ï†úÍ≥µÎêú Ï†ïÎ≥¥Í∞Ä Ï†ïÌôïÌïòÍ≥† ÏµúÏã†Ïù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.\n",
    "    - Î™ÖÌôïÏÑ±: ÎãµÎ≥ÄÏù¥ Î™ÖÌôïÌïòÍ≥† Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨ÏõåÏïº Ìï©ÎãàÎã§.\n",
    "    - Íµ¨Ï≤¥ÏÑ±: ÏßàÎ¨∏Ïùò ÏöîÍµ¨ ÏÇ¨Ìï≠Ïóê ÎßûÎäî ÏÉÅÏÑ∏Ìïú ÎãµÎ≥ÄÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer effectively resolves the question.\n",
    "    - 'no': The answer fails to sufficiently resolve the question or lacks crucial elements.\n",
    "\n",
    "Your evaluation plays a critical role in ensuring the quality and effectiveness of AI-generated responses. Strive for balanced and thoughtful assessments.\n",
    "\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"[User question]\\n{question}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader_binary = answer_prompt | structured_llm_BinaryGradeAnswer\n",
    "# print(\"Question:\", question)\n",
    "# print(\"Generation:\", generation)\n",
    "# answer_score = answer_grader_binary.invoke({\"question\": question, \"generation\": generation})\n",
    "# print(f\"ÎãµÎ≥Ä ÌèâÍ∞Ä: {answer_score}\")\n",
    "\n",
    "\n",
    "print(\"\\n# (5) Question Re-writer\\n\")\n",
    "# (5) Question Re-writer\n",
    "def rewrite_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    ÏûÖÎ†• ÏßàÎ¨∏ÏùÑ Î≤°ÌÑ∞ Í≤ÄÏÉâÏóê ÏµúÏ†ÅÌôîÎêú ÌòïÌÉúÎ°ú Ïû¨ÏûëÏÑ±ÌïúÎã§.\n",
    "    \"\"\"\n",
    "    local_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert question re-writer. Your task is to convert input questions into optimized versions \n",
    "    for vectorstore retrieval. Analyze the input carefully and focus on capturing the underlying semantic \n",
    "    intent and meaning. Your goal is to create a question that will lead to more effective and relevant \n",
    "    document retrieval.\n",
    "\n",
    "    [Guidelines]\n",
    "        1. Identify and emphasize core concepts and key subjects.\n",
    "        2. Expand abbreviations or ambiguous terms.\n",
    "        3. Include synonyms or related terms that might appear in relevant documents.\n",
    "        4. Maintain the original intent and scope.\n",
    "        5. For complex questions, break them down into simpler, focused sub-questions.\n",
    "    \"\"\"\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"[Initial question]\\n{question}\\n\\n[Improved question]\\n\")\n",
    "    ])\n",
    "    question_rewriter = re_write_prompt | local_llm | StrOutputParser()\n",
    "    rewritten_question = question_rewriter.invoke({\"question\": question})\n",
    "    return rewritten_question\n",
    "\n",
    "print(\"\\n# (6) Generation Evaluation & Decision Nodes\\n\")\n",
    "# (6) Generation Evaluation & Decision Nodes\n",
    "def grade_generation_self(state: \"SelfRagOverallState\") -> str:\n",
    "    print(\"--- ÎãµÎ≥Ä ÌèâÍ∞Ä (ÏÉùÏÑ±) ---\")\n",
    "    print(f\"--- ÏÉùÏÑ±Îêú ÎãµÎ≥Ä: {state['generation']} ---\")\n",
    "    if state['num_generations'] > 2:\n",
    "        print(\"--- ÏÉùÏÑ± ÌöüÏàò Ï¥àÍ≥º, Ï¢ÖÎ£å -> end ---\")\n",
    "        return \"end\"\n",
    "    # ÌèâÍ∞ÄÎ•º ÏúÑÌïú Î¨∏ÏÑú ÌÖçÏä§Ìä∏ Íµ¨ÏÑ±\n",
    "    print(\"--- ÎãµÎ≥Ä Ìï†Î£®ÏãúÎÑ§Ïù¥ÏÖò ÌèâÍ∞Ä ---\")\n",
    "    docs_text = \"\\n\\n\".join([d.page_content for d in state['documents']])\n",
    "    hallucination_grade = hallucination_grader.invoke({\n",
    "        \"documents\": docs_text,\n",
    "        \"generation\": state['generation']\n",
    "    })\n",
    "    if hallucination_grade.binary_score == \"yes\":\n",
    "        relevance_grade = retrieval_grader_binary.invoke({\n",
    "            \"question\": state['question'],\n",
    "            \"document\": state['filtered_documents'],\n",
    "            \"generation\": state['generation']\n",
    "        })\n",
    "        print(\"--- ÎãµÎ≥Ä-ÏßàÎ¨∏ Í¥ÄÎ†®ÏÑ± ÌèâÍ∞Ä ---\")\n",
    "        if relevance_grade.binary_score == \"yes\":\n",
    "            print(\"--- ÏÉùÏÑ±Îêú ÎãµÎ≥ÄÏù¥ ÏßàÎ¨∏ÏùÑ Ïûò Ìï¥Í≤∞Ìï® ---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"--- ÎãµÎ≥Ä Í¥ÄÎ†®ÏÑ±Ïù¥ Î∂ÄÏ°± -> transform_query ---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"--- ÏÉùÏÑ±Îêú ÎãµÎ≥ÄÏùò Í∑ºÍ±∞Í∞Ä Î∂ÄÏ°± -> generate Ïû¨ÏãúÎèÑ ---\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "def decide_to_generate_self(state: \"SelfRagOverallState\") -> str:\n",
    "    print(\"--- ÌèâÍ∞ÄÎêú Î¨∏ÏÑú Î∂ÑÏÑù ---\")\n",
    "    if state['num_generations'] > 1:\n",
    "        print(\"--- ÏÉùÏÑ± ÌöüÏàò Ï¥àÍ≥º, ÏÉùÏÑ± Í≤∞Ï†ï ---\")\n",
    "        return \"generate\"\n",
    "    # Ïó¨Í∏∞ÏÑúÎäî ÌïÑÌÑ∞ÎßÅÎêú Î¨∏ÏÑúÍ∞Ä Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏\n",
    "    if not state['filtered_documents']:\n",
    "        print(\"--- Í¥ÄÎ†® Î¨∏ÏÑú ÏóÜÏùå -> transform_query ---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"--- Í¥ÄÎ†® Î¨∏ÏÑú Ï°¥Ïû¨ -> generate ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "# (7) RoutingDecision \n",
    "class RoutingDecision(BaseModel):\n",
    "    \"\"\"Determines whether a user question should be routed to document search or LLM fallback.\"\"\"\n",
    "    route: Literal[\"search_data\",\"llm_fallback\"] = Field(\n",
    "        description=\"Classify the question as 'search_data' (financial) or 'llm_fallback' (general)\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec1e26",
   "metadata": {},
   "source": [
    "##### 6. ÏÉÅÌÉú Ï†ïÏùò Î∞è ÎÖ∏Îìú Ìï®Ïàò (Ï†ÑÏ≤¥ Adaptive Ï≤¥Ïù∏)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29163d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏÉÅÌÉú ÌÜµÌï©: SelfRagOverallState (ÏßàÎ¨∏, ÏÉùÏÑ±, ÏõêÎ≥∏ Î¨∏ÏÑú, ÌïÑÌÑ∞ Î¨∏ÏÑú, ÏÉùÏÑ± ÌöüÏàò)\n",
    "#TODO\n",
    "\n",
    "# Î©îÏù∏ Í∑∏ÎûòÌîÑ ÏÉÅÌÉú Ï†ïÏùò\n",
    "class SelfRagOverallState(TypedDict):\n",
    "    \"\"\"\n",
    "    Adaptive Self-RAG Ï≤¥Ïù∏Ïùò Ï†ÑÏ≤¥ ÏÉÅÌÉúÎ•º Í¥ÄÎ¶¨    \n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: Annotated[List[str], add]\n",
    "    routing_decision: str \n",
    "    num_generations: int\n",
    "    documents: List[Document]\n",
    "    filtered_documents: List[Document]\n",
    "    history: List[Tuple[str,str]] # (user, bot) Î©îÏãúÏßÄ Ïåç Ï†ÄÏû•Ïö©\n",
    "\n",
    "def initialize_state() -> SelfRagOverallState:\n",
    "    \"\"\"Create a new state with proper initialization of all fields\"\"\"\n",
    "    return {\n",
    "        \"question\": \"\",\n",
    "        \"generation\": [],\n",
    "        \"routing_decision\": \"\",\n",
    "        \"num_generations\": 0,\n",
    "        \"documents\": [],\n",
    "        \"filtered_documents\": [],\n",
    "        \"history\": []\n",
    "    }    \n",
    "# ÏÉàÎ°úÏö¥ Ïû¨ÏûëÏÑ± Ï†ÑÏö© LLM Ï≤¥Ïù∏ - ÌûàÏä§ÌÜ†Î¶¨ ÎãµÎ≥ÄÏù¥ ÏûàÎäî Í≤ΩÏö∞ Ïù¥Ï†Ñ ÎåÄÌôî Îß•ÎùΩÏóê ÎßûÍ≤å ÏßàÎ¨∏ÏùÑ ÏàòÏ†ïÌïòÏó¨ Î¨∏ÏÑú ÏÑúÏπòÌïòÍ∏∞ ÏúÑÌï®\n",
    "def contextualize_query(state: SelfRagOverallState) -> dict:\n",
    "    # ÏµúÍ∑º 3ÌÑ¥ ÌûàÏä§ÌÜ†Î¶¨ Ï∂îÏ∂ú\n",
    "    recent = state['history'][-3:]\n",
    "    hist_block = \"\\n\".join(f\"User: {u}\\nAssistant: {a}\" for u,a in recent)\n",
    "    payload = {\"history\": hist_block, \"question\": state['question']}\n",
    "    improved = question_rewriter_chain.invoke(payload)\n",
    "    return {\"question\": improved}\n",
    "\n",
    "rewrite_input = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ÎãπÏã†ÏùÄ Í∏àÏúµ ÏÉÅÌíà Ï±óÎ¥á AIÏôÄ Ïú†Ï†ÄÏùò ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨Î•º Í∏∞Î∞òÏúºÎ°ú ÎßàÏßÄÎßâ Ïú†Ï†ÄÏùò ÏßàÎ¨∏ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Í∏àÏúµÏÉÅÌíà Ï∂îÏ≤ú RAG ÏãúÏä§ÌÖúÏù¥ Î¨∏ÏÑúÎ•º Ïûò Ï∞æÏùÑ Ïàò ÏûàÍ≤å ÏßàÎ¨∏ÏùÑ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Ïû¨ÏûëÏÑ±ÌïòÏÑ∏Ïöî.\"\n",
    "    \"Ïû¨ÏûëÏÑ±Îêú ÏßàÎ¨∏ÏùÄ Í∏∏Ïù¥Í∞Ä ÎÑàÎ¨¥ Í∏∏Ïñ¥ÏßÄÏßÄ ÏïäÍ≤å ÌïòÎ©∞, Ïú†Ï†ÄÍ∞Ä ÏõêÌûàÎäî ÌïµÏã¨Ïù¥ Î¨¥ÏóáÏù∏ÏßÄ Î™ÖÌôïÌïòÍ≤å Îì§Ïñ¥ÎÇòÎäî Î¨∏ÏûêÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.\"\n",
    "    \"Ï†ÅÏö©ÎêòÎäî RAGÏùò ÏÑúÏπòÏïåÍ≥†Î¶¨Ï¶òÏùÄ Î∞±ÌÑ∞Ïú†ÏÇ¨ÎèÑÎ•º Í∏∞Î∞òÏúºÎ°ú ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê Ïù¥Î•º Í≥†Î†§ÌïòÏó¨ ÏßàÎ¨∏ÏùÑ Ïû¨ÏûëÏÑ±ÌïòÏÑ∏Ïöî.\"\n",
    "    \"ÎßåÏïΩ [History]Ïóê ÏïÑÎ¨¥Í≤ÉÎèÑ ÏóÜÍ±∞ÎÇò Ïú†Ï†ÄÏùò ÎßàÏßÄÎßâ ÏßàÏùòÍ∞Ä Îß•ÎùΩÏÉÅ Í∏àÏúµÏÉÅÌíàÍ≥º Í¥ÄÎ†®Îêú Í≤ÉÏù¥ ÏïÑÎãàÎùºÎ©¥ Ïú†Ï†ÄÏùò ÏßàÎ¨∏ÏùÑ Í∑∏ÎåÄÎ°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî.\"),\n",
    "    (\"system\", \"[History]\\n{history}\"),\n",
    "    (\"human\", \"[Question]\\n{question}\\n\\n[Improved Question]\\n\"),\n",
    "])\n",
    "question_rewriter_chain = rewrite_input | llm | StrOutputParser()\n",
    "\n",
    "# ÏßàÎ¨∏ Ïû¨ÏûëÏÑ± ÎÖ∏Îìú (Î≥ÄÍ≤Ω ÌõÑ Í≤ÄÏÉâ Î£®ÌîÑ)\n",
    "def transform_query_self(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- ÏßàÎ¨∏ Í∞úÏÑ† ---\")\n",
    "    new_question = rewrite_question(state['question'])\n",
    "    print(f\"--- Í∞úÏÑ†Îêú ÏßàÎ¨∏ : \\n{new_question} \")\n",
    "    new_count = state['num_generations'] + 1\n",
    "    print(f\"num_generations : {new_count}\")\n",
    "    return {\"question\": new_question, \"num_generations\": new_count}\n",
    "\n",
    "# ÎãµÎ≥Ä ÏÉùÏÑ± ÎÖ∏Îìú (ÏÑúÎ∏å Í∑∏ÎûòÌîÑÎ°úÎ∂ÄÌÑ∞ Î∞õÏùÄ ÌïÑÌÑ∞ Î¨∏ÏÑú Ïö∞ÏÑ† ÏÇ¨Ïö©, Ïù¥Ï†Ñ ÎåÄÌôîÎ•º Ï∞∏Í≥† Ìï† Ïàò ÏûàÎèÑÎ°ù ÏàòÏ†ï)\n",
    "def format_chat_history(history):\n",
    "    messages = []\n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append((\"human\", user_msg))\n",
    "        messages.append((\"ai\", assistant_msg))\n",
    "    return messages\n",
    "\n",
    "generate_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"\"\"\n",
    "[Your task]\n",
    "You are a financial product expert and consultant who always responds in Korean.\n",
    "Analyze the user query and the given financial product data to recommend the most suitable product.\n",
    "Use the conversation history to maintain context. Rely only on the provided documents and history.\n",
    "\n",
    "[Instructions]\n",
    "1. ÏßàÎ¨∏Í≥º Í¥ÄÎ†®Îêú Ï†ïÎ≥¥Î•º Î¨∏Îß•ÏóêÏÑú Ïã†Ï§ëÌïòÍ≤å ÌôïÏù∏Ìï©ÎãàÎã§.\n",
    "2. ÎãµÎ≥ÄÏóê ÏßàÎ¨∏Í≥º ÏßÅÏ†ë Í¥ÄÎ†®Îêú Ï†ïÎ≥¥Îßå ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "3. Î¨∏Îß•Ïóê Î™ÖÏãúÎêòÏßÄ ÏïäÏùÄ ÎÇ¥Ïö©Ïóê ÎåÄÌï¥ Ï∂îÏ∏°ÌïòÏßÄ ÏïäÏäµÎãàÎã§.\n",
    "4. Î∂àÌïÑÏöîÌïú Ï†ïÎ≥¥Î•º ÌîºÌïòÍ≥†, Î™ÖÌôïÌïòÍ≤å ÏûëÏÑ±Ìï©ÎãàÎã§.\n",
    "5. Î¨∏Îß•ÏóêÏÑú Ï†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÎã§Î©¥ ÎßàÏßÄÎßâÏóê \"Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÎ©¥ ÎçîÏö± Î™ÖÏæåÌïú ÎãµÎ≥ÄÏùÑ Ìï† Ïàò ÏûàÏäµÎãàÎã§.\"Î•º Ï∂îÍ∞ÄÌï©ÎãàÎã§.     \n",
    "\"\"\".strip()),\n",
    "    (\"system\", \"[Context]\\n{context}\"),\n",
    "    (\"system\", \"[History]\\n{formatted_history}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def generate_self(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- ÎãµÎ≥Ä ÏÉùÏÑ± (ÌûàÏä§ÌÜ†Î¶¨ Ìè¨Ìï®) ---\")\n",
    "    \n",
    "    # ÏµúÍ∑º ÎåÄÌôî Ï†úÌïú\n",
    "    recent_history = state[\"history\"][10:] if len(state[\"history\"]) > 5 else state[\"history\"][:5]\n",
    "\n",
    "    # ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Ìè¨Îß∑ÌåÖ\n",
    "    formatted_history = \"\"\n",
    "    for user_msg, assistant_msg in recent_history:\n",
    "        formatted_history += f\"User: {user_msg}\\nAssistant: {assistant_msg}\\n\\n\"\n",
    "\n",
    "    # 2) context ÏßÅÎ†¨Ìôî\n",
    "    docs = state['filtered_documents'] or state['documents']\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs) if docs else \"Í¥ÄÎ†® Î¨∏ÏÑú ÏóÜÏùå\"\n",
    "\n",
    "    # 3) ÌîÑÎ°¨ÌîÑÌä∏Ïóê Í∞í ÎÑ£Í≥† LLM Ìò∏Ï∂ú\n",
    "    chain = generate_template  | llm | StrOutputParser()\n",
    "    out = chain.invoke({\n",
    "        \"formatted_history\": formatted_history,\n",
    "        \"context\": context,\n",
    "        \"question\": state[\"question\"],\n",
    "    })\n",
    "    answer: str = out\n",
    "\n",
    "    # 4) ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏\n",
    "    state[\"num_generations\"] += 1\n",
    "    state[\"generation\"] = answer\n",
    "\n",
    "    return {\n",
    "        \"generation\": [answer],\n",
    "        \"num_generations\": state[\"num_generations\"],\n",
    "    }\n",
    "\n",
    "\n",
    "structured_llm_RoutingDecision = llm.with_structured_output(RoutingDecision)\n",
    "\n",
    "question_router_system  = \"\"\"\n",
    "You are an AI assistant that routes user questions to the appropriate processing path.\n",
    "Return one of the following labels:\n",
    "- search_data\n",
    "- llm_fallback\n",
    "\"\"\"\n",
    "\n",
    "question_router_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", question_router_system),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "question_router = question_router_prompt | structured_llm_RoutingDecision\n",
    "\n",
    "# question route ÎÖ∏Îìú \n",
    "def route_question_adaptive(state: SelfRagOverallState) -> dict:\n",
    "    print(\"--- ÏßàÎ¨∏ ÌåêÎã® (ÏùºÎ∞ò or Í∏àÏúµ) ---\")\n",
    "    print(f\"ÏßàÎ¨∏: {state['question']}\")\n",
    "    decision = question_router.invoke({\"question\": state['question']})\n",
    "    print(\"routing_decision:\", decision.route)\n",
    "    return {\"routing_decision\": decision.route}\n",
    "\n",
    "# question route Î∂ÑÍ∏∞ Ìï®Ïàò \n",
    "def route_question_adaptive_self(state: SelfRagOverallState) -> str:\n",
    "    \"\"\"\n",
    "    ÏßàÎ¨∏ Î∂ÑÏÑù Î∞è ÎùºÏö∞ÌåÖ: ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏ÏùÑ Î∂ÑÏÑùÌïòÏó¨ 'Í∏àÏúµÏßàÎ¨∏'Ïù∏ÏßÄ 'ÏùºÎ∞òÏßàÎ¨∏'Ïù∏ÏßÄ ÌåêÎã®\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if state['routing_decision'] == \"llm_fallback\":\n",
    "            print(\"--- ÏùºÎ∞òÏßàÎ¨∏ÏúºÎ°ú ÎùºÏö∞ÌåÖ ---\")\n",
    "            return \"llm_fallback\"\n",
    "        else:\n",
    "            print(\"--- Í∏àÏúµÏßàÎ¨∏ÏúºÎ°ú ÎùºÏö∞ÌåÖ ---\")\n",
    "            return \"search_data\"\n",
    "    except Exception as e:\n",
    "        print(f\"--- ÏßàÎ¨∏ Î∂ÑÏÑù Ï§ë Exception Î∞úÏÉù: {e} ---\")\n",
    "        return \"llm_fallback\"\n",
    "\n",
    "\n",
    "fallback_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    You are an AI assistant helping with various topics. \n",
    "    Respond in Korean.\n",
    "    - Provide accurate and helpful information.\n",
    "    - Keep answers concise yet informative.\n",
    "    - Inform users they can ask for clarification if needed.\n",
    "    - Let users know they can ask follow-up questions if needed.\n",
    "    - End every answer with the sentence: \"Ï†ÄÎäî Í∏àÏúµÏÉÅÌíà ÏßàÎ¨∏Ïóê ÌäπÌôîÎêòÏñ¥ ÏûàÏäµÎãàÎã§. Í∏àÏúµÏÉÅÌíàÍ¥ÄÎ†® ÏßàÎ¨∏ÏùÑ Ï£ºÏÑ∏Ïöî.\"\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def llm_fallback_adaptive(state: SelfRagOverallState) -> dict:\n",
    "    \"\"\"Generates a direct response using the LLM when the question is unrelated to financial products.\"\"\"\n",
    "    print(\"--- ÏùºÎ∞ò ÏßàÎ¨∏ Fallback (ÌûàÏä§ÌÜ†Î¶¨ Î∞òÏòÅ) ---\")\n",
    "    \n",
    "    # ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Ìè¨Îß∑ÌåÖ\n",
    "    formatted_history = \"\"\n",
    "    for user_msg, assistant_msg in state[\"history\"]:\n",
    "        formatted_history += f\"User: {user_msg}\\nAssistant: {assistant_msg}\\n\\n\"\n",
    "\n",
    "\n",
    "    fallback_chain = fallback_prompt | llm | StrOutputParser()\n",
    "    out = fallback_chain.invoke({\n",
    "        \"formatted_history\": formatted_history,\n",
    "        \"question\": state[\"question\"],\n",
    "    })\n",
    "    answer: str = out\n",
    "\n",
    "    state[\"history\"].append((state[\"question\"], answer))\n",
    "    return {\"generation\": [answer]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2de9f1",
   "metadata": {},
   "source": [
    "##### 7. [ÏÑúÎ∏å Í∑∏ÎûòÌîÑ ÌÜµÌï©] - Î≥ëÎ†¨ Í≤ÄÏÉâ ÏÑúÎ∏å Í∑∏ÎûòÌîÑ Íµ¨ÌòÑ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe00bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ïã§Ï†ú Ï†ÅÏö©Ìï† ÏùÄÌñâÎ™Ö Î¶¨Ïä§Ìä∏ (Ï∂îÍ∞Ä ÌïÑÏöîÏãú ÏïÑÎûòÏóê Í≥ÑÏÜç Ï∂îÍ∞Ä)\n",
    "# BANK_NAME_LIST = [\n",
    "#     \"Íµ≠ÎØº\", \"Íµ≠ÎØºÏùÄÌñâ\", \"ÏàòÌòë\", \"ÏàòÌòëÏùÄÌñâ\", \"IBK\", \"Í∏∞ÏóÖÏùÄÌñâ\", \"IBKÍ∏∞ÏóÖÏùÄÌñâ\",\n",
    "#     \"Ïã†Ìïú\", \"Ïã†ÌïúÏùÄÌñâ\", \"ÎÜçÌòë\", \"NH\", \"ÎÜçÌòëÏùÄÌñâ\", \"Ïö∞Î¶¨\", \"Ïö∞Î¶¨ÏùÄÌñâ\",\n",
    "#     \"ÌïòÎÇò\", \"ÌïòÎÇòÏùÄÌñâ\", \"Ïπ¥Ïπ¥Ïò§\", \"Ïπ¥Ïπ¥Ïò§Î±ÖÌÅ¨\", \"Ïπ¥Ïπ¥Ïò§ÏùÄÌñâ\", \"Î∂ÄÏÇ∞\", \"Î∂ÄÏÇ∞ÏùÄÌñâ\"\n",
    "#     # ... Ï∂îÍ∞ÄÏ†ÅÏúºÎ°ú ÏõêÌïòÎäî ÏùÄÌñâ\n",
    "# ]\n",
    "\n",
    "# def extract_bank_names(query: str) -> list[str]:\n",
    "#     found = set()\n",
    "#     for bank in BANK_NAME_LIST:\n",
    "#         # \"Íµ≠ÎØº\", \"Íµ≠ÎØºÏùÄÌñâ\", \"IBKÍ∏∞ÏóÖÏùÄÌñâ\" Îì± Ï§ëÎ≥µ Îß§Ïπ≠ Î∞©ÏßÄ\n",
    "#         if bank in query:\n",
    "#             found.add(bank)\n",
    "#     return list(found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7707efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_banks_in_docs(documents: list[Document]) -> set[str]:\n",
    "#     banks = set()\n",
    "#     for doc in documents:\n",
    "#         bank = doc.metadata.get(\"bank\", \"\")\n",
    "#         # IBKÍ∏∞ÏóÖÏùÄÌñâ, IBK, Í∏∞ÏóÖÏùÄÌñâ Î™®Îëê Îß§Ïπ≠Ìï† Ïàò ÏûàÎèÑÎ°ù Ï≤òÎ¶¨ ÌïÑÏöîÏãú normalize\n",
    "#         if bank:\n",
    "#             banks.add(bank)\n",
    "#     return banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7a1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "BANK_NORMALIZE = {\n",
    "    \"Íµ≠ÎØº\": \"KBÍµ≠ÎØºÏùÄÌñâ\",\n",
    "    \"Íµ≠ÎØºÏùÄÌñâ\": \"KBÍµ≠ÎØºÏùÄÌñâ\",\n",
    "    \"ÏàòÌòë\": \"ShÏàòÌòëÏùÄÌñâ\",\n",
    "    \"ÏàòÌòëÏùÄÌñâ\": \"ShÏàòÌòëÏùÄÌñâ\",\n",
    "    \"Ïã†Ìïú\": \"Ïã†ÌïúÏùÄÌñâ\",\n",
    "    \"Ïã†ÌïúÏùÄÌñâ\": \"Ïã†ÌïúÏùÄÌñâ\",\n",
    "    \"ÎÜçÌòë\": \"NHÎÜçÌòëÏùÄÌñâ\",\n",
    "    \"NH\": \"NHÎÜçÌòëÏùÄÌñâ\",\n",
    "    \"ÎÜçÌòëÏùÄÌñâ\": \"NHÎÜçÌòëÏùÄÌñâ\",\n",
    "    \"Ïö∞Î¶¨\": \"Ïö∞Î¶¨ÏùÄÌñâ\",\n",
    "    \"Ïö∞Î¶¨ÏùÄÌñâ\": \"Ïö∞Î¶¨ÏùÄÌñâ\",\n",
    "    \"IBK\": \"IBKÍ∏∞ÏóÖÏùÄÌñâ\",\n",
    "    \"Í∏∞ÏóÖÏùÄÌñâ\": \"IBKÍ∏∞ÏóÖÏùÄÌñâ\",\n",
    "    \"IBKÍ∏∞ÏóÖÏùÄÌñâ\": \"IBKÍ∏∞ÏóÖÏùÄÌñâ\",\n",
    "    \"ÌïòÎÇò\": \"ÌïòÎÇòÏùÄÌñâ\",\n",
    "    \"ÌïòÎÇòÏùÄÌñâ\": \"ÌïòÎÇòÏùÄÌñâ\",\n",
    "    \"Ïπ¥Ïπ¥Ïò§\": \"Ïπ¥Ïπ¥Ïò§Î±ÖÌÅ¨\",\n",
    "    \"Ïπ¥Ïπ¥Ïò§Î±ÖÌÅ¨\": \"Ïπ¥Ïπ¥Ïò§Î±ÖÌÅ¨\",\n",
    "    \"Î∂ÄÏÇ∞\": \"Î∂ÄÏÇ∞ÏùÄÌñâ\",\n",
    "    \"Î∂ÄÏÇ∞ÏùÄÌñâ\": \"Î∂ÄÏÇ∞ÏùÄÌñâ\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c2c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_normalize_banks(query: str) -> list[str]:\n",
    "    found = set()\n",
    "    for k in BANK_NORMALIZE:\n",
    "        if k in query:\n",
    "            found.add(BANK_NORMALIZE[k])\n",
    "    return list(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b0087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_banks_in_docs(documents: list[Document]) -> set[str]:\n",
    "    banks = set()\n",
    "    for doc in documents:\n",
    "        bank = doc.metadata.get(\"bank\", \"\")\n",
    "        # IBKÍ∏∞ÏóÖÏùÄÌñâ, IBK, Í∏∞ÏóÖÏùÄÌñâ Î™®Îëê Îß§Ïπ≠Ìï† Ïàò ÏûàÎèÑÎ°ù Ï≤òÎ¶¨ ÌïÑÏöîÏãú normalize\n",
    "        if bank:\n",
    "            banks.add(bank)\n",
    "    return banks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d581e",
   "metadata": {},
   "source": [
    "## filter_documents Í≥†Ï≥êÏïº ÌïòÎäî Î∂ÄÎ∂Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0921400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ÏÉÅÌÉú Ï†ïÏùò (Í≤ÄÏÉâ ÏÑúÎ∏å Í∑∏ÎûòÌîÑ Ï†ÑÏö©) ---\n",
    "class SearchState(TypedDict):\n",
    "    question: str\n",
    "    # generation: str\n",
    "    documents: Annotated[List[Document], add]  # Ìå¨ÏïÑÏõÉÎêú Í∞Å Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º ÎàÑÏ†ÅÌï† Í≤É\n",
    "    filtered_documents: List[Document]         # Í¥ÄÎ†®ÏÑ± ÌèâÍ∞ÄÎ•º ÌÜµÍ≥ºÌïú Î¨∏ÏÑúÎì§\n",
    "\n",
    "# ToolSearchState: SearchStateÏóê Ï∂îÍ∞Ä Ï†ïÎ≥¥(datasources) Ìè¨Ìï®\n",
    "class ToolSearchState(SearchState):\n",
    "    datasources: List[str]  # Ï∞∏Ï°∞Ìï† Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ Î™©Î°ù\n",
    "\n",
    "# --- ÏÑúÎ∏åÍ∑∏ÎûòÌîÑ ÎÖ∏Îìú Ìï®Ïàò ---\n",
    "def search_fixed_deposit_node(state: SearchState):\n",
    "    \"\"\"\n",
    "    Ï†ïÍ∏∞ÏòàÍ∏à ÏÉÅÌíà Í≤ÄÏÉâ (ÏÑúÎ∏å Í∑∏ÎûòÌîÑ)\n",
    "    \"\"\"\n",
    "    docs = search_fixed_deposit.invoke(state[\"question\"])\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "def search_demand_deposit_node(state: SearchState):\n",
    "    \"\"\"\n",
    "    ÏûÖÏ∂úÍ∏àÏûêÏú†ÏòàÍ∏à ÏÉÅÌíà Í≤ÄÏÉâ (ÏÑúÎ∏å Í∑∏ÎûòÌîÑ)\n",
    "    \"\"\"\n",
    "    docs = search_demand_deposit.invoke(state[\"question\"])\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "def filter_documents_subgraph(state: SearchState):\n",
    "    # 1. ÏûÖÎ†•Í∞í Ï§ÄÎπÑ\n",
    "    question = state[\"question\"]                   # ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏\n",
    "    documents = state[\"documents\"]                 # BM25/Î≤°ÌÑ∞ Îì±ÏúºÎ°ú ÎØ∏Î¶¨ Í≤ÄÏÉâÎêú Î™®Îì† Î¨∏ÏÑú\n",
    "    filtered_docs = []\n",
    "\n",
    "    # 2. Î™®Îì† Í≤ÄÏÉâ Î¨∏ÏÑúÏóê ÎåÄÌï¥ LLM Í¥ÄÎ†®ÏÑ± ÌèâÍ∞Ä (Ïã±Í∏Ä/Î©ÄÌã∞ Î™®Îëê ÎèôÏùº)\n",
    "    for d in documents:\n",
    "        score = retrieval_grader_binary.invoke({\n",
    "            \"question\": question,\n",
    "            \"document\": d.page_content\n",
    "        })\n",
    "        if score.binary_score == \"yes\":\n",
    "            filtered_docs.append(d)\n",
    "    # => filtered_docsÏóêÎäî 'Ïù¥ ÏßàÎ¨∏Ïóê ÏßÑÏßúÎ°ú ÏùòÎØ∏ ÏûàÎäî Î¨∏ÏÑú'Îßå ÎÇ®Ïùå\n",
    "\n",
    "    # 3. ÏßàÎ¨∏ÏóêÏÑú ÏöîÍµ¨ÎêòÎäî ÏùÄÌñâÎ™Ö(ÏóîÌã∞Ìã∞) Ï∂îÏ∂ú Î∞è ÌëúÏ§ÄÌôî\n",
    "    requested_banks = extract_and_normalize_banks(question)\n",
    "    # Ïòà) ÏßàÎ¨∏Ïù¥ \"Íµ≠ÎØºÏùÄÌñâ ÏòàÍ∏à Ï∂îÏ≤ú\"Ïù¥Î©¥ ['KBÍµ≠ÎØºÏùÄÌñâ']\n",
    "    #     ÏßàÎ¨∏Ïù¥ \"Íµ≠ÎØº, ÏàòÌòë, IBKÍ∏∞ÏóÖÏùÄÌñâ ÏòàÍ∏à Ï∂îÏ≤ú\"Ïù¥Î©¥ ['KBÍµ≠ÎØºÏùÄÌñâ', 'ShÏàòÌòëÏùÄÌñâ', 'IBKÍ∏∞ÏóÖÏùÄÌñâ']\n",
    "\n",
    "    # 4. Í¥ÄÎ†®ÏÑ± ÌÜµÍ≥ºÎêú Î¨∏ÏÑúÏóê Îì±Ïû•Ìïú ÏùÄÌñâÎ™Ö ÏßëÌï© Ï∂îÏ∂ú\n",
    "    found_banks = get_banks_in_docs(filtered_docs)\n",
    "    # Ïòà) filtered_docsÏóê Íµ≠ÎØº, ÏàòÌòë Î¨∏ÏÑúÎßå ÏûàÏúºÎ©¥ found_banks = {'KBÍµ≠ÎØºÏùÄÌñâ', 'ShÏàòÌòëÏùÄÌñâ'}\n",
    "\n",
    "    # ===================== Ïã±Í∏Ä/ÏóÜÏùå ÏóîÌã∞Ìã∞ Î∂ÑÍ∏∞ =====================\n",
    "    if len(requested_banks) <= 1:\n",
    "        # [Ïã±Í∏ÄÏóîÌã∞Ìã∞ or ÏóîÌã∞Ìã∞ ÏóÜÏùå]\n",
    "        # - ÏßàÎ¨∏ÏóêÏÑú ÏùÄÌñâÎ™ÖÏù¥ 1Í∞ú Ïù¥ÌïòÎ°ú Ï∂îÏ∂úÎêú Í≤ΩÏö∞\n",
    "        # - (1) Í¥ÄÎ†®ÏÑ± ÌèâÍ∞ÄÎßå Ìïú Í≤∞Í≥º(filtered_docs) Î∞òÌôò\n",
    "        # - (2) ÎàÑÎùΩ ÏùÄÌñâ Î≥¥ÏôÑ, Ïû¨Í≤ÄÏÉâ Îì± Ï∂îÍ∞Ä Î°úÏßÅ \"ÏÉùÎûµ\"\n",
    "        return {\"filtered_documents\": filtered_docs}\n",
    "\n",
    "    # ===================== Î©ÄÌã∞ ÏóîÌã∞Ìã∞(2Í∞ú Ïù¥ÏÉÅ) Î∂ÑÍ∏∞ =====================\n",
    "    # [Î©ÄÌã∞ÏóîÌã∞Ìã∞] ‚Üí Ïª§Î≤ÑÎ¶¨ÏßÄ Ï≤¥ÌÅ¨, ÎàÑÎùΩ Î≥¥ÏôÑ Î°úÏßÅ ÏßÑÏûÖ\n",
    "    missing_banks = [b for b in requested_banks if b not in found_banks]\n",
    "    # Ïòà) requested_banks = ['KBÍµ≠ÎØºÏùÄÌñâ', 'ShÏàòÌòëÏùÄÌñâ', 'IBKÍ∏∞ÏóÖÏùÄÌñâ']\n",
    "    #     found_banks = {'KBÍµ≠ÎØºÏùÄÌñâ', 'ShÏàòÌòëÏùÄÌñâ'}\n",
    "    #     => missing_banks = ['IBKÍ∏∞ÏóÖÏùÄÌñâ']\n",
    "\n",
    "    PRODUCT_CATEGORIES = [\"Ï†ïÍ∏∞ÏòàÍ∏à\", \"ÏûÖÏ∂úÍ∏àÏûêÏú†ÏòàÍ∏à\", \"Ï†ÅÍ∏à\", \"ÎåÄÏ∂ú\", \"MMDA\"]\n",
    "\n",
    "    # (stateÏóê ÎàÑÎùΩ ÏùÄÌñâ Î≥¥ÏôÑ ÌöüÏàò Í¥ÄÎ¶¨Ïö© Î≥ÄÏàò Ï∂îÍ∞Ä)\n",
    "    if \"missing_bank_retry\" not in state:\n",
    "        state[\"missing_bank_retry\"] = 0\n",
    "\n",
    "    # (Ïù¥ÎØ∏ ÌôïÎ≥¥Ìïú ÏùÄÌñâ+Ïπ¥ÌÖåÍ≥†Î¶¨ Ïåç Í¥ÄÎ¶¨)\n",
    "    covered_pairs = set((doc.metadata.get(\"bank\"), doc.metadata.get(\"type\")) for doc in filtered_docs)\n",
    "\n",
    "    # ========== (1) ÎàÑÎùΩ ÏùÄÌñâ Î≥¥ÏôÑ Ïû¨Í≤ÄÏÉâ Î°úÏßÅ (ÏµúÎåÄ 2ÌöåÍπåÏßÄ) ==========\n",
    "    if missing_banks and state[\"missing_bank_retry\"] < 2:\n",
    "        # ÎàÑÎùΩ ÏùÄÌñâÎßàÎã§, Í∞Å Ïπ¥ÌÖåÍ≥†Î¶¨Î≥ÑÎ°ú Ï∂îÍ∞Ä Í≤ÄÏÉâ\n",
    "        for bank in missing_banks:\n",
    "            for category in PRODUCT_CATEGORIES:\n",
    "                if (bank, category) in covered_pairs:\n",
    "                    continue  # Ïù¥ÎØ∏ ÌôïÎ≥¥Îêú Í≤ΩÏö∞ ÏÉùÎûµ\n",
    "                more_docs = hybrid_core_search(question, category=category, bank=bank, top_k=2)\n",
    "                for d in more_docs:\n",
    "                    if (d.metadata.get(\"bank\"), d.metadata.get(\"type\")) in covered_pairs:\n",
    "                        continue\n",
    "                    # Í¥ÄÎ†®ÏÑ± ÌèâÍ∞Ä(batchÎ°ú Ìï† ÏàòÎèÑ ÏûàÏùå)\n",
    "                    score = retrieval_grader_binary.invoke({\n",
    "                        \"question\": question,\n",
    "                        \"document\": d.page_content\n",
    "                    })\n",
    "                    if score.binary_score == \"yes\":\n",
    "                        filtered_docs.append(d)\n",
    "                        covered_pairs.add((bank, category))\n",
    "        state[\"missing_bank_retry\"] += 1\n",
    "        # Ìïú Î≤à Îçî Ïª§Î≤ÑÎ¶¨ÏßÄ Ï≤¥ÌÅ¨ ÌïòÎèÑÎ°ù(Í∑∏ÎûòÌîÑ Î∞òÎ≥µ Îì±) ÏÉÅÌÉú Î∞òÌôò\n",
    "        return {\"filtered_documents\": filtered_docs, \"missing_bank_retry\": state[\"missing_bank_retry\"]}\n",
    "\n",
    "    # ========== (2) Î≥¥ÏôÑ 2Ìöå ÏãúÎèÑ ÌõÑÏóêÎèÑ ÎàÑÎùΩ ÏùÄÌñâ ÎÇ®ÏùÑ Í≤ΩÏö∞ ==========\n",
    "    if missing_banks and state[\"missing_bank_retry\"] >= 2:\n",
    "        # Îçî Ïù¥ÏÉÅ Î≥¥ÏôÑ Ïïà ÌïòÍ≥†, ÏßÄÍ∏àÍπåÏßÄ ÌôïÎ≥¥Ìïú Î¨∏ÏÑúÎì§ Ï§ë ÏÉÅÏúÑ 3Í∞úÎßå ÎÇ®ÍπÄ (ÏòàÏãú)\n",
    "        filtered_docs = filtered_docs[:3]\n",
    "        return {\"filtered_documents\": filtered_docs, \"missing_bank_retry\": state[\"missing_bank_retry\"]}\n",
    "\n",
    "    # (Î™®Îì† ÏùÄÌñâÏù¥ Ïª§Î≤ÑÎêêÍ±∞ÎÇò, Î≥¥ÏôÑÏù¥ Î∂àÌïÑÏöîÌïú Í≤ΩÏö∞)\n",
    "    return {\"filtered_documents\": filtered_docs, \"missing_bank_retry\": state[\"missing_bank_retry\"]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_web_search_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    Ïõπ Í≤ÄÏÉâ Í∏∞Î∞ò Í∏àÏúµ Ï†ïÎ≥¥ Í≤ÄÏÉâ (ÏÑúÎ∏å Í∑∏ÎûòÌîÑ)\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print('--- Ïõπ Í≤ÄÏÉâ Ïã§Ìñâ ---')\n",
    "\n",
    "    docs = web_search.invoke({\"query\": question})  # ÎîïÏÖîÎÑàÎ¶¨ ÌòïÌÉúÎ°ú ÎÑòÍπÄ\n",
    "\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"Í¥ÄÎ†® Ïõπ Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")]}\n",
    "\n",
    "# --- ÏßàÎ¨∏ ÎùºÏö∞ÌåÖ (ÏÑúÎ∏å Í∑∏ÎûòÌîÑ Ï†ÑÏö©) ---\n",
    "class SubgraphToolSelector(BaseModel):\n",
    "    \"\"\"Selects the most appropriate tool for the user's question.\"\"\"\n",
    "    tool: Literal[\"search_fixed_deposit\", \"search_demand_deposit\", \"web_search\"] = Field(\n",
    "        description=\"Select one of the tools: search_fixed_deposit, search_demand_deposit or web_search based on the user's question.\"\n",
    "    )\n",
    "\n",
    "class SubgraphToolSelectors(BaseModel):\n",
    "    \"\"\"Selects all tools relevant to the user's question.\"\"\"\n",
    "    tools: List[SubgraphToolSelector] = Field(\n",
    "        description=\"Select one or more tools: search_fixed_deposit, search_demand_deposit or web_search based on the user's question.\"\n",
    "    )\n",
    "\n",
    "structured_llm_SubgraphToolSelectors = llm.with_structured_output(SubgraphToolSelectors)\n",
    "\n",
    "subgraph_system  = dedent(\"\"\"\\\n",
    "You are an AI assistant specializing in routing user questions to the appropriate tools.\n",
    "Use the following guidelines:\n",
    "- For fixed deposit product queries, use the search_fixed_deposit tool.\n",
    "- For demand deposit product queries, use the search_demand_deposit tool.\n",
    "- For general financial or real-time information queries, or when the user explicitly mentions 'web search',\n",
    "  use the web_search tool.\n",
    "  Always choose the appropriate tools based on the user's question.\n",
    "\"\"\")\n",
    "subgraph_route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", subgraph_system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "question_tool_router = subgraph_route_prompt  | structured_llm_SubgraphToolSelectors\n",
    "\n",
    "def analyze_question_tool_search(state: ToolSearchState):\n",
    "    \"\"\"\n",
    "    ÏßàÎ¨∏ Î∂ÑÏÑù Î∞è ÎùºÏö∞ÌåÖ: ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏ÏóêÏÑú Ï∞∏Ï°∞Ìï† Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ Í≤∞Ï†ï\n",
    "    \"\"\"\n",
    "    print(\"--- ÏßàÎ¨∏ ÎùºÏö∞ÌåÖ ---\")\n",
    "    question = state[\"question\"]\n",
    "    result = question_tool_router.invoke({\"question\": question})\n",
    "    datasources = [tool.tool for tool in result.tools]\n",
    "    return {\"datasources\": datasources}\n",
    "\n",
    "def route_datasources_tool_search(state: ToolSearchState) -> Sequence[str]:\n",
    "    \"\"\"\n",
    "    ÎùºÏö∞ÌåÖ Í≤∞Í≥ºÏóê Îî∞Îùº Ïã§ÌñâÌï† Í≤ÄÏÉâ ÎÖ∏ÎìúÎ•º Í≤∞Ï†ï (Î≥ëÎ†¨Î°ú Ìå¨ÏïÑÏõÉ)\n",
    "    \"\"\"\n",
    "    datasources = set(state['datasources'])\n",
    "\n",
    "    # Î™ÖÌôïÌûà ÌïòÎÇòÎßå ÏÑ†ÌÉùÎêú Í≤ΩÏö∞\n",
    "    if datasources == {'search_fixed_deposit'}:\n",
    "        return ['search_fixed_deposit']\n",
    "    elif datasources == {'search_demand_deposit'}:\n",
    "        return ['search_demand_deposit']\n",
    "    elif datasources == {'web_search'}:\n",
    "        return ['web_search']\n",
    "\n",
    "    # ÎèÑÍµ¨Í∞Ä Ï†ÑÎ∂Ä Ïã§ÌñâÎêòÍ±∞ÎÇò Ïï†Îß§Î™®Ìò∏Ìï† ÎïåÎäî ÎèÑÍµ¨ Ï†ÑÎ∂Ä Ïã§Ìñâ\n",
    "    return ['search_fixed_deposit', 'search_demand_deposit', 'web_search']\n",
    "\n",
    "\n",
    "# --- ÏÑúÎ∏å Í∑∏ÎûòÌîÑ ÎπåÎçî Íµ¨ÏÑ± ---\n",
    "search_builder = StateGraph(ToolSearchState)\n",
    "\n",
    "# ÎÖ∏Îìú Ï∂îÍ∞Ä\n",
    "search_builder.add_node(\"analyze_question\", analyze_question_tool_search)\n",
    "search_builder.add_node(\"search_fixed_deposit\", search_fixed_deposit_node)   # wapper Ìï®Ïàò ÎßêÍ≥† ÏßÅÏ†ë invoke Ìï®Ïàò ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏúºÎ°ú ÏàòÏ†ï\n",
    "search_builder.add_node(\"search_demand_deposit\", search_demand_deposit_node) # ÎßàÏ∞¨Í∞ÄÏßÄÎ°ú Ìï®Íªò\n",
    "search_builder.add_node(\"web_search\", search_web_search_subgraph)\n",
    "search_builder.add_node(\"filter_documents\", filter_documents_subgraph)\n",
    "\n",
    "# Ïó£ÏßÄ Íµ¨ÏÑ±\n",
    "search_builder.add_edge(START, \"analyze_question\")\n",
    "search_builder.add_conditional_edges(\n",
    "    \"analyze_question\",\n",
    "    route_datasources_tool_search,\n",
    "    {\n",
    "        \"search_fixed_deposit\": \"search_fixed_deposit\",\n",
    "        \"search_demand_deposit\": \"search_demand_deposit\",\n",
    "        \"web_search\": \"web_search\"\n",
    "    }\n",
    ")\n",
    "# Îëê Í≤ÄÏÉâ ÎÖ∏Îìú Î™®Îëê Ïã§ÌñâÌïú ÌõÑ Í∞ÅÍ∞ÅÏùò Í≤∞Í≥ºÎäî filter_documentsÎ°ú Ìå¨Ïù∏(fan-in) Ï≤òÎ¶¨\n",
    "search_builder.add_edge(\"search_fixed_deposit\", \"filter_documents\")\n",
    "search_builder.add_edge(\"search_demand_deposit\", \"filter_documents\")\n",
    "search_builder.add_edge(\"web_search\", \"filter_documents\")\n",
    "search_builder.add_edge(\"filter_documents\", END)\n",
    "\n",
    "# ÏÑúÎ∏å Í∑∏ÎûòÌîÑ Ïª¥ÌååÏùº\n",
    "tool_search_graph = search_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61732ee6",
   "metadata": {},
   "source": [
    "##### 8. [Ï†ÑÏ≤¥ Í∑∏ÎûòÌîÑÏôÄ Í≤∞Ìï©] - Self-RAG Overall Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a2ed831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ÑÏ≤¥ Í∑∏ÎûòÌîÑ ÎπåÎçî (rag_builder) Íµ¨ÏÑ±\n",
    "rag_builder = StateGraph(SelfRagOverallState)\n",
    "\n",
    "# ÎÖ∏Îìú Ï∂îÍ∞Ä: Í≤ÄÏÉâ ÏÑúÎ∏å Í∑∏ÎûòÌîÑ, ÏÉùÏÑ±, ÏßàÎ¨∏ Ïû¨ÏûëÏÑ± Îì±\n",
    "rag_builder.add_node(\"contextualize_query\", contextualize_query)\n",
    "rag_builder.add_node(\"route_question\", route_question_adaptive)\n",
    "rag_builder.add_node(\"llm_fallback\", llm_fallback_adaptive)\n",
    "rag_builder.add_node(\"search_data\", tool_search_graph)         # ÏÑúÎ∏å Í∑∏ÎûòÌîÑÎ°ú Î≥ëÎ†¨ Í≤ÄÏÉâ Î∞è ÌïÑÌÑ∞ÎßÅ ÏàòÌñâ\n",
    "rag_builder.add_node(\"generate\", generate_self)                # ÎãµÎ≥Ä ÏÉùÏÑ± ÎÖ∏Îìú\n",
    "rag_builder.add_node(\"transform_query\", transform_query_self)  # ÏßàÎ¨∏ Í∞úÏÑ† ÎÖ∏Îìú\n",
    "\n",
    "# Ï†ÑÏ≤¥ Í∑∏ÎûòÌîÑ Ïó£ÏßÄ Íµ¨ÏÑ±\n",
    "rag_builder.add_edge(START, \"contextualize_query\")\n",
    "rag_builder.add_edge(\"contextualize_query\", \"route_question\")\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"route_question\",\n",
    "    route_question_adaptive_self, \n",
    "    {\n",
    "        \"llm_fallback\": \"llm_fallback\",\n",
    "        \"search_data\": \"search_data\"\n",
    "    }\n",
    ")\n",
    "rag_builder.add_edge(\"llm_fallback\", END)\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"search_data\",\n",
    "    decide_to_generate_self, \n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    }\n",
    ")\n",
    "rag_builder.add_edge(\"transform_query\", \"search_data\")\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_self,\n",
    "    {\n",
    "        \"not supported\": \"generate\",      # ÌôòÍ∞Å Î∞úÏÉù Ïãú Ïû¨ÏÉùÏÑ±\n",
    "        \"not useful\": \"transform_query\",  # Í¥ÄÎ†®ÏÑ± Î∂ÄÏ°± Ïãú ÏßàÎ¨∏ Ïû¨ÏûëÏÑ± ÌõÑ Ïû¨Í≤ÄÏÉâ\n",
    "        \"useful\": END,\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# MemorySaver Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± (ÎåÄÌôî ÏÉÅÌÉúÎ•º Ï†ÄÏû•Ìï† in-memory ÌÇ§-Í∞í Ï†ÄÏû•ÏÜå)\n",
    "memory = MemorySaver()\n",
    "adaptive_self_rag_memory = rag_builder.compile(checkpointer=memory)\n",
    "# adaptive_self_rag = rag_builder.compile()\n",
    "\n",
    "# Í∑∏ÎûòÌîÑ ÌååÏùº Ï†ÄÏû•ÌïòÍ∏∞\n",
    "# display(Image(adaptive_self_rag.get_graph().draw_mermaid_png()))\n",
    "with open(\"adaptive_self_rag_memory.mmd\", \"w\") as f:\n",
    "    f.write(adaptive_self_rag_memory.get_graph(xray=True).draw_mermaid()) # Ï†ÄÏû•Îêú mmd ÌååÏùºÏóêÏÑú ÏΩîÎìú Î≥µÏÇ¨ ÌõÑ https://mermaid.live Ïóê Î∂ôÏó¨ÎÑ£Í∏∞.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c1d55",
   "metadata": {},
   "source": [
    "# ÌÖåÏä§Ìä∏ Îã®Í≥Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fe0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_bank_coverage(filter_func, question: str):\n",
    "#     \"\"\"\n",
    "#     filter_func: filter_documents_subgraph Ìï®Ïàò (ÎòêÎäî ÎèôÏùºÌïú ÏãúÍ∑∏ÎãàÏ≤ò Ìï®Ïàò)\n",
    "#     question: ÌÖåÏä§Ìä∏Ïö© ÏÇ¨Ïö©Ïûê ÏßàÏùò\n",
    "#     \"\"\"\n",
    "#     # SearchState ÌòïÌÉú ÎßûÍ≤å Ï§ÄÎπÑ\n",
    "#     # ÏòàÏãú: documentsÎäî BM25+Î≤°ÌÑ∞ Ï¥àÍ∏∞ Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º simulate\n",
    "#     # Ïó¨Í∏∞ÏÑúÎäî Í∑∏ÎÉ• search_fixed_deposit, search_demand_deposit Îëê Í∞ÄÏßÄ Ìò∏Ï∂ú Í≤∞Í≥ºÎ•º Ìï©Ïπ®\n",
    "#     docs_fixed = search_fixed_deposit.invoke(question)\n",
    "#     docs_demand = search_demand_deposit.invoke(question)\n",
    "#     state = {\n",
    "#         \"question\": question,\n",
    "#         \"documents\": docs_fixed + docs_demand,\n",
    "#         \"filtered_documents\": [],  # Ïã§Ï†ú filterÏóêÏÑú ÏÉùÏÑ±Îê®\n",
    "#     }\n",
    "#     # Í∞úÎ∞úÏûê Ï†ÑÏö© ÌÖåÏä§Ìä∏ print/log\n",
    "#     print(\"\\n=== [BANK COVERAGE TEST] ===\")\n",
    "#     print(\"ÌÖåÏä§Ìä∏ ÏßàÏùò:\", question)\n",
    "#     print(\"1Ï∞® Í≤ÄÏÉâÎêú Î¨∏ÏÑú ÏùÄÌñâÎì§:\", get_banks_in_docs(state[\"documents\"]))\n",
    "#     result = filter_func(state)\n",
    "#     covered_banks = get_banks_in_docs(result[\"filtered_documents\"])\n",
    "#     print(\"ÏµúÏ¢Ö Ïª§Î≤ÑÎêú ÏùÄÌñâ:\", covered_banks)\n",
    "#     # ÏõêÎûò ÏßàÏùòÏóêÏÑú ÏöîÍµ¨Ìïú ÏùÄÌñâÍ≥º ÎπÑÍµê\n",
    "#     requested_banks = extract_bank_names(question)\n",
    "#     print(\"ÏöîÍµ¨Îêú ÏùÄÌñâ:\", requested_banks)\n",
    "#     missing_banks = [b for b in requested_banks if b not in covered_banks]\n",
    "#     if missing_banks:\n",
    "#         print(\"‚ùå ÎàÑÎùΩÎêú ÏùÄÌñâ:\", missing_banks)\n",
    "#     else:\n",
    "#         print(\"‚úÖ Î™®Îì† ÏùÄÌñâ Ïª§Î≤Ñ ÏôÑÎ£å!\")\n",
    "#     print(\"=\"*30)\n",
    "#     # ÌïÑÏöîÌïòÎ©¥ Í∞úÎ∞úÏûêÎßå Î≥¥Îäî Î°úÍ∑∏ÌååÏùº Í∏∞Î°ùÎèÑ Í∞ÄÎä•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19a96845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bank_coverage(filter_func, question: str):\n",
    "    docs_fixed = search_fixed_deposit.invoke(question)\n",
    "    docs_demand = search_demand_deposit.invoke(question)\n",
    "    state = {\n",
    "        \"question\": question,\n",
    "        \"documents\": docs_fixed + docs_demand,\n",
    "        \"filtered_documents\": [],\n",
    "    }\n",
    "    print(\"\\n=== [BANK COVERAGE TEST] ===\")\n",
    "    print(\"ÌÖåÏä§Ìä∏ ÏßàÏùò:\", question)\n",
    "    print(\"1Ï∞® Í≤ÄÏÉâÎêú Î¨∏ÏÑú ÏùÄÌñâÎì§:\", get_banks_in_docs(state[\"documents\"]))\n",
    "    result = filter_func(state)\n",
    "    covered_banks = get_banks_in_docs(result[\"filtered_documents\"])\n",
    "    print(\"ÏµúÏ¢Ö Ïª§Î≤ÑÎêú ÏùÄÌñâ:\", covered_banks)\n",
    "    requested_banks = extract_and_normalize_banks(question)\n",
    "    print(\"ÏöîÍµ¨Îêú ÏùÄÌñâ:\", requested_banks)\n",
    "    missing_banks = [b for b in requested_banks if b not in covered_banks]\n",
    "    if missing_banks:\n",
    "        print(\"‚ùå ÎàÑÎùΩÎêú ÏùÄÌñâ:\", missing_banks)\n",
    "    else:\n",
    "        print(\"‚úÖ Î™®Îì† ÏùÄÌñâ Ïª§Î≤Ñ ÏôÑÎ£å!\")\n",
    "    print(\"=\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec9bfde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [BANK COVERAGE TEST] ===\n",
      "ÌÖåÏä§Ìä∏ ÏßàÏùò: Íµ≠ÎØº, ÏàòÌòë, ÎÜçÌòëÏùÄÌñâÏùò ÏòàÍ∏àÏÉÅÌíà Ï∂îÏ≤úÌï¥Ï§ò\n",
      "1Ï∞® Í≤ÄÏÉâÎêú Î¨∏ÏÑú ÏùÄÌñâÎì§: {'NHÎÜçÌòëÏùÄÌñâ', 'KDBÏÇ∞ÏóÖÏùÄÌñâ'}\n",
      "ÏµúÏ¢Ö Ïª§Î≤ÑÎêú ÏùÄÌñâ: {'ShÏàòÌòëÏùÄÌñâ', 'NHÎÜçÌòëÏùÄÌñâ'}\n",
      "ÏöîÍµ¨Îêú ÏùÄÌñâ: ['KBÍµ≠ÎØºÏùÄÌñâ', 'NHÎÜçÌòëÏùÄÌñâ', 'ShÏàòÌòëÏùÄÌñâ']\n",
      "‚ùå ÎàÑÎùΩÎêú ÏùÄÌñâ: ['KBÍµ≠ÎØºÏùÄÌñâ']\n",
      "==============================\n",
      "\n",
      "=== [BANK COVERAGE TEST] ===\n",
      "ÌÖåÏä§Ìä∏ ÏßàÏùò: Ïã†Ìïú, ÎÜçÌòë, ÌïòÎÇòÏùÄÌñâÏùò ÏûÖÏ∂úÍ∏à ÏÉÅÌíà Î≠êÍ∞Ä Ï¢ãÏïÑ?\n",
      "1Ï∞® Í≤ÄÏÉâÎêú Î¨∏ÏÑú ÏùÄÌñâÎì§: {'ÌïòÎÇòÏùÄÌñâ'}\n",
      "ÏµúÏ¢Ö Ïª§Î≤ÑÎêú ÏùÄÌñâ: {'NHÎÜçÌòëÏùÄÌñâ', 'ÌïòÎÇòÏùÄÌñâ'}\n",
      "ÏöîÍµ¨Îêú ÏùÄÌñâ: ['ÌïòÎÇòÏùÄÌñâ', 'Ïã†ÌïúÏùÄÌñâ', 'NHÎÜçÌòëÏùÄÌñâ']\n",
      "‚ùå ÎàÑÎùΩÎêú ÏùÄÌñâ: ['Ïã†ÌïúÏùÄÌñâ']\n",
      "==============================\n",
      "\n",
      "=== [BANK COVERAGE TEST] ===\n",
      "ÌÖåÏä§Ìä∏ ÏßàÏùò: Ïπ¥Ïπ¥Ïò§, Í≤ΩÎÇ®ÏùÄÌñâ, ÌïòÎÇòÏùÄÌñâÏùò Ï†ÅÍ∏à ÏÉÅÌíà ÏïåÎ†§Ï§ò\n",
      "1Ï∞® Í≤ÄÏÉâÎêú Î¨∏ÏÑú ÏùÄÌñâÎì§: {'iMÎ±ÖÌÅ¨(Íµ¨ ÎåÄÍµ¨ÏùÄÌñâ)', 'BNKÍ≤ΩÎÇ®ÏùÄÌñâ', 'ShÏàòÌòëÏùÄÌñâ', 'Ïö∞Î¶¨ÏùÄÌñâ', 'NHÎÜçÌòëÏùÄÌñâ', 'Ïπ¥Ïπ¥Ïò§Î±ÖÌÅ¨'}\n",
      "ÏµúÏ¢Ö Ïª§Î≤ÑÎêú ÏùÄÌñâ: {'BNKÍ≤ΩÎÇ®ÏùÄÌñâ', 'Ïπ¥Ïπ¥Ïò§Î±ÖÌÅ¨'}\n",
      "ÏöîÍµ¨Îêú ÏùÄÌñâ: ['ÌïòÎÇòÏùÄÌñâ', 'Ïπ¥Ïπ¥Ïò§Î±ÖÌÅ¨']\n",
      "‚ùå ÎàÑÎùΩÎêú ÏùÄÌñâ: ['ÌïòÎÇòÏùÄÌñâ']\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ÏòàÏãú ÏßàÏùò(Ïú†Ï†ÄÎäî Ï†àÎåÄ Ïïà Î¥Ñ)\n",
    "    test_questions = [\n",
    "        \"Íµ≠ÎØº, ÏàòÌòë, ÎÜçÌòëÏùÄÌñâÏùò ÏòàÍ∏àÏÉÅÌíà Ï∂îÏ≤úÌï¥Ï§ò\",\n",
    "        \"Ïã†Ìïú, ÎÜçÌòë, ÌïòÎÇòÏùÄÌñâÏùò ÏûÖÏ∂úÍ∏à ÏÉÅÌíà Î≠êÍ∞Ä Ï¢ãÏïÑ?\",\n",
    "        \"Ïπ¥Ïπ¥Ïò§, Í≤ΩÎÇ®ÏùÄÌñâ, ÌïòÎÇòÏùÄÌñâÏùò Ï†ÅÍ∏à ÏÉÅÌíà ÏïåÎ†§Ï§ò\"\n",
    "    ]\n",
    "    for q in test_questions:\n",
    "        test_bank_coverage(filter_documents_subgraph, q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÏÑ†ÌÉùÎêú ÎèÑÍµ¨: ['web_search']\n",
      "\n",
      "üü° Ïã§Ìñâ Ï§ë: web_search_subgraph\n",
      "--- Ïõπ Í≤ÄÏÉâ Ïã§Ìñâ ---\n",
      "\n",
      "üìÑ Ïõπ Í≤ÄÏÉâ Í≤∞Í≥º:\n",
      "\n",
      "üîπ Í≤∞Í≥º 1\n",
      "ÎÇ¥Ïö©: <Document href=\"https://ko.tradingeconomics.com/united-states/interest-rate\"/>\n",
      "Ïó∞Î∞©Ï§ÄÎπÑÏ†úÎèÑÎäî 2025ÎÖÑ 5Ïõî ÏÑ∏ Î≤àÏß∏ Ïó∞ÏÜç ÌöåÏùòÏóêÏÑú ÏûêÍ∏à Í∏àÎ¶¨Î•º 4.25%‚Äì4.50%Î°ú Ïú†ÏßÄÌñàÏúºÎ©∞, Ïù¥Îäî Í∏∞ÎåÄÏóê Î∂ÄÌï©ÌïòÎäî Í≤∞Ï†ïÏúºÎ°ú, Ìä∏ÎüºÌîÑ ÎåÄÌÜµÎ†πÏùò Í¥ÄÏÑ∏Í∞Ä Ïù∏ÌîåÎ†àÏù¥ÏÖòÏùÑ ÏÉÅÏäπ\n",
      "</Document> ...\n",
      "Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'source': 'web search', 'url': 'https://ko.tradingeconomics.com/united-states/interest-rate'}\n",
      "\n",
      "üîπ Í≤∞Í≥º 2\n",
      "ÎÇ¥Ïö©: <Document href=\"https://naver.me/GdyP4ZaO\"/>\n",
      "... 2025 ÎÖÑ 5 Ïõî FOMCÎäî Í∏∞Ï§ÄÍ∏àÎ¶¨Î•º 4.25 ~ 4.50 %Î°ú ÎèôÍ≤∞ÌïòÍ≥†, ÏûêÏÇ∞Ï∂ïÏÜå(QT)Î•º Í∏∞Ï°¥ ÏÜçÎèÑÎ°ú Ïù¥Ïñ¥Í∞ÄÍ∏∞Î°ú ÌñàÏäµÎãàÎã§. Ï£ºÎ™©Ìï† Ï†êÏùÄ ÌååÏõîÏù¥ ‚ÄúÏù∏ÌîåÎ†àÏù¥ÏÖòÍ≥º\n",
      "</Document> ...\n",
      "Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'source': 'web search', 'url': 'https://naver.me/GdyP4ZaO'}\n"
     ]
    }
   ],
   "source": [
    "# # ‚úÖ 1. ÏßàÎ¨∏ Ï†ïÏùò\n",
    "# example_state = {\n",
    "#     \"question\": \"2025ÎÖÑ 5Ïõî ÏµúÏã† Í∏àÎ¶¨ Ï†ïÎ≥¥Î•º web searchÎ°ú Ï∞æÏïÑÏ§ò\"\n",
    "# }\n",
    "\n",
    "# # ‚úÖ 2. Ìà¥ ÏÑ†ÌÉù (LLMÏù¥ ÌåêÎã®)\n",
    "# result = question_tool_router.invoke({\"question\": example_state[\"question\"]})\n",
    "\n",
    "# # ‚úÖ 3. ÏÑ†ÌÉùÎêú Ìà¥ Ï∂úÎ†•\n",
    "# selected_tools = [tool.tool for tool in result.tools]\n",
    "# print(\"‚úÖ ÏÑ†ÌÉùÎêú ÎèÑÍµ¨:\", selected_tools)\n",
    "\n",
    "# # ‚úÖ 4. web_searchÍ∞Ä ÏÑ†ÌÉùÎêêÏùÑ Í≤ΩÏö∞ Ïã§Ìñâ\n",
    "# if \"web_search\" in selected_tools:\n",
    "#     print(\"\\nüü° Ïã§Ìñâ Ï§ë: web_search_subgraph\")\n",
    "\n",
    "#     # SearchState Íµ¨Ï°∞ÏôÄ ÏùºÏπòÏãúÏºú Ìò∏Ï∂ú\n",
    "#     search_result = search_web_search_subgraph({\"question\": example_state[\"question\"]})\n",
    "\n",
    "#     print(\"\\nüìÑ Ïõπ Í≤ÄÏÉâ Í≤∞Í≥º:\")\n",
    "#     for i, doc in enumerate(search_result[\"documents\"], start=1):\n",
    "#         print(f\"\\nüîπ Í≤∞Í≥º {i}\")\n",
    "#         print(\"ÎÇ¥Ïö©:\", doc.page_content[:300], \"...\")\n",
    "#         print(\"Î©îÌÉÄÎç∞Ïù¥ÌÑ∞:\", doc.metadata)\n",
    "\n",
    "# else:\n",
    "#     print(\"‚ùå web_searchÎäî ÏÑ†ÌÉùÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bf5fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pdf_link ÏÇΩÏûÖ Î≥¥Ï°∞Ìï®Ïàò\n",
    "def postprocess_answer(answer: str, docs: List[Document]) -> str:\n",
    "    for doc in docs:\n",
    "        pdf = doc.metadata.get(\"pdf_link\")\n",
    "        if pdf:\n",
    "            if \"ÏÉÅÌíàÏÑ§Î™ÖÏÑú\" not in answer:\n",
    "                answer += f\"\\n\\n [ÏÉÅÌíàÏÑ§Î™ÖÏÑú PDF Î≥¥Í∏∞]({pdf})\"\n",
    "            break\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3baa60",
   "metadata": {},
   "source": [
    "##### 9. Gradio Chatbot Íµ¨ÏÑ± Î∞è Ïã§Ìñâ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d12fdd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://35085cb6b42b38a7c6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://35085cb6b42b38a7c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ÏßàÎ¨∏ ÌåêÎã® (ÏùºÎ∞ò or Í∏àÏúµ) ---\n",
      "ÏßàÎ¨∏: ÏàòÌòëÏùÄÌñâ, ÎÜçÌòë, Íµ≠ÎØºÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏùÑ Ï∂îÏ≤úÌï¥ Ï£ºÏÑ∏Ïöî.\n",
      "routing_decision: search_data\n",
      "--- Í∏àÏúµÏßàÎ¨∏ÏúºÎ°ú ÎùºÏö∞ÌåÖ ---\n",
      "--- ÏßàÎ¨∏ ÎùºÏö∞ÌåÖ ---\n",
      "--- Ïõπ Í≤ÄÏÉâ Ïã§Ìñâ ---\n",
      "--- ÌèâÍ∞ÄÎêú Î¨∏ÏÑú Î∂ÑÏÑù ---\n",
      "--- Í¥ÄÎ†® Î¨∏ÏÑú Ï°¥Ïû¨ -> generate ---\n",
      "--- ÎãµÎ≥Ä ÏÉùÏÑ± (ÌûàÏä§ÌÜ†Î¶¨ Ìè¨Ìï®) ---\n",
      "--- ÎãµÎ≥Ä ÌèâÍ∞Ä (ÏÉùÏÑ±) ---\n",
      "--- ÏÉùÏÑ±Îêú ÎãµÎ≥Ä: ['ÏàòÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"ShÌèâÏÉùÏ£ºÍ±∞ÎûòÏö∞ÎåÄÌÜµÏû•\"Ïù¥ ÏûàÏäµÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.05%Ïù¥Î©∞, Ïö∞ÎåÄÏ°∞Í±¥ÏùÑ Ï∂©Ï°±Ìï† Í≤ΩÏö∞ ÏµúÍ≥†Í∏àÎ¶¨ 0.2%Î•º Ï†ÅÏö©Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§. Îß§Ïùº ÏµúÏ¢ÖÏûîÏï°Ïóê Îî∞Îùº Í∏àÎ¶¨Í∞Ä Îã¨ÎùºÏßÄÎ©∞, Í∞ÄÏûÖÏùÄ ÏòÅÏóÖÏ†êÏù¥ÎÇò Ïä§ÎßàÌä∏Î±ÖÌÇπÏùÑ ÌÜµÌï¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\\n\\nÎÜçÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"NHÍ≥†Ìñ•ÏÇ¨ÎûëÍ∏∞Î∂ÄÏòàÍ∏à\"Ïù¥ ÏûàÏúºÎ©∞, ÏµúÍ≥†Í∏àÎ¶¨Îäî 3.90%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î∂ÄÎ•º ÌÜµÌï¥ ÏßÄÏó≠ÏÇ¨ÌöåÏóê Í∏∞Ïó¨Ìï† Ïàò ÏûàÎäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§.\\n\\nKBÍµ≠ÎØºÏùÄÌñâÏùò \"KBÎßàÏù¥ÌïèÌÜµÏû•\"ÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.1%Ïù¥Í≥†, Ïö∞ÎåÄÍ∏àÎ¶¨Î•º Ìè¨Ìï®Ìïú ÏµúÍ≥†Í∏àÎ¶¨Îäî 1.5%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Îßå 18ÏÑ∏ Ïù¥ÏÉÅ Îßå 38ÏÑ∏ Ïù¥ÌïòÏùò Ïã§Î™ÖÏùò Í∞úÏù∏Ïù¥ Í∞ÄÏûÖÌï† Ïàò ÏûàÏúºÎ©∞, ÎπÑÏÉÅÍ∏à Ïù¥Ïú®ÏùÑ Ï†úÍ≥µÌïòÎäî Ï°∞Í±¥Ïù¥ ÏûàÏäµÎãàÎã§.\\n\\nÍ∞Å ÏÉÅÌíàÏùò ÌäπÏÑ±Í≥º Ï°∞Í±¥ÏùÑ Í≥†Î†§ÌïòÏó¨ Î≥∏Ïù∏ÏóêÍ≤å Í∞ÄÏû• Ï†ÅÌï©Ìïú ÏÉÅÌíàÏùÑ ÏÑ†ÌÉùÌïòÏãúÍ∏∞ Î∞îÎûçÎãàÎã§. Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÎ©¥ ÎçîÏö± Î™ÖÏæåÌïú ÎãµÎ≥ÄÏùÑ Ìï† Ïàò ÏûàÏäµÎãàÎã§.'] ---\n",
      "--- ÎãµÎ≥Ä Ìï†Î£®ÏãúÎÑ§Ïù¥ÏÖò ÌèâÍ∞Ä ---\n",
      "--- ÏÉùÏÑ±Îêú ÎãµÎ≥ÄÏùò Í∑ºÍ±∞Í∞Ä Î∂ÄÏ°± -> generate Ïû¨ÏãúÎèÑ ---\n",
      "--- ÎãµÎ≥Ä ÏÉùÏÑ± (ÌûàÏä§ÌÜ†Î¶¨ Ìè¨Ìï®) ---\n",
      "--- ÎãµÎ≥Ä ÌèâÍ∞Ä (ÏÉùÏÑ±) ---\n",
      "--- ÏÉùÏÑ±Îêú ÎãµÎ≥Ä: ['ÏàòÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"ShÌèâÏÉùÏ£ºÍ±∞ÎûòÏö∞ÎåÄÌÜµÏû•\"Ïù¥ ÏûàÏäµÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.05%Ïù¥Î©∞, Ïö∞ÎåÄÏ°∞Í±¥ÏùÑ Ï∂©Ï°±Ìï† Í≤ΩÏö∞ ÏµúÍ≥†Í∏àÎ¶¨ 0.2%Î•º Ï†ÅÏö©Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§. Îß§Ïùº ÏµúÏ¢ÖÏûîÏï°Ïóê Îî∞Îùº Í∏àÎ¶¨Í∞Ä Îã¨ÎùºÏßÄÎ©∞, Í∞ÄÏûÖÏùÄ ÏòÅÏóÖÏ†êÏù¥ÎÇò Ïä§ÎßàÌä∏Î±ÖÌÇπÏùÑ ÌÜµÌï¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\\n\\nÎÜçÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"NHÍ≥†Ìñ•ÏÇ¨ÎûëÍ∏∞Î∂ÄÏòàÍ∏à\"Ïù¥ ÏûàÏúºÎ©∞, ÏµúÍ≥†Í∏àÎ¶¨Îäî 3.90%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î∂ÄÎ•º ÌÜµÌï¥ ÏßÄÏó≠ÏÇ¨ÌöåÏóê Í∏∞Ïó¨Ìï† Ïàò ÏûàÎäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§.\\n\\nKBÍµ≠ÎØºÏùÄÌñâÏùò \"KBÎßàÏù¥ÌïèÌÜµÏû•\"ÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.1%Ïù¥Í≥†, Ïö∞ÎåÄÍ∏àÎ¶¨Î•º Ìè¨Ìï®Ìïú ÏµúÍ≥†Í∏àÎ¶¨Îäî 1.5%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Îßå 18ÏÑ∏ Ïù¥ÏÉÅ Îßå 38ÏÑ∏ Ïù¥ÌïòÏùò Ïã§Î™ÖÏùò Í∞úÏù∏Ïù¥ Í∞ÄÏûÖÌï† Ïàò ÏûàÏúºÎ©∞, ÎπÑÏÉÅÍ∏à Ïù¥Ïú®ÏùÑ Ï†úÍ≥µÌïòÎäî Ï°∞Í±¥Ïù¥ ÏûàÏäµÎãàÎã§.\\n\\nÍ∞Å ÏÉÅÌíàÏùò ÌäπÏÑ±Í≥º Ï°∞Í±¥ÏùÑ Í≥†Î†§ÌïòÏó¨ Î≥∏Ïù∏ÏóêÍ≤å Í∞ÄÏû• Ï†ÅÌï©Ìïú ÏÉÅÌíàÏùÑ ÏÑ†ÌÉùÌïòÏãúÍ∏∞ Î∞îÎûçÎãàÎã§. Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÎ©¥ ÎçîÏö± Î™ÖÏæåÌïú ÎãµÎ≥ÄÏùÑ Ìï† Ïàò ÏûàÏäµÎãàÎã§.', 'ÏàòÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"ShÌèâÏÉùÏ£ºÍ±∞ÎûòÏö∞ÎåÄÌÜµÏû•\"Ïù¥ ÏûàÏäµÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.05%Ïù¥Î©∞, Ïö∞ÎåÄÏ°∞Í±¥ÏùÑ Ï∂©Ï°±Ìï† Í≤ΩÏö∞ ÏµúÍ≥†Í∏àÎ¶¨ 0.2%Î•º Ï†ÅÏö©Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§. ÏõîÏßÄÍ∏â Î∞©ÏãùÏúºÎ°ú Ïù¥ÏûêÍ∞Ä ÏßÄÍ∏âÎêòÎ©∞, Í∞ÄÏûÖÏùÄ ÏòÅÏóÖÏ†êÏù¥ÎÇò Ïä§ÎßàÌä∏Î±ÖÌÇπÏùÑ ÌÜµÌï¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\\n\\nÎÜçÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"NHÍ≥†Ìñ•ÏÇ¨ÎûëÍ∏∞Î∂ÄÏòàÍ∏à\"Ïù¥ ÏûàÏúºÎ©∞, ÏµúÍ≥†Í∏àÎ¶¨Îäî 3.90%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î∂ÄÎ•º ÌÜµÌï¥ ÏÇ¨ÌöåÏóê Í∏∞Ïó¨Ìï† Ïàò ÏûàÎäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§.\\n\\nKBÍµ≠ÎØºÏùÄÌñâÏùò \"KBÎßàÏù¥ÌïèÌÜµÏû•\"ÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.1%Ïù¥Í≥†, Ïö∞ÎåÄÍ∏àÎ¶¨Î•º Ìè¨Ìï®Ìïú ÏµúÍ≥†Í∏àÎ¶¨Îäî 1.5%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Îßå 18ÏÑ∏ Ïù¥ÏÉÅ Îßå 38ÏÑ∏ Ïù¥ÌïòÏùò Ïã§Î™ÖÏùò Í∞úÏù∏Ïù¥ Í∞ÄÏûÖÌï† Ïàò ÏûàÏúºÎ©∞, Î∂ÑÍ∏∞Î≥ÑÎ°ú Ïù¥ÏûêÍ∞Ä ÏßÄÍ∏âÎê©ÎãàÎã§.\\n\\nÍ∞Å ÏÉÅÌíàÏùò ÌäπÏÑ±Í≥º Ï°∞Í±¥ÏùÑ Í≥†Î†§ÌïòÏó¨ Î≥∏Ïù∏ÏóêÍ≤å Í∞ÄÏû• Ï†ÅÌï©Ìïú ÏÉÅÌíàÏùÑ ÏÑ†ÌÉùÌïòÏãúÍ∏∞ Î∞îÎûçÎãàÎã§. Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÎ©¥ ÎçîÏö± Î™ÖÏæåÌïú ÎãµÎ≥ÄÏùÑ Ìï† Ïàò ÏûàÏäµÎãàÎã§.'] ---\n",
      "--- ÎãµÎ≥Ä Ìï†Î£®ÏãúÎÑ§Ïù¥ÏÖò ÌèâÍ∞Ä ---\n",
      "--- ÏÉùÏÑ±Îêú ÎãµÎ≥ÄÏùò Í∑ºÍ±∞Í∞Ä Î∂ÄÏ°± -> generate Ïû¨ÏãúÎèÑ ---\n",
      "--- ÎãµÎ≥Ä ÏÉùÏÑ± (ÌûàÏä§ÌÜ†Î¶¨ Ìè¨Ìï®) ---\n",
      "--- ÎãµÎ≥Ä ÌèâÍ∞Ä (ÏÉùÏÑ±) ---\n",
      "--- ÏÉùÏÑ±Îêú ÎãµÎ≥Ä: ['ÏàòÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"ShÌèâÏÉùÏ£ºÍ±∞ÎûòÏö∞ÎåÄÌÜµÏû•\"Ïù¥ ÏûàÏäµÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.05%Ïù¥Î©∞, Ïö∞ÎåÄÏ°∞Í±¥ÏùÑ Ï∂©Ï°±Ìï† Í≤ΩÏö∞ ÏµúÍ≥†Í∏àÎ¶¨ 0.2%Î•º Ï†ÅÏö©Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§. Îß§Ïùº ÏµúÏ¢ÖÏûîÏï°Ïóê Îî∞Îùº Í∏àÎ¶¨Í∞Ä Îã¨ÎùºÏßÄÎ©∞, Í∞ÄÏûÖÏùÄ ÏòÅÏóÖÏ†êÏù¥ÎÇò Ïä§ÎßàÌä∏Î±ÖÌÇπÏùÑ ÌÜµÌï¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\\n\\nÎÜçÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"NHÍ≥†Ìñ•ÏÇ¨ÎûëÍ∏∞Î∂ÄÏòàÍ∏à\"Ïù¥ ÏûàÏúºÎ©∞, ÏµúÍ≥†Í∏àÎ¶¨Îäî 3.90%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î∂ÄÎ•º ÌÜµÌï¥ ÏßÄÏó≠ÏÇ¨ÌöåÏóê Í∏∞Ïó¨Ìï† Ïàò ÏûàÎäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§.\\n\\nKBÍµ≠ÎØºÏùÄÌñâÏùò \"KBÎßàÏù¥ÌïèÌÜµÏû•\"ÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.1%Ïù¥Í≥†, Ïö∞ÎåÄÍ∏àÎ¶¨Î•º Ìè¨Ìï®Ìïú ÏµúÍ≥†Í∏àÎ¶¨Îäî 1.5%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Îßå 18ÏÑ∏ Ïù¥ÏÉÅ Îßå 38ÏÑ∏ Ïù¥ÌïòÏùò Ïã§Î™ÖÏùò Í∞úÏù∏Ïù¥ Í∞ÄÏûÖÌï† Ïàò ÏûàÏúºÎ©∞, ÎπÑÏÉÅÍ∏à Ïù¥Ïú®ÏùÑ Ï†úÍ≥µÌïòÎäî Ï°∞Í±¥Ïù¥ ÏûàÏäµÎãàÎã§.\\n\\nÍ∞Å ÏÉÅÌíàÏùò ÌäπÏÑ±Í≥º Ï°∞Í±¥ÏùÑ Í≥†Î†§ÌïòÏó¨ Î≥∏Ïù∏ÏóêÍ≤å Í∞ÄÏû• Ï†ÅÌï©Ìïú ÏÉÅÌíàÏùÑ ÏÑ†ÌÉùÌïòÏãúÍ∏∞ Î∞îÎûçÎãàÎã§. Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÎ©¥ ÎçîÏö± Î™ÖÏæåÌïú ÎãµÎ≥ÄÏùÑ Ìï† Ïàò ÏûàÏäµÎãàÎã§.', 'ÏàòÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"ShÌèâÏÉùÏ£ºÍ±∞ÎûòÏö∞ÎåÄÌÜµÏû•\"Ïù¥ ÏûàÏäµÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.05%Ïù¥Î©∞, Ïö∞ÎåÄÏ°∞Í±¥ÏùÑ Ï∂©Ï°±Ìï† Í≤ΩÏö∞ ÏµúÍ≥†Í∏àÎ¶¨ 0.2%Î•º Ï†ÅÏö©Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§. ÏõîÏßÄÍ∏â Î∞©ÏãùÏúºÎ°ú Ïù¥ÏûêÍ∞Ä ÏßÄÍ∏âÎêòÎ©∞, Í∞ÄÏûÖÏùÄ ÏòÅÏóÖÏ†êÏù¥ÎÇò Ïä§ÎßàÌä∏Î±ÖÌÇπÏùÑ ÌÜµÌï¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\\n\\nÎÜçÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"NHÍ≥†Ìñ•ÏÇ¨ÎûëÍ∏∞Î∂ÄÏòàÍ∏à\"Ïù¥ ÏûàÏúºÎ©∞, ÏµúÍ≥†Í∏àÎ¶¨Îäî 3.90%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î∂ÄÎ•º ÌÜµÌï¥ ÏÇ¨ÌöåÏóê Í∏∞Ïó¨Ìï† Ïàò ÏûàÎäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§.\\n\\nKBÍµ≠ÎØºÏùÄÌñâÏùò \"KBÎßàÏù¥ÌïèÌÜµÏû•\"ÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.1%Ïù¥Í≥†, Ïö∞ÎåÄÍ∏àÎ¶¨Î•º Ìè¨Ìï®Ìïú ÏµúÍ≥†Í∏àÎ¶¨Îäî 1.5%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Îßå 18ÏÑ∏ Ïù¥ÏÉÅ Îßå 38ÏÑ∏ Ïù¥ÌïòÏùò Ïã§Î™ÖÏùò Í∞úÏù∏Ïù¥ Í∞ÄÏûÖÌï† Ïàò ÏûàÏúºÎ©∞, Î∂ÑÍ∏∞Î≥ÑÎ°ú Ïù¥ÏûêÍ∞Ä ÏßÄÍ∏âÎê©ÎãàÎã§.\\n\\nÍ∞Å ÏÉÅÌíàÏùò ÌäπÏÑ±Í≥º Ï°∞Í±¥ÏùÑ Í≥†Î†§ÌïòÏó¨ Î≥∏Ïù∏ÏóêÍ≤å Í∞ÄÏû• Ï†ÅÌï©Ìïú ÏÉÅÌíàÏùÑ ÏÑ†ÌÉùÌïòÏãúÍ∏∞ Î∞îÎûçÎãàÎã§. Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÎ©¥ ÎçîÏö± Î™ÖÏæåÌïú ÎãµÎ≥ÄÏùÑ Ìï† Ïàò ÏûàÏäµÎãàÎã§.', 'ÏàòÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"ShÌèâÏÉùÏ£ºÍ±∞ÎûòÏö∞ÎåÄÌÜµÏû•\"Ïù¥ ÏûàÏäµÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.05%Ïù¥Î©∞, Ïö∞ÎåÄÏ°∞Í±¥ÏùÑ Ï∂©Ï°±Ìï† Í≤ΩÏö∞ ÏµúÍ≥†Í∏àÎ¶¨ 0.2%Î•º Ï†ÅÏö©Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§. Îß§Ïùº ÏµúÏ¢ÖÏûîÏï°Ïóê Îî∞Îùº Í∏àÎ¶¨Í∞Ä Îã¨ÎùºÏßÄÎ©∞, Í∞ÄÏûÖÏùÄ ÏòÅÏóÖÏ†êÏù¥ÎÇò Ïä§ÎßàÌä∏Î±ÖÌÇπÏùÑ ÌÜµÌï¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\\n\\nÎÜçÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"NHÍ≥†Ìñ•ÏÇ¨ÎûëÍ∏∞Î∂ÄÏòàÍ∏à\"Ïù¥ ÏûàÏúºÎ©∞, ÏµúÍ≥†Í∏àÎ¶¨Îäî 3.90%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î∂ÄÎ•º ÌÜµÌï¥ ÏßÄÏó≠ÏÇ¨ÌöåÏóê Í∏∞Ïó¨Ìï† Ïàò ÏûàÎäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§.\\n\\nÍµ≠ÎØºÏùÄÌñâÏùò \"KBÎßàÏù¥ÌïèÌÜµÏû•\"ÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.1%Ïù¥Í≥†, Ïö∞ÎåÄÏ°∞Í±¥ÏùÑ Ï∂©Ï°±Ìï† Í≤ΩÏö∞ ÏµúÍ≥†Í∏àÎ¶¨ 1.5%Î•º Ï†ÅÏö©Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§. Ïù¥ ÌÜµÏû•ÏùÄ Îßå 18ÏÑ∏ Ïù¥ÏÉÅ Îßå 38ÏÑ∏ Ïù¥ÌïòÏùò Ïã§Î™ÖÏùò Í∞úÏù∏Ïù¥ Í∞ÄÏûÖÌï† Ïàò ÏûàÏúºÎ©∞, Îã§ÏñëÌïú ÏûÖÍ∏à Ïã§Ï†ÅÏóê Îî∞Îùº ÎπÑÏÉÅÍ∏à Ïù¥Ïú®ÏùÑ Ï†úÍ≥µÎ∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\\n\\nÍ∞Å ÏÉÅÌíàÏùò ÌäπÏÑ±ÏùÑ Í≥†Î†§ÌïòÏó¨ Î≥∏Ïù∏Ïùò ÏÉÅÌô©Ïóê ÎßûÎäî ÏÉÅÌíàÏùÑ ÏÑ†ÌÉùÌïòÏãúÎ©¥ Ï¢ãÍ≤†ÏäµÎãàÎã§. Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÎ©¥ ÎçîÏö± Î™ÖÏæåÌïú ÎãµÎ≥ÄÏùÑ Ìï† Ïàò ÏûàÏäµÎãàÎã§.'] ---\n",
      "--- ÏÉùÏÑ± ÌöüÏàò Ï¥àÍ≥º, Ï¢ÖÎ£å -> end ---\n",
      "--- History ÌôïÏù∏ ---\n",
      "[('ÏàòÌòëÏùÄÌñâÏù¥Îûë ÎÜçÌòëÏù¥Îûë Íµ≠ÎØºÏùÄÌñâ Í∞Å ÏùÄÌñâÎì§Ïùò ÏòàÍ∏à ÏÉÅÌíà Ï∂îÏ≤úÌï¥Ï§ò', 'ÏàòÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"ShÌèâÏÉùÏ£ºÍ±∞ÎûòÏö∞ÎåÄÌÜµÏû•\"Ïù¥ ÏûàÏäµÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.05%Ïù¥Î©∞, Ïö∞ÎåÄÏ°∞Í±¥ÏùÑ Ï∂©Ï°±Ìï† Í≤ΩÏö∞ ÏµúÍ≥†Í∏àÎ¶¨ 0.2%Î•º Ï†ÅÏö©Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§. Îß§Ïùº ÏµúÏ¢ÖÏûîÏï°Ïóê Îî∞Îùº Í∏àÎ¶¨Í∞Ä Îã¨ÎùºÏßÄÎ©∞, Í∞ÄÏûÖÏùÄ ÏòÅÏóÖÏ†êÏù¥ÎÇò Ïä§ÎßàÌä∏Î±ÖÌÇπÏùÑ ÌÜµÌï¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\\n\\nÎÜçÌòëÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏúºÎ°úÎäî \"NHÍ≥†Ìñ•ÏÇ¨ÎûëÍ∏∞Î∂ÄÏòàÍ∏à\"Ïù¥ ÏûàÏúºÎ©∞, ÏµúÍ≥†Í∏àÎ¶¨Îäî 3.90%ÏûÖÎãàÎã§. Ïù¥ ÏÉÅÌíàÏùÄ Í∏∞Î∂ÄÎ•º ÌÜµÌï¥ ÏßÄÏó≠ÏÇ¨ÌöåÏóê Í∏∞Ïó¨Ìï† Ïàò ÏûàÎäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§.\\n\\nÍµ≠ÎØºÏùÄÌñâÏùò \"KBÎßàÏù¥ÌïèÌÜµÏû•\"ÏùÄ Í∏∞Î≥∏Í∏àÎ¶¨Í∞Ä 0.1%Ïù¥Í≥†, Ïö∞ÎåÄÏ°∞Í±¥ÏùÑ Ï∂©Ï°±Ìï† Í≤ΩÏö∞ ÏµúÍ≥†Í∏àÎ¶¨ 1.5%Î•º Ï†ÅÏö©Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§. Ïù¥ ÌÜµÏû•ÏùÄ Îßå 18ÏÑ∏ Ïù¥ÏÉÅ Îßå 38ÏÑ∏ Ïù¥ÌïòÏùò Ïã§Î™ÖÏùò Í∞úÏù∏Ïù¥ Í∞ÄÏûÖÌï† Ïàò ÏûàÏúºÎ©∞, Îã§ÏñëÌïú ÏûÖÍ∏à Ïã§Ï†ÅÏóê Îî∞Îùº ÎπÑÏÉÅÍ∏à Ïù¥Ïú®ÏùÑ Ï†úÍ≥µÎ∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\\n\\nÍ∞Å ÏÉÅÌíàÏùò ÌäπÏÑ±ÏùÑ Í≥†Î†§ÌïòÏó¨ Î≥∏Ïù∏Ïùò ÏÉÅÌô©Ïóê ÎßûÎäî ÏÉÅÌíàÏùÑ ÏÑ†ÌÉùÌïòÏãúÎ©¥ Ï¢ãÍ≤†ÏäµÎãàÎã§. Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÎ©¥ ÎçîÏö± Î™ÖÏæåÌïú ÎãµÎ≥ÄÏùÑ Ìï† Ïàò ÏûàÏäµÎãàÎã§.\\n\\n [ÏÉÅÌíàÏÑ§Î™ÖÏÑú PDF Î≥¥Í∏∞](http://localhost:8000/pdf/demand_deposit/ÏàòÌòë/ÏàòÌòë_ÌèâÏÉùÏ£ºÍ±∞ÎûòÏö∞ÎåÄÌÜµÏû•.pdf)')]\n"
     ]
    }
   ],
   "source": [
    "# Ï±óÎ¥á ÌÅ¥ÎûòÏä§\n",
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.thread_id = str(uuid.uuid4())\n",
    "\n",
    "    def chat(self, message: str, history: List[Tuple[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        ÏûÖÎ†• Î©îÏãúÏßÄÏôÄ ÎåÄÌôî Ïù¥Î†•ÏùÑ Í∏∞Î∞òÏúºÎ°ú Adaptive Self-RAG Ï≤¥Ïù∏ÏùÑ Ìò∏Ï∂úÌïòÍ≥†,\n",
    "        ÏùëÎãµÏùÑ Î∞òÌôòÌï©ÎãàÎã§.\n",
    "        \"\"\"\n",
    "        config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
    "        state = initialize_state()\n",
    "        state[\"question\"] = message\n",
    "        \n",
    "        # historyÍ∞Ä ÏûàÏúºÎ©¥ Ï∂îÍ∞Ä\n",
    "        if history:\n",
    "            state[\"history\"] = history\n",
    "        \n",
    "        result = adaptive_self_rag_memory.invoke(state, config=config)\n",
    "\n",
    "        gen_list = result.get(\"generation\", [])\n",
    "        docs = result.get(\"filtered_documents\", [])\n",
    "\n",
    "        if not gen_list:\n",
    "            bot_response = \"Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§.\"\n",
    "        else:\n",
    "            raw_answer = gen_list[-1]\n",
    "            bot_response = postprocess_answer(raw_answer, docs)\n",
    "\n",
    "        # ÎåÄÌôî Ïù¥Î†• ÏóÖÎç∞Ïù¥Ìä∏\n",
    "        state[\"history\"].append((message, bot_response))\n",
    "        print(f\"--- History ÌôïÏù∏ ---\\n{state['history']}\")\n",
    "        return bot_response\n",
    "\n",
    "\n",
    "# Ï±óÎ¥á Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±\n",
    "chatbot = ChatBot() \n",
    "\n",
    "# Gradio Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÉùÏÑ±\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot.chat,\n",
    "    title=\"Adaptive Self-RAG Í∏∞Î∞ò RAG Ï±óÎ¥á ÏãúÏä§ÌÖú\",\n",
    "    description=\"Ï†ïÍ∏∞ÏòàÍ∏à, ÏûÖÏ∂úÍ∏àÏûêÏú†ÏòàÍ∏à ÏÉÅÌíà Î∞è Í∏∞ÌÉÄ ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌï©ÎãàÎã§.\",\n",
    "    examples=[\n",
    "        \"Ï†ïÍ∏∞ÏòàÍ∏à ÏÉÅÌíà Ï§ë Í∏àÎ¶¨Í∞Ä Í∞ÄÏû• ÎÜíÏùÄ Í≤ÉÏùÄ?\",\n",
    "        \"Ï†ïÍ∏∞ÏòàÍ∏àÍ≥º ÏûÖÏ∂úÍ∏àÏûêÏú†ÏòàÍ∏àÏùÄ Ïñ¥Îñ§ Ï∞®Ïù¥Ï†êÏù¥ ÏûàÎÇòÏöî?\",\n",
    "        \"ÏùÄÌñâÏùò ÏòàÍ∏à ÏÉÅÌíàÏùÑ Ï∂îÏ≤úÌï¥ Ï£ºÏÑ∏Ïöî.\"\n",
    "    ],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "# Gradio Ïï± Ïã§Ìñâ: Ïù¥ ÌååÏùºÏùÑ Î©îÏù∏ÏúºÎ°ú Ïã§ÌñâÌï† ÎïåÎßå ÎùÑÏõÅÎãàÎã§.\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eefd7c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e4f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
