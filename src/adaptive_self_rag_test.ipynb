{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09ba001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adaptive_self_rag\n",
    "금융상품(예: 정기예금, 입출금자유예금) 관련 질의에 대해:\n",
    "1. 질문 라우팅 → (금융상품 관련이면) 문서 검색 (병렬 서브 그래프) → 문서 평가 → (조건부) 질문 재작성 → 답변 생성\n",
    "   / (금융상품과 무관하면) LLM fallback을 통해 바로 답변 생성\n",
    "그리고 생성된 답변의 품질(환각, 관련성) 평가 후 필요시 재생성 또는 재작성하는 Adaptive Self-RAG 체인.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#############################\n",
    "# 1. 기본 환경 및 라이브러리\n",
    "#############################\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 기타 유틸\n",
    "import json\n",
    "import uuid\n",
    "from pprint import pprint\n",
    "from textwrap import dedent\n",
    "from operator import add\n",
    "\n",
    "\n",
    "# LangChain, Chroma, LLM 관련\n",
    "from typing import List, Literal, Sequence, TypedDict, Annotated, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Grader 평가지표용\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 그래프 관련\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Gradio 관련\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c73b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbb1fc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#############################\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 4. LLM 초기화 & 도구 바인딩\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#############################\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m llm = \u001b[43mChatOpenAI\u001b[49m(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0\u001b[39m, streaming=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#############################\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 5. LLM 체인 (Retrieval Grader / Answer Generator / Hallucination / Answer Graders / Question Re-writer)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#############################\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===================================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ChatOpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# 4. LLM 초기화 & 도구 바인딩\n",
    "#############################\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, streaming=True)\n",
    "\n",
    "#############################\n",
    "# 5. LLM 체인 (Retrieval Grader / Answer Generator / Hallucination / Answer Graders / Question Re-writer)\n",
    "#############################\n",
    "print(\"\\n===================================================================\\n \")\n",
    "print(\"LLM 체인\\n\")\n",
    "print(\"# (1) Retrieval Grader\\n\")\n",
    "\n",
    "# (1) Retrieval Grader (검색평가)\n",
    "class BinaryGradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_BinaryGradeDocuments = llm.with_structured_output(BinaryGradeDocuments)\n",
    "\n",
    "system_prompt = \"\"\"You are an expert in evaluating the relevance of search results to user queries.\n",
    "\n",
    "[Evaluation criteria]\n",
    "1. 키워드 관련성: 문서가 질문의 주요 단어나 유사어를 포함하는지 확인\n",
    "2. 의미적 관련성: 문서의 전반적인 주제가 질문의 의도와 일치하는지 평가\n",
    "3. 부분 관련성: 질문의 일부를 다루거나 맥락 정보를 제공하는 문서도 고려\n",
    "4. 답변 가능성: 직접적인 답이 아니더라도 답변 형성에 도움될 정보 포함 여부 평가\n",
    "\n",
    "[Scoring]\n",
    "- Rate 'yes' if relevant, 'no' if not\n",
    "- Default to 'no' when uncertain\n",
    "\n",
    "[Key points]\n",
    "- Consider the full context of the query, not just word matching\n",
    "- Rate as relevant if useful information is present, even if not a complete answer\n",
    "\n",
    "Your evaluation is crucial for improving information retrieval systems. Provide balanced assessments.\n",
    "\"\"\"\n",
    "# 채점 프롬프트 템플릿\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"[Retrieved document]\\n{document}\\n\\n[User question]\\n{question}\")\n",
    "])\n",
    "\n",
    "retrieval_grader_binary = grade_prompt | structured_llm_BinaryGradeDocuments\n",
    "\n",
    "question = \"어떤 예금 상품이 있는지 설명해주세요.\"\n",
    "print(f'\\nquestion : {question}\\n')\n",
    "retrieved_docs = fixed_deposit_db.similarity_search(question, k=2)\n",
    "print(f\"검색된 문서 수: {len(retrieved_docs)}\")\n",
    "print(\"===============================================================================\")\n",
    "print()\n",
    "\n",
    "relevant_docs = []\n",
    "for doc in retrieved_docs:\n",
    "    print(\"문서:\\n\", doc.page_content)\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "\n",
    "    relevance = retrieval_grader_binary.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "    print(f\"문서 관련성: {relevance}\")\n",
    "\n",
    "    if relevance.binary_score == 'yes':\n",
    "        relevant_docs.append(doc)\n",
    "    \n",
    "    print(\"===========================================================================\")\n",
    "\n",
    "print(\"\\n# (2) Answer Generator (일반 RAG) \\n\")\n",
    "\n",
    "# (2) Answer Generator (일반 RAG)\n",
    "def generator_rag_answer(question, docs):\n",
    "\n",
    "    template = \"\"\"\n",
    "    [Your task]\n",
    "    You are a financial product expert and consultant who always responds in Korean.\n",
    "    Your task is to analyze the user query and the given financial product data to recommend the most suitable financial product.\n",
    "    \n",
    "    [Instructions]\n",
    "    1. 질문과 관련된 정보를 문맥에서 신중하게 확인합니다.\n",
    "    2. 답변에 질문과 직접 관련된 정보만 사용합니다.\n",
    "    3. 문맥에 명시되지 않은 내용에 대해 추측하지 않습니다.\n",
    "    4. 불필요한 정보를 피하고, 답변을 간결하고 명확하게 작성합니다.\n",
    "    5. 문맥에서 정확한 답변을 생성할 수 없다면 최대한 필요한 답변을 생성한 뒤 마지막에 \"더 구체적인 정보를 알려주시면 더욱 명쾌한 답변을 할 수 있습니다.\"라고 덧붙여 답변합니다.\n",
    "    6. 적절한 경우 문맥에서 직접 인용하며, 따옴표를 사용합니다.\n",
    "\n",
    "    [Context]\n",
    "    {context}\n",
    "\n",
    "    [Question]\n",
    "    {question}\n",
    "\n",
    "    [Answer]\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    local_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    rag_chain = prompt | local_llm | StrOutputParser()\n",
    "    generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "    return generation\n",
    "\n",
    "generation = generator_rag_answer(question, docs=relevant_docs)\n",
    "print(\"Generated Answer (일반 RAG):\")\n",
    "print(generation)\n",
    "\n",
    "# (3) Hallucination Grader\n",
    "print(\"\\n# (3) Hallucination Grader\\n\")\n",
    "\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_HradeHallucinations = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# 환각 평가를 위한 시스템 프롬프트 정의\n",
    "halluci_system_prompt = \"\"\"\n",
    "You are an expert evaluator assessing whether an LLM-generated answer is grounded in and supported by a given set of facts.\n",
    "\n",
    "[Your task]\n",
    "    - Review the LLM-generated answer.\n",
    "    - Determine if the answer is fully supported by the given facts.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - 답변에 주어진 사실이나 명확히 추론할 수 있는 정보 외의 내용이 없어야 합니다.\n",
    "    - 답변의 모든 핵심 내용이 주어진 사실에서 비롯되어야 합니다.\n",
    "    - 사실적 정확성에 집중하고, 글쓰기 스타일이나 완전성은 평가하지 않습니다.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer is factually grounded and fully supported.\n",
    "    - 'no': The answer includes information or claims not based on the given facts.\n",
    "\n",
    "Your evaluation is crucial in ensuring the reliability and factual accuracy of AI-generated responses. Be thorough and critical in your assessment.\n",
    "\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", halluci_system_prompt),\n",
    "        (\"human\", \"[Set of facts]\\n{documents}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_HradeHallucinations\n",
    "hallucination = hallucination_grader.invoke({\"documents\": relevant_docs, \"generation\": generation})\n",
    "print(f\"환각 평가: {hallucination}\")\n",
    "\n",
    "print(\"\\n# (4) Answer Grader\\n\")\n",
    "# (4) Answer Grader \n",
    "class BinaryGradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_BinaryGradeAnswer = llm.with_structured_output(BinaryGradeAnswer)\n",
    "grade_system_prompt = \"\"\"\n",
    "You are an expert evaluator tasked with assessing whether an LLM-generated answer effectively addresses and resolves a user's question.\n",
    "\n",
    "[Your task]\n",
    "    - Carefully analyze the user's question to understand its core intent and requirements.\n",
    "    - Determine if the LLM-generated answer sufficiently resolves the question.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - 관련성: 답변이 질문과 직접적으로 관련되어야 합니다.\n",
    "    - 완전성: 질문의 모든 측면이 다뤄져야 합니다.\n",
    "    - 정확성: 제공된 정보가 정확하고 최신이어야 합니다.\n",
    "    - 명확성: 답변이 명확하고 이해하기 쉬워야 합니다.\n",
    "    - 구체성: 질문의 요구 사항에 맞는 상세한 답변이어야 합니다.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer effectively resolves the question.\n",
    "    - 'no': The answer fails to sufficiently resolve the question or lacks crucial elements.\n",
    "\n",
    "Your evaluation plays a critical role in ensuring the quality and effectiveness of AI-generated responses. Strive for balanced and thoughtful assessments.\n",
    "\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"[User question]\\n{question}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader_binary = answer_prompt | structured_llm_BinaryGradeAnswer\n",
    "print(\"Question:\", question)\n",
    "print(\"Generation:\", generation)\n",
    "answer_score = answer_grader_binary.invoke({\"question\": question, \"generation\": generation})\n",
    "print(f\"답변 평가: {answer_score}\")\n",
    "\n",
    "\n",
    "print(\"\\n# (5) Question Re-writer\\n\")\n",
    "# (5) Question Re-writer\n",
    "def rewrite_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    입력 질문을 벡터 검색에 최적화된 형태로 재작성한다.\n",
    "    \"\"\"\n",
    "    local_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert question re-writer. Your task is to convert input questions into optimized versions \n",
    "    for vectorstore retrieval. Analyze the input carefully and focus on capturing the underlying semantic \n",
    "    intent and meaning. Your goal is to create a question that will lead to more effective and relevant \n",
    "    document retrieval.\n",
    "\n",
    "    [Guidelines]\n",
    "        1. Identify and emphasize core concepts and key subjects.\n",
    "        2. Expand abbreviations or ambiguous terms.\n",
    "        3. Include synonyms or related terms that might appear in relevant documents.\n",
    "        4. Maintain the original intent and scope.\n",
    "        5. For complex questions, break them down into simpler, focused sub-questions.\n",
    "    \"\"\"\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"[Initial question]\\n{question}\\n\\n[Improved question]\\n\")\n",
    "    ])\n",
    "    question_rewriter = re_write_prompt | local_llm | StrOutputParser()\n",
    "    rewritten_question = question_rewriter.invoke({\"question\": question})\n",
    "    return rewritten_question\n",
    "\n",
    "print(\"\\n# (6) Generation Evaluation & Decision Nodes\\n\")\n",
    "# (6) Generation Evaluation & Decision Nodes\n",
    "def grade_generation_self(state: \"SelfRagOverallState\") -> str:\n",
    "    print(\"--- 답변 평가 (생성) ---\")\n",
    "    print(f\"--- 생성된 답변: {state['generation']} ---\")\n",
    "    if state['num_generations'] > 2:\n",
    "        print(\"--- 생성 횟수 초과, 종료 -> end ---\")\n",
    "        return \"end\"\n",
    "    # 평가를 위한 문서 텍스트 구성\n",
    "    print(\"--- 답변 할루시네이션 평가 ---\")\n",
    "    docs_text = \"\\n\\n\".join([d.page_content for d in state['documents']])\n",
    "    hallucination_grade = hallucination_grader.invoke({\n",
    "        \"documents\": docs_text,\n",
    "        \"generation\": state['generation']\n",
    "    })\n",
    "    if hallucination_grade.binary_score == \"yes\":\n",
    "        relevance_grade = retrieval_grader_binary.invoke({\n",
    "            \"question\": state['question'],\n",
    "            \"generation\": state['generation']\n",
    "        })\n",
    "        print(\"--- 답변-질문 관련성 평가 ---\")\n",
    "        if relevance_grade.binary_score == \"yes\":\n",
    "            print(\"--- 생성된 답변이 질문을 잘 해결함 ---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"--- 답변 관련성이 부족 -> transform_query ---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"--- 생성된 답변의 근거가 부족 -> generate 재시도 ---\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "def decide_to_generate_self(state: \"SelfRagOverallState\") -> str:\n",
    "    print(\"--- 평가된 문서 분석 ---\")\n",
    "    if state['num_generations'] > 2:\n",
    "        print(\"--- 생성 횟수 초과, 생성 결정 ---\")\n",
    "        return \"generate\"\n",
    "    # 여기서는 필터링된 문서가 존재하는지 확인\n",
    "    if not state['filtered_documents']:\n",
    "        print(\"--- 관련 문서 없음 -> transform_query ---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"--- 관련 문서 존재 -> generate ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "# (7) RoutingDecision \n",
    "class RoutingDecision(BaseModel):\n",
    "    \"\"\"Determines whether a user question should be routed to document search or LLM fallback.\"\"\"\n",
    "    route: Literal[\"search_data\",\"llm_fallback\"] = Field(\n",
    "        description=\"Classify the question as 'search_data' (financial) or 'llm_fallback' (general)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb9b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
